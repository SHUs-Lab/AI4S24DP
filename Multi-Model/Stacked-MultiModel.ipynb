{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, sys\n",
    "from calflops import calculate_flops\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "LiGO_bool = True\n",
    "emb_size = 128\n",
    "new_emb_size = 256\n",
    "block_size = 64\n",
    "multi_heads = 2\n",
    "batch_size = 512\n",
    "epochs = 3\n",
    "learning_rate = 3e-4\n",
    "device = torch.device('cuda:0')\n",
    "num_small_layers = 4\n",
    "multi_heads = 2\n",
    "num_large_layers = 8\n",
    "batches_per_epoch = 10\n",
    "num_models = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset wikitext-2\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-v1')\n",
    "\n",
    "# Dataset wikitext-103\n",
    "# dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# padding token \n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], return_tensors='pt', truncation=True, padding='max_length', max_length=block_size)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "val_dataset = tokenized_datasets[\"validation\"]\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size, collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# Calculate the split points\n",
    "train_split = len(train_dataset) // 2\n",
    "val_split = len(val_dataset) // 2\n",
    "\n",
    "# Create subsets for each half\n",
    "train_dataset_1 = Subset(train_dataset, range(train_split))\n",
    "train_dataset_2 = Subset(train_dataset, range(train_split, len(train_dataset)))\n",
    "\n",
    "val_dataset_1 = Subset(val_dataset, range(val_split))\n",
    "val_dataset_2 = Subset(val_dataset, range(val_split, len(val_dataset)))\n",
    "\n",
    "# Create DataLoaders for each half\n",
    "train_dataloader_1 = DataLoader(train_dataset_1, shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "train_dataloader_2 = DataLoader(train_dataset_2, shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "val_dataloader_1 = DataLoader(val_dataset_1, shuffle=False, batch_size=batch_size, collate_fn=data_collator)\n",
    "val_dataloader_2 = DataLoader(val_dataset_2, shuffle=False, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "train_dataloader_1 = list(train_dataloader_1)\n",
    "train_dataloader_2 = list(train_dataloader_2)\n",
    "\n",
    "val_dataloader_1 = list(val_dataloader_1)\n",
    "val_dataloader_2 = list(val_dataloader_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = list(train_dataloader)\n",
    "val_dataloader = list(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(mdl, dataloader, device=device):\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    mdl.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    ix = torch.randint(len(dataloader), tuple([5]))\n",
    "    # dataloader = list(dataloader)\n",
    "    for index in ix:\n",
    "        batch = dataloader[index]\n",
    "        inputs = batch[\"input_ids\"].detach().clone().detach().squeeze(1).to(device)\n",
    "        labels = batch[\"labels\"].detach().clone().detach().to(device)\n",
    "        outputs = mdl(inputs)\n",
    "        loss = loss_fn(outputs.view(-1, vocab_size), labels.view(-1))\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        del loss\n",
    "        del outputs\n",
    "    return total_loss / num_batches\n",
    "\n",
    "@torch.enable_grad()\n",
    "def train(mdl, optim, epochs, train_dataloader=train_dataloader, val_dataloader=val_dataloader, device=device):\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    mdl.to(device)\n",
    "    val_losses_epoch = []\n",
    "    # train_dataloader = list(train_dataloader)\n",
    "    for epoch in range(epochs):\n",
    "        mdl.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        ix = torch.randint(len(train_dataloader), tuple([batches_per_epoch]))\n",
    "        for index in ix:\n",
    "            batch = train_dataloader[index]\n",
    "            # print(f\"Processing Batch: {index} in Epoch: {epoch}\")\n",
    "            inputs = batch[\"input_ids\"].detach().clone().detach().to(device).squeeze(1)\n",
    "            optim.zero_grad()\n",
    "            outputs = mdl(inputs)\n",
    "            labels = batch[\"labels\"].detach().clone().detach().to(device)\n",
    "            loss = loss_fn(outputs.view(-1, vocab_size), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            del loss\n",
    "            del outputs\n",
    "        avg_loss = total_loss / num_batches\n",
    "        val_loss = validate(mdl, val_dataloader, device=device)\n",
    "        val_losses_epoch.append(val_loss)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return val_losses_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(torch.nn.Module):\n",
    "    def __init__(self, big=False):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.k = torch.nn.Linear(new_emb_size if big else emb_size, new_emb_size if big else emb_size, bias=False)\n",
    "        self.q = torch.nn.Linear(new_emb_size if big else emb_size, new_emb_size if big else emb_size, bias=False)\n",
    "        self.v = torch.nn.Linear(new_emb_size if big else emb_size, new_emb_size if big else emb_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
    "    def forward(self, e):\n",
    "        keys = self.k(e)\n",
    "        queries = self.q(e)\n",
    "        values = self.v(e)\n",
    "        ret = keys @ queries.transpose(1, 2)*(1.0/math.sqrt(keys.size(-1)))\n",
    "        ret = torch.masked_fill(ret, self.tril==0, -torch.inf)\n",
    "        ret = torch.softmax(ret, 2)\n",
    "        ret = ret @ values\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(torch.nn.Module):\n",
    "    def __init__(self, big=False):\n",
    "        super(MultiHead, self).__init__()\n",
    "        self.head1 = AttentionHead(big)\n",
    "        self.head2 = AttentionHead(big)\n",
    "        self.mh_lin = torch.nn.Linear(multi_heads*(new_emb_size if big else emb_size), new_emb_size if big else emb_size, bias=False)\n",
    "        self.drop = torch.nn.Dropout(0.1)\n",
    "    def forward(self, inp):\n",
    "        x1 = self.head1(inp)\n",
    "        x2 = self.head2(inp)\n",
    "        return self.mh_lin(self.drop(torch.cat([x1,x2], dim=2))).relu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, big=False):\n",
    "        super(Block, self).__init__()\n",
    "        self.multihead = MultiHead(big)\n",
    "        self.l_norm_1 = torch.nn.LayerNorm(new_emb_size if big else emb_size)\n",
    "        self.l_norm_2 = torch.nn.LayerNorm(new_emb_size if big else emb_size)\n",
    "        self.ffn = torch.nn.Linear(new_emb_size if big else emb_size, new_emb_size if big else emb_size)\n",
    "        self.drop = torch.nn.Dropout(0.1)\n",
    "    def forward(self, inp):\n",
    "        m = self.l_norm_1(inp + self.multihead(inp))\n",
    "        m = self.l_norm_2(m + self.ffn(self.drop(m)).relu())\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layers_num):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.pe = PositionalEncoding(d_model=emb_size)\n",
    "        \n",
    "        # Create a ModuleList to hold the blocks\n",
    "        self.blocks = nn.ModuleList([Block() for _ in range(layers_num)])\n",
    "        \n",
    "        self.f_lin = nn.Linear(emb_size, vocab_size)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        e = self.embedding(inp)\n",
    "        e = self.pe(e)\n",
    "        \n",
    "        # Pass input through all blocks sequentially\n",
    "        for block in self.blocks:\n",
    "            e = block(e)\n",
    "        \n",
    "        r = self.f_lin(self.drop(e))\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model(4).to(device)\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr = learning_rate)\n",
    "\n",
    "model2 = Model(4).to(device)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 10.5971, Val Loss: 9.8715\n",
      "Epoch 2, Train Loss: 9.7959, Val Loss: 9.1473\n",
      "Epoch 3, Train Loss: 9.1520, Val Loss: 8.3703\n",
      "Epoch 4, Train Loss: 8.5235, Val Loss: 7.7712\n",
      "Epoch 5, Train Loss: 7.9393, Val Loss: 7.0743\n",
      "Epoch 6, Train Loss: 7.4053, Val Loss: 6.6063\n",
      "Epoch 7, Train Loss: 6.9145, Val Loss: 6.1506\n",
      "Epoch 8, Train Loss: 6.4591, Val Loss: 5.7286\n",
      "Epoch 9, Train Loss: 6.1084, Val Loss: 5.2531\n",
      "Epoch 10, Train Loss: 5.7283, Val Loss: 4.9085\n",
      "Epoch 11, Train Loss: 5.4267, Val Loss: 4.7048\n",
      "Epoch 12, Train Loss: 5.1455, Val Loss: 4.3199\n",
      "Epoch 13, Train Loss: 4.8981, Val Loss: 4.0747\n",
      "Epoch 14, Train Loss: 4.6916, Val Loss: 3.9109\n",
      "Epoch 15, Train Loss: 4.4852, Val Loss: 3.6799\n",
      "Epoch 16, Train Loss: 4.3014, Val Loss: 3.5497\n",
      "Epoch 17, Train Loss: 4.1153, Val Loss: 3.4317\n",
      "Epoch 18, Train Loss: 3.9702, Val Loss: 3.2563\n",
      "Epoch 19, Train Loss: 3.7739, Val Loss: 3.1678\n",
      "Epoch 20, Train Loss: 3.6532, Val Loss: 2.9592\n",
      "Epoch 21, Train Loss: 3.5112, Val Loss: 2.8590\n",
      "Epoch 22, Train Loss: 3.3910, Val Loss: 2.7609\n",
      "Epoch 23, Train Loss: 3.2465, Val Loss: 2.5926\n",
      "Epoch 24, Train Loss: 3.1411, Val Loss: 2.4876\n",
      "Epoch 25, Train Loss: 3.0256, Val Loss: 2.4324\n",
      "Epoch 26, Train Loss: 2.9088, Val Loss: 2.3815\n",
      "Epoch 27, Train Loss: 2.8515, Val Loss: 2.2127\n",
      "Epoch 28, Train Loss: 2.7023, Val Loss: 2.2319\n",
      "Epoch 29, Train Loss: 2.6020, Val Loss: 2.1175\n",
      "Epoch 30, Train Loss: 2.4771, Val Loss: 1.9583\n",
      "Epoch 1, Train Loss: 10.6320, Val Loss: 9.9905\n",
      "Epoch 2, Train Loss: 9.8427, Val Loss: 9.2052\n",
      "Epoch 3, Train Loss: 9.1739, Val Loss: 8.4773\n",
      "Epoch 4, Train Loss: 8.5740, Val Loss: 7.8070\n",
      "Epoch 5, Train Loss: 7.9981, Val Loss: 7.1854\n",
      "Epoch 6, Train Loss: 7.4386, Val Loss: 6.5776\n",
      "Epoch 7, Train Loss: 6.9495, Val Loss: 6.0440\n",
      "Epoch 8, Train Loss: 6.4800, Val Loss: 5.6462\n",
      "Epoch 9, Train Loss: 6.1025, Val Loss: 5.2635\n",
      "Epoch 10, Train Loss: 5.7707, Val Loss: 4.8640\n",
      "Epoch 11, Train Loss: 5.4131, Val Loss: 4.5677\n",
      "Epoch 12, Train Loss: 5.1596, Val Loss: 4.2937\n",
      "Epoch 13, Train Loss: 4.9002, Val Loss: 4.0821\n",
      "Epoch 14, Train Loss: 4.6830, Val Loss: 3.9730\n",
      "Epoch 15, Train Loss: 4.4785, Val Loss: 3.6577\n",
      "Epoch 16, Train Loss: 4.2811, Val Loss: 3.5695\n",
      "Epoch 17, Train Loss: 4.1221, Val Loss: 3.3808\n",
      "Epoch 18, Train Loss: 3.9656, Val Loss: 3.2991\n",
      "Epoch 19, Train Loss: 3.8106, Val Loss: 3.1011\n",
      "Epoch 20, Train Loss: 3.6882, Val Loss: 2.9867\n",
      "Epoch 21, Train Loss: 3.5427, Val Loss: 2.8980\n",
      "Epoch 22, Train Loss: 3.4123, Val Loss: 2.7296\n",
      "Epoch 23, Train Loss: 3.2820, Val Loss: 2.6633\n",
      "Epoch 24, Train Loss: 3.1614, Val Loss: 2.5442\n",
      "Epoch 25, Train Loss: 3.0759, Val Loss: 2.4732\n",
      "Epoch 26, Train Loss: 2.9397, Val Loss: 2.3730\n",
      "Epoch 27, Train Loss: 2.8476, Val Loss: 2.2754\n",
      "Epoch 28, Train Loss: 2.7279, Val Loss: 2.1835\n",
      "Epoch 29, Train Loss: 2.6245, Val Loss: 2.1020\n",
      "Epoch 30, Train Loss: 2.5184, Val Loss: 2.0168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.990516471862794,\n",
       " 9.205245208740234,\n",
       " 8.477274131774902,\n",
       " 7.806951713562012,\n",
       " 7.185374164581299,\n",
       " 6.5776115417480465,\n",
       " 6.044035148620606,\n",
       " 5.646188163757325,\n",
       " 5.263533020019532,\n",
       " 4.864009761810303,\n",
       " 4.567665672302246,\n",
       " 4.29365758895874,\n",
       " 4.082083034515381,\n",
       " 3.9729843616485594,\n",
       " 3.6576522827148437,\n",
       " 3.5694539546966553,\n",
       " 3.380833387374878,\n",
       " 3.2991469860076905,\n",
       " 3.1010719299316407,\n",
       " 2.9866864681243896,\n",
       " 2.897994565963745,\n",
       " 2.72961802482605,\n",
       " 2.663317012786865,\n",
       " 2.544172430038452,\n",
       " 2.4732322692871094,\n",
       " 2.373012638092041,\n",
       " 2.2753975868225096,\n",
       " 2.18351354598999,\n",
       " 2.102008819580078,\n",
       " 2.0167514801025392]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model1, optimizer1, 30, train_dataloader=train_dataloader_1, val_dataloader=val_dataloader_1)\n",
    "train(model2, optimizer2, 30, train_dataloader=train_dataloader_2, val_dataloader=val_dataloader_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelComparator:\n",
    "    def __init__(self):\n",
    "        self.model1_state = None\n",
    "        self.model2_state = None\n",
    "\n",
    "    def save_model_states(self, model1, model2):\n",
    "        self.model1_state = self._get_model_state(model1)\n",
    "        self.model2_state = self._get_model_state(model2)\n",
    "        print(\"Model states saved successfully.\")\n",
    "\n",
    "    def _get_model_state(self, model):\n",
    "        return {name: param.clone().detach().cpu() for name, param in model.named_parameters()}\n",
    "\n",
    "    def models_match(self, check_model1, check_model2):\n",
    "        if self.model1_state is None or self.model2_state is None:\n",
    "            raise ValueError(\"Model states have not been saved. Call save_model_states() first.\")\n",
    "\n",
    "        match1 = self._compare_model(check_model1, self.model1_state)\n",
    "        match2 = self._compare_model(check_model2, self.model2_state)\n",
    "        \n",
    "        return match1 and match2\n",
    "\n",
    "    def _compare_model(self, model, saved_state):\n",
    "        current_state = self._get_model_state(model)\n",
    "        \n",
    "        if current_state.keys() != saved_state.keys():\n",
    "            print(f\"Model has different parameter structure than the saved state.\")\n",
    "            return False\n",
    "\n",
    "        for name, param in current_state.items():\n",
    "            if not torch.allclose(param, saved_state[name]):\n",
    "                print(f\"Mismatch in parameter: {name}\")\n",
    "                return False\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model states saved successfully.\n"
     ]
    }
   ],
   "source": [
    "comparator = ModelComparator()\n",
    "comparator.save_model_states(model1, model2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models still match their original states\n"
     ]
    }
   ],
   "source": [
    "if comparator.models_match(model1, model2):\n",
    "    print(\"Both models still match their original states\")\n",
    "else:\n",
    "    print(\"At least one model has changed from its original state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 10.4963, Val Loss: 9.6975\n",
      "Epoch 2, Train Loss: 9.6304, Val Loss: 8.7755\n",
      "Epoch 3, Train Loss: 8.9549, Val Loss: 8.0772\n",
      "Epoch 4, Train Loss: 8.3216, Val Loss: 7.3435\n",
      "Epoch 5, Train Loss: 7.7643, Val Loss: 6.7570\n",
      "Epoch 6, Train Loss: 7.2245, Val Loss: 6.2625\n",
      "Epoch 7, Train Loss: 6.7792, Val Loss: 5.7913\n",
      "Epoch 8, Train Loss: 6.3390, Val Loss: 5.4332\n",
      "Epoch 9, Train Loss: 5.9169, Val Loss: 5.0178\n",
      "Epoch 10, Train Loss: 5.6246, Val Loss: 4.6711\n",
      "Epoch 11, Train Loss: 5.3175, Val Loss: 4.4280\n",
      "Epoch 12, Train Loss: 5.0401, Val Loss: 4.1109\n",
      "Epoch 13, Train Loss: 4.8161, Val Loss: 3.9098\n",
      "Epoch 14, Train Loss: 4.6268, Val Loss: 3.7041\n",
      "Epoch 15, Train Loss: 4.4287, Val Loss: 3.5764\n",
      "Epoch 16, Train Loss: 4.2273, Val Loss: 3.4637\n",
      "Epoch 17, Train Loss: 4.0779, Val Loss: 3.2754\n",
      "Epoch 18, Train Loss: 3.9639, Val Loss: 3.1761\n",
      "Epoch 19, Train Loss: 3.8049, Val Loss: 2.9698\n",
      "Epoch 20, Train Loss: 3.7039, Val Loss: 2.9309\n",
      "Epoch 21, Train Loss: 3.5601, Val Loss: 2.7570\n",
      "Epoch 22, Train Loss: 3.4208, Val Loss: 2.6873\n",
      "Epoch 23, Train Loss: 3.2969, Val Loss: 2.5702\n",
      "Epoch 24, Train Loss: 3.2097, Val Loss: 2.4341\n",
      "Epoch 25, Train Loss: 3.1069, Val Loss: 2.3498\n",
      "Epoch 26, Train Loss: 2.9973, Val Loss: 2.3051\n",
      "Epoch 27, Train Loss: 2.9075, Val Loss: 2.2031\n",
      "Epoch 28, Train Loss: 2.8138, Val Loss: 2.0925\n",
      "Epoch 29, Train Loss: 2.7087, Val Loss: 2.0515\n",
      "Epoch 30, Train Loss: 2.6192, Val Loss: 1.9373\n",
      "Epoch 31, Train Loss: 2.5625, Val Loss: 1.8762\n",
      "Epoch 32, Train Loss: 2.4673, Val Loss: 1.8569\n",
      "Epoch 33, Train Loss: 2.3756, Val Loss: 1.8365\n",
      "Epoch 34, Train Loss: 2.3104, Val Loss: 1.7108\n",
      "Epoch 35, Train Loss: 2.2486, Val Loss: 1.5973\n",
      "Epoch 36, Train Loss: 2.1557, Val Loss: 1.5922\n",
      "Epoch 37, Train Loss: 2.1037, Val Loss: 1.5745\n",
      "Epoch 38, Train Loss: 2.0457, Val Loss: 1.5017\n",
      "Epoch 39, Train Loss: 1.9860, Val Loss: 1.4512\n",
      "Epoch 40, Train Loss: 1.9228, Val Loss: 1.3658\n",
      "Epoch 41, Train Loss: 1.8649, Val Loss: 1.3707\n",
      "Epoch 42, Train Loss: 1.7953, Val Loss: 1.2842\n",
      "Epoch 43, Train Loss: 1.7659, Val Loss: 1.2362\n",
      "Epoch 44, Train Loss: 1.7080, Val Loss: 1.1765\n",
      "Epoch 45, Train Loss: 1.6527, Val Loss: 1.1921\n",
      "Epoch 46, Train Loss: 1.5966, Val Loss: 1.1046\n",
      "Epoch 47, Train Loss: 1.5552, Val Loss: 1.1221\n",
      "Epoch 48, Train Loss: 1.5155, Val Loss: 1.1027\n",
      "Epoch 49, Train Loss: 1.4643, Val Loss: 1.0172\n",
      "Epoch 50, Train Loss: 1.4216, Val Loss: 0.9918\n",
      "Epoch 51, Train Loss: 1.3777, Val Loss: 0.9704\n",
      "Epoch 52, Train Loss: 1.3478, Val Loss: 0.9221\n",
      "Epoch 53, Train Loss: 1.3113, Val Loss: 0.9940\n",
      "Epoch 54, Train Loss: 1.2908, Val Loss: 0.9325\n",
      "Epoch 55, Train Loss: 1.2068, Val Loss: 0.9443\n",
      "Epoch 56, Train Loss: 1.1955, Val Loss: 0.8765\n",
      "Epoch 57, Train Loss: 1.1597, Val Loss: 0.8067\n",
      "Epoch 58, Train Loss: 1.1444, Val Loss: 0.8123\n",
      "Epoch 59, Train Loss: 1.1141, Val Loss: 0.7610\n",
      "Epoch 60, Train Loss: 1.0930, Val Loss: 0.7896\n",
      "Epoch 61, Train Loss: 1.0750, Val Loss: 0.7289\n",
      "Epoch 62, Train Loss: 1.0451, Val Loss: 0.7005\n",
      "Epoch 63, Train Loss: 1.0031, Val Loss: 0.6667\n",
      "Epoch 64, Train Loss: 0.9753, Val Loss: 0.6983\n",
      "Epoch 65, Train Loss: 0.9518, Val Loss: 0.6638\n",
      "Epoch 66, Train Loss: 0.9207, Val Loss: 0.6487\n",
      "Epoch 67, Train Loss: 0.9219, Val Loss: 0.6792\n",
      "Epoch 68, Train Loss: 0.8912, Val Loss: 0.5952\n",
      "Epoch 69, Train Loss: 0.8276, Val Loss: 0.6353\n",
      "Epoch 70, Train Loss: 0.8594, Val Loss: 0.6057\n"
     ]
    }
   ],
   "source": [
    "scratch = Model(8)\n",
    "optim_s = torch.optim.Adam(params=scratch.parameters(), lr=learning_rate)\n",
    "scratch_curve = train(scratch, optim_s, 70, train_dataloader=train_dataloader, val_dataloader=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Stacking Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedModel(torch.nn.Module):\n",
    "    def __init__(self, models, num_models, same=False):\n",
    "        super(StackedModel, self).__init__()\n",
    "        \n",
    "        if len(models) != num_models:\n",
    "            raise ValueError(f\"Expected {num_models} models, but got {len(models)}\")\n",
    "        \n",
    "        # Randomize the order of models\n",
    "        random.shuffle(models)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.embedding = copy.deepcopy(models[0].embedding)\n",
    "        self.pe = copy.deepcopy(models[0].pe)\n",
    "        \n",
    "        # Stack the blocks\n",
    "        self.blocks = torch.nn.ModuleList()\n",
    "        for model in models:\n",
    "            self.blocks.extend(copy.deepcopy(model.blocks))\n",
    "        \n",
    "        if same:\n",
    "            model_rand = random.choice(models)\n",
    "            self.embedding = copy.deepcopy(model_rand.embedding)\n",
    "            self.pe = copy.deepcopy(model_rand.pe)\n",
    "            self.f_lin = copy.deepcopy(model_rand.f_lin)\n",
    "        else:\n",
    "            self.f_lin = copy.deepcopy(models[-1].f_lin)\n",
    "        \n",
    "        self.drop = torch.nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        e = self.embedding(inp)\n",
    "        e = self.pe(e)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            e = block(e)\n",
    "        \n",
    "        r = self.f_lin(self.drop(e))\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StackedModel3(torch.nn.Module):\n",
    "#     def __init__(self, model1, model2, model3, same=False):\n",
    "#         super(StackedModel3, self).__init__()\n",
    "#         self.embedding = copy.deepcopy(model1.embedding)\n",
    "#         self.pe = PositionalEncoding(d_model=128)\n",
    "\n",
    "#         # Stack the blocks\n",
    "#         self.block1 = copy.deepcopy(model1.block1)\n",
    "#         self.block2 = copy.deepcopy(model1.block2)\n",
    "#         self.block3 = copy.deepcopy(model1.block3)\n",
    "#         self.block4 = copy.deepcopy(model1.block4)\n",
    "        \n",
    "#         self.block5 = copy.deepcopy(model2.block1)\n",
    "#         self.block6 = copy.deepcopy(model2.block2)\n",
    "#         self.block7 = copy.deepcopy(model2.block3)\n",
    "#         self.block8 = copy.deepcopy(model2.block4)\n",
    "        \n",
    "#         self.block9 = copy.deepcopy(model3.block1)\n",
    "#         self.block10 = copy.deepcopy(model3.block2)\n",
    "#         self.block11 = copy.deepcopy(model3.block3)\n",
    "#         self.block12 = copy.deepcopy(model3.block4)\n",
    "\n",
    "#         if same:\n",
    "#             self.final_linear = copy.deepcopy(model1.f_lin)\n",
    "#         else: \n",
    "#             self.final_linear = copy.deepcopy(model3.f_lin)\n",
    "#         self.drop = torch.nn.Dropout(0.1)\n",
    "\n",
    "#     def forward(self, inp):\n",
    "#         e = self.embedding(inp)\n",
    "#         e = self.pe(e)\n",
    "        \n",
    "        \n",
    "#         e = self.block1(e)\n",
    "#         e = self.block2(e)\n",
    "#         e = self.block3(e)\n",
    "#         e = self.block4(e)\n",
    "        \n",
    "#         e = self.block5(e)\n",
    "#         e = self.block6(e)\n",
    "#         e = self.block7(e)\n",
    "#         e = self.block8(e)\n",
    "        \n",
    "#         e = self.block9(e)\n",
    "#         e = self.block10(e)\n",
    "#         e = self.block11(e)\n",
    "#         e = self.block12(e)\n",
    "\n",
    "#         r = self.final_linear(self.drop(e))\n",
    "#         return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StackedModel4(torch.nn.Module):\n",
    "#     def __init__(self, model1, model2, model3, model4, emb_size, vocab_size):\n",
    "#         super(StackedModel4, self).__init__()\n",
    "#         self.embedding = model1.embedding\n",
    "#         self.pe = model1.pe\n",
    "\n",
    "#         # Stack the blocks\n",
    "#         self.block1 = copy.deepcopy(model1.block1)\n",
    "#         self.block2 = copy.deepcopy(model1.block2)\n",
    "#         self.block3 = copy.deepcopy(model1.block3)\n",
    "#         self.block4 = copy.deepcopy(model1.block4)\n",
    "#         self.block5 = copy.deepcopy(model2.block1)\n",
    "#         self.block6 = copy.deepcopy(model2.block2)\n",
    "#         self.block7 = copy.deepcopy(model2.block3)\n",
    "#         self.block8 = copy.deepcopy(model2.block4)\n",
    "#         self.block9 = copy.deepcopy(model3.block1)\n",
    "#         self.block10 = copy.deepcopy(model3.block2)\n",
    "#         self.block11 = copy.deepcopy(model3.block3)\n",
    "#         self.block12 = copy.deepcopy(model3.block4)\n",
    "#         self.block13 = copy.deepcopy(model4.block1)\n",
    "#         self.block14 = copy.deepcopy(model4.block2)\n",
    "#         self.block15 = copy.deepcopy(model4.block3)\n",
    "#         self.block16 = copy.deepcopy(model4.block4)\n",
    "        \n",
    "\n",
    "       \n",
    "#         self.final_linear = model4.f_lin\n",
    "#         self.drop = torch.nn.Dropout(0.1)\n",
    "\n",
    "#     def forward(self, inp):\n",
    "#         e = self.embedding(inp)\n",
    "#         e = self.pe(e)\n",
    "        \n",
    "        \n",
    "#         e = self.block1(e)\n",
    "#         e = self.block2(e)\n",
    "#         e = self.block3(e)\n",
    "#         e = self.block4(e)\n",
    "#         e = self.block5(e)\n",
    "#         e = self.block6(e)\n",
    "#         e = self.block7(e)\n",
    "#         e = self.block8(e)\n",
    "#         e = self.block9(e)\n",
    "#         e = self.block10(e)\n",
    "#         e = self.block11(e)\n",
    "#         e = self.block12(e)\n",
    "#         e = self.block13(e)\n",
    "#         e = self.block14(e)\n",
    "#         e = self.block15(e)\n",
    "#         e = self.block16(e)\n",
    "\n",
    "#         r = self.final_linear(self.drop(e))\n",
    "#         return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedModelAVG(torch.nn.Module):\n",
    "    def __init__(self, models, num_models):\n",
    "        super(StackedModelAVG, self).__init__()\n",
    "        \n",
    "        if len(models) != num_models:\n",
    "            raise ValueError(f\"Expected {num_models} models, but got {len(models)}\")\n",
    "        \n",
    "        # Randomize the order of models\n",
    "        random.shuffle(models)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, emb_size)\n",
    "        \n",
    "        embedding_weights = [copy.deepcopy(model.embedding.weight) for model in models]\n",
    "        avg_embedding_weight = sum(embedding_weights) / num_models\n",
    "\n",
    "        self.embedding.weight = torch.nn.Parameter(avg_embedding_weight, requires_grad=True)\n",
    "\n",
    "        self.pe = copy.deepcopy(models[0].pe)\n",
    "        \n",
    "        # Stack the blocks\n",
    "        self.blocks = torch.nn.ModuleList()\n",
    "        for model in models:\n",
    "            self.blocks.extend(copy.deepcopy(model.blocks))\n",
    "            \n",
    "        self.f_lin = torch.nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "        linear_weights = [copy.deepcopy(model.f_lin.weight) for model in models]\n",
    "        linear_biases = [copy.deepcopy(model.f_lin.bias) for model in models]\n",
    "\n",
    "        avg_linear_weight = sum(linear_weights) / num_models\n",
    "        avg_linear_bias = sum(linear_biases) / num_models\n",
    "\n",
    "        self.f_lin.weight = torch.nn.Parameter(avg_linear_weight, requires_grad=True)\n",
    "        self.f_lin.bias = torch.nn.Parameter(avg_linear_bias, requires_grad=True)\n",
    "        \n",
    "        self.drop = torch.nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        e = self.embedding(inp)\n",
    "        e = self.pe(e)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            e = block(e)\n",
    "        \n",
    "        r = self.f_lin(self.drop(e))\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_parameters(large_model_params, small_model_params, layer_range, prefix, offset=0):\n",
    "    for layer_idx in layer_range:\n",
    "        large_layer_idx = layer_idx + offset\n",
    "        small_layer_idx = layer_idx\n",
    "\n",
    "        # List of parameter types in each block to compare\n",
    "        param_types = [\n",
    "            'multihead.head1.tril', 'multihead.head1.k.weight', 'multihead.head1.q.weight', 'multihead.head1.v.weight',\n",
    "            'multihead.head2.tril', 'multihead.head2.k.weight', 'multihead.head2.q.weight', 'multihead.head2.v.weight',\n",
    "            'multihead.mh_lin.weight',\n",
    "            'l_norm_1.weight', 'l_norm_1.bias',\n",
    "            'l_norm_2.weight', 'l_norm_2.bias',\n",
    "            'ffn.weight', 'ffn.bias'\n",
    "        ]\n",
    "\n",
    "        for param_type in param_types:\n",
    "            large_param_name = f'blocks.{large_layer_idx}.{param_type}'\n",
    "            small_param_name = f'blocks.{small_layer_idx}.{param_type}'\n",
    "\n",
    "            if large_param_name not in large_model_params:\n",
    "                print(f\"Key {large_param_name} not found in large_model_params.\")\n",
    "                continue\n",
    "            if small_param_name not in small_model_params:\n",
    "                print(f\"Key {small_param_name} not found in small_model_params.\")\n",
    "                continue\n",
    "\n",
    "            # Move parameters to the same device for comparison\n",
    "            param_large = large_model_params[large_param_name].to(device)\n",
    "            param_small = small_model_params[small_param_name].to(device)\n",
    "\n",
    "            if not torch.equal(param_large, param_small):\n",
    "                print(f\"Parameter {param_type} does not match in {prefix} at layer {large_layer_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small model 1 parameters: dict_keys(['embedding.weight', 'pe.pe', 'blocks.0.multihead.head1.tril', 'blocks.0.multihead.head1.k.weight', 'blocks.0.multihead.head1.q.weight', 'blocks.0.multihead.head1.v.weight', 'blocks.0.multihead.head2.tril', 'blocks.0.multihead.head2.k.weight', 'blocks.0.multihead.head2.q.weight', 'blocks.0.multihead.head2.v.weight', 'blocks.0.multihead.mh_lin.weight', 'blocks.0.l_norm_1.weight', 'blocks.0.l_norm_1.bias', 'blocks.0.l_norm_2.weight', 'blocks.0.l_norm_2.bias', 'blocks.0.ffn.weight', 'blocks.0.ffn.bias', 'blocks.1.multihead.head1.tril', 'blocks.1.multihead.head1.k.weight', 'blocks.1.multihead.head1.q.weight', 'blocks.1.multihead.head1.v.weight', 'blocks.1.multihead.head2.tril', 'blocks.1.multihead.head2.k.weight', 'blocks.1.multihead.head2.q.weight', 'blocks.1.multihead.head2.v.weight', 'blocks.1.multihead.mh_lin.weight', 'blocks.1.l_norm_1.weight', 'blocks.1.l_norm_1.bias', 'blocks.1.l_norm_2.weight', 'blocks.1.l_norm_2.bias', 'blocks.1.ffn.weight', 'blocks.1.ffn.bias', 'blocks.2.multihead.head1.tril', 'blocks.2.multihead.head1.k.weight', 'blocks.2.multihead.head1.q.weight', 'blocks.2.multihead.head1.v.weight', 'blocks.2.multihead.head2.tril', 'blocks.2.multihead.head2.k.weight', 'blocks.2.multihead.head2.q.weight', 'blocks.2.multihead.head2.v.weight', 'blocks.2.multihead.mh_lin.weight', 'blocks.2.l_norm_1.weight', 'blocks.2.l_norm_1.bias', 'blocks.2.l_norm_2.weight', 'blocks.2.l_norm_2.bias', 'blocks.2.ffn.weight', 'blocks.2.ffn.bias', 'blocks.3.multihead.head1.tril', 'blocks.3.multihead.head1.k.weight', 'blocks.3.multihead.head1.q.weight', 'blocks.3.multihead.head1.v.weight', 'blocks.3.multihead.head2.tril', 'blocks.3.multihead.head2.k.weight', 'blocks.3.multihead.head2.q.weight', 'blocks.3.multihead.head2.v.weight', 'blocks.3.multihead.mh_lin.weight', 'blocks.3.l_norm_1.weight', 'blocks.3.l_norm_1.bias', 'blocks.3.l_norm_2.weight', 'blocks.3.l_norm_2.bias', 'blocks.3.ffn.weight', 'blocks.3.ffn.bias', 'f_lin.weight', 'f_lin.bias'])\n",
      "Large model parameters: dict_keys(['embedding.weight', 'pe.pe', 'blocks.0.multihead.head1.tril', 'blocks.0.multihead.head1.k.weight', 'blocks.0.multihead.head1.q.weight', 'blocks.0.multihead.head1.v.weight', 'blocks.0.multihead.head2.tril', 'blocks.0.multihead.head2.k.weight', 'blocks.0.multihead.head2.q.weight', 'blocks.0.multihead.head2.v.weight', 'blocks.0.multihead.mh_lin.weight', 'blocks.0.l_norm_1.weight', 'blocks.0.l_norm_1.bias', 'blocks.0.l_norm_2.weight', 'blocks.0.l_norm_2.bias', 'blocks.0.ffn.weight', 'blocks.0.ffn.bias', 'blocks.1.multihead.head1.tril', 'blocks.1.multihead.head1.k.weight', 'blocks.1.multihead.head1.q.weight', 'blocks.1.multihead.head1.v.weight', 'blocks.1.multihead.head2.tril', 'blocks.1.multihead.head2.k.weight', 'blocks.1.multihead.head2.q.weight', 'blocks.1.multihead.head2.v.weight', 'blocks.1.multihead.mh_lin.weight', 'blocks.1.l_norm_1.weight', 'blocks.1.l_norm_1.bias', 'blocks.1.l_norm_2.weight', 'blocks.1.l_norm_2.bias', 'blocks.1.ffn.weight', 'blocks.1.ffn.bias', 'blocks.2.multihead.head1.tril', 'blocks.2.multihead.head1.k.weight', 'blocks.2.multihead.head1.q.weight', 'blocks.2.multihead.head1.v.weight', 'blocks.2.multihead.head2.tril', 'blocks.2.multihead.head2.k.weight', 'blocks.2.multihead.head2.q.weight', 'blocks.2.multihead.head2.v.weight', 'blocks.2.multihead.mh_lin.weight', 'blocks.2.l_norm_1.weight', 'blocks.2.l_norm_1.bias', 'blocks.2.l_norm_2.weight', 'blocks.2.l_norm_2.bias', 'blocks.2.ffn.weight', 'blocks.2.ffn.bias', 'blocks.3.multihead.head1.tril', 'blocks.3.multihead.head1.k.weight', 'blocks.3.multihead.head1.q.weight', 'blocks.3.multihead.head1.v.weight', 'blocks.3.multihead.head2.tril', 'blocks.3.multihead.head2.k.weight', 'blocks.3.multihead.head2.q.weight', 'blocks.3.multihead.head2.v.weight', 'blocks.3.multihead.mh_lin.weight', 'blocks.3.l_norm_1.weight', 'blocks.3.l_norm_1.bias', 'blocks.3.l_norm_2.weight', 'blocks.3.l_norm_2.bias', 'blocks.3.ffn.weight', 'blocks.3.ffn.bias', 'blocks.4.multihead.head1.tril', 'blocks.4.multihead.head1.k.weight', 'blocks.4.multihead.head1.q.weight', 'blocks.4.multihead.head1.v.weight', 'blocks.4.multihead.head2.tril', 'blocks.4.multihead.head2.k.weight', 'blocks.4.multihead.head2.q.weight', 'blocks.4.multihead.head2.v.weight', 'blocks.4.multihead.mh_lin.weight', 'blocks.4.l_norm_1.weight', 'blocks.4.l_norm_1.bias', 'blocks.4.l_norm_2.weight', 'blocks.4.l_norm_2.bias', 'blocks.4.ffn.weight', 'blocks.4.ffn.bias', 'blocks.5.multihead.head1.tril', 'blocks.5.multihead.head1.k.weight', 'blocks.5.multihead.head1.q.weight', 'blocks.5.multihead.head1.v.weight', 'blocks.5.multihead.head2.tril', 'blocks.5.multihead.head2.k.weight', 'blocks.5.multihead.head2.q.weight', 'blocks.5.multihead.head2.v.weight', 'blocks.5.multihead.mh_lin.weight', 'blocks.5.l_norm_1.weight', 'blocks.5.l_norm_1.bias', 'blocks.5.l_norm_2.weight', 'blocks.5.l_norm_2.bias', 'blocks.5.ffn.weight', 'blocks.5.ffn.bias', 'blocks.6.multihead.head1.tril', 'blocks.6.multihead.head1.k.weight', 'blocks.6.multihead.head1.q.weight', 'blocks.6.multihead.head1.v.weight', 'blocks.6.multihead.head2.tril', 'blocks.6.multihead.head2.k.weight', 'blocks.6.multihead.head2.q.weight', 'blocks.6.multihead.head2.v.weight', 'blocks.6.multihead.mh_lin.weight', 'blocks.6.l_norm_1.weight', 'blocks.6.l_norm_1.bias', 'blocks.6.l_norm_2.weight', 'blocks.6.l_norm_2.bias', 'blocks.6.ffn.weight', 'blocks.6.ffn.bias', 'blocks.7.multihead.head1.tril', 'blocks.7.multihead.head1.k.weight', 'blocks.7.multihead.head1.q.weight', 'blocks.7.multihead.head1.v.weight', 'blocks.7.multihead.head2.tril', 'blocks.7.multihead.head2.k.weight', 'blocks.7.multihead.head2.q.weight', 'blocks.7.multihead.head2.v.weight', 'blocks.7.multihead.mh_lin.weight', 'blocks.7.l_norm_1.weight', 'blocks.7.l_norm_1.bias', 'blocks.7.l_norm_2.weight', 'blocks.7.l_norm_2.bias', 'blocks.7.ffn.weight', 'blocks.7.ffn.bias', 'f_lin.weight', 'f_lin.bias'])\n"
     ]
    }
   ],
   "source": [
    "small_model_1_params = {k: v.to(device) for k, v in model1.state_dict().items()}\n",
    "\n",
    "print(\"Small model 1 parameters:\", small_model_1_params.keys())\n",
    "\n",
    "stacked_model = StackedModel([model1, model2], num_models = 2)\n",
    "optimizer_stacked = torch.optim.Adam(stacked_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Extract parameters from the models and move to the same device\n",
    "large_model_params = {k: v.to(device) for k, v in stacked_model.state_dict().items()}\n",
    "small_model_1_params = {k: v.to(device) for k, v in model1.state_dict().items()}\n",
    "small_model_2_params = {k: v.to(device) for k, v in model2.state_dict().items()}\n",
    "print(\"Large model parameters:\", large_model_params.keys())\n",
    "\n",
    "# Compare the first 4 layers with model1\n",
    "compare_parameters(large_model_params, small_model_1_params, range(4), 'model1')\n",
    "\n",
    "# Compare the next 4 layers with model2\n",
    "compare_parameters(large_model_params, small_model_2_params, range(4), 'model2', offset=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Model Parameters:\n",
      "dict_keys(['embedding.weight', 'pe.pe', 'blocks.0.multihead.head1.tril', 'blocks.0.multihead.head1.k.weight', 'blocks.0.multihead.head1.q.weight', 'blocks.0.multihead.head1.v.weight', 'blocks.0.multihead.head2.tril', 'blocks.0.multihead.head2.k.weight', 'blocks.0.multihead.head2.q.weight', 'blocks.0.multihead.head2.v.weight', 'blocks.0.multihead.mh_lin.weight', 'blocks.0.l_norm_1.weight', 'blocks.0.l_norm_1.bias', 'blocks.0.l_norm_2.weight', 'blocks.0.l_norm_2.bias', 'blocks.0.ffn.weight', 'blocks.0.ffn.bias', 'blocks.1.multihead.head1.tril', 'blocks.1.multihead.head1.k.weight', 'blocks.1.multihead.head1.q.weight', 'blocks.1.multihead.head1.v.weight', 'blocks.1.multihead.head2.tril', 'blocks.1.multihead.head2.k.weight', 'blocks.1.multihead.head2.q.weight', 'blocks.1.multihead.head2.v.weight', 'blocks.1.multihead.mh_lin.weight', 'blocks.1.l_norm_1.weight', 'blocks.1.l_norm_1.bias', 'blocks.1.l_norm_2.weight', 'blocks.1.l_norm_2.bias', 'blocks.1.ffn.weight', 'blocks.1.ffn.bias', 'blocks.2.multihead.head1.tril', 'blocks.2.multihead.head1.k.weight', 'blocks.2.multihead.head1.q.weight', 'blocks.2.multihead.head1.v.weight', 'blocks.2.multihead.head2.tril', 'blocks.2.multihead.head2.k.weight', 'blocks.2.multihead.head2.q.weight', 'blocks.2.multihead.head2.v.weight', 'blocks.2.multihead.mh_lin.weight', 'blocks.2.l_norm_1.weight', 'blocks.2.l_norm_1.bias', 'blocks.2.l_norm_2.weight', 'blocks.2.l_norm_2.bias', 'blocks.2.ffn.weight', 'blocks.2.ffn.bias', 'blocks.3.multihead.head1.tril', 'blocks.3.multihead.head1.k.weight', 'blocks.3.multihead.head1.q.weight', 'blocks.3.multihead.head1.v.weight', 'blocks.3.multihead.head2.tril', 'blocks.3.multihead.head2.k.weight', 'blocks.3.multihead.head2.q.weight', 'blocks.3.multihead.head2.v.weight', 'blocks.3.multihead.mh_lin.weight', 'blocks.3.l_norm_1.weight', 'blocks.3.l_norm_1.bias', 'blocks.3.l_norm_2.weight', 'blocks.3.l_norm_2.bias', 'blocks.3.ffn.weight', 'blocks.3.ffn.bias', 'blocks.4.multihead.head1.tril', 'blocks.4.multihead.head1.k.weight', 'blocks.4.multihead.head1.q.weight', 'blocks.4.multihead.head1.v.weight', 'blocks.4.multihead.head2.tril', 'blocks.4.multihead.head2.k.weight', 'blocks.4.multihead.head2.q.weight', 'blocks.4.multihead.head2.v.weight', 'blocks.4.multihead.mh_lin.weight', 'blocks.4.l_norm_1.weight', 'blocks.4.l_norm_1.bias', 'blocks.4.l_norm_2.weight', 'blocks.4.l_norm_2.bias', 'blocks.4.ffn.weight', 'blocks.4.ffn.bias', 'blocks.5.multihead.head1.tril', 'blocks.5.multihead.head1.k.weight', 'blocks.5.multihead.head1.q.weight', 'blocks.5.multihead.head1.v.weight', 'blocks.5.multihead.head2.tril', 'blocks.5.multihead.head2.k.weight', 'blocks.5.multihead.head2.q.weight', 'blocks.5.multihead.head2.v.weight', 'blocks.5.multihead.mh_lin.weight', 'blocks.5.l_norm_1.weight', 'blocks.5.l_norm_1.bias', 'blocks.5.l_norm_2.weight', 'blocks.5.l_norm_2.bias', 'blocks.5.ffn.weight', 'blocks.5.ffn.bias', 'blocks.6.multihead.head1.tril', 'blocks.6.multihead.head1.k.weight', 'blocks.6.multihead.head1.q.weight', 'blocks.6.multihead.head1.v.weight', 'blocks.6.multihead.head2.tril', 'blocks.6.multihead.head2.k.weight', 'blocks.6.multihead.head2.q.weight', 'blocks.6.multihead.head2.v.weight', 'blocks.6.multihead.mh_lin.weight', 'blocks.6.l_norm_1.weight', 'blocks.6.l_norm_1.bias', 'blocks.6.l_norm_2.weight', 'blocks.6.l_norm_2.bias', 'blocks.6.ffn.weight', 'blocks.6.ffn.bias', 'blocks.7.multihead.head1.tril', 'blocks.7.multihead.head1.k.weight', 'blocks.7.multihead.head1.q.weight', 'blocks.7.multihead.head1.v.weight', 'blocks.7.multihead.head2.tril', 'blocks.7.multihead.head2.k.weight', 'blocks.7.multihead.head2.q.weight', 'blocks.7.multihead.head2.v.weight', 'blocks.7.multihead.mh_lin.weight', 'blocks.7.l_norm_1.weight', 'blocks.7.l_norm_1.bias', 'blocks.7.l_norm_2.weight', 'blocks.7.l_norm_2.bias', 'blocks.7.ffn.weight', 'blocks.7.ffn.bias', 'f_lin.weight', 'f_lin.bias'])\n",
      "\n",
      "Small Model 1 Parameters:\n",
      "dict_keys(['embedding.weight', 'pe.pe', 'blocks.0.multihead.head1.tril', 'blocks.0.multihead.head1.k.weight', 'blocks.0.multihead.head1.q.weight', 'blocks.0.multihead.head1.v.weight', 'blocks.0.multihead.head2.tril', 'blocks.0.multihead.head2.k.weight', 'blocks.0.multihead.head2.q.weight', 'blocks.0.multihead.head2.v.weight', 'blocks.0.multihead.mh_lin.weight', 'blocks.0.l_norm_1.weight', 'blocks.0.l_norm_1.bias', 'blocks.0.l_norm_2.weight', 'blocks.0.l_norm_2.bias', 'blocks.0.ffn.weight', 'blocks.0.ffn.bias', 'blocks.1.multihead.head1.tril', 'blocks.1.multihead.head1.k.weight', 'blocks.1.multihead.head1.q.weight', 'blocks.1.multihead.head1.v.weight', 'blocks.1.multihead.head2.tril', 'blocks.1.multihead.head2.k.weight', 'blocks.1.multihead.head2.q.weight', 'blocks.1.multihead.head2.v.weight', 'blocks.1.multihead.mh_lin.weight', 'blocks.1.l_norm_1.weight', 'blocks.1.l_norm_1.bias', 'blocks.1.l_norm_2.weight', 'blocks.1.l_norm_2.bias', 'blocks.1.ffn.weight', 'blocks.1.ffn.bias', 'blocks.2.multihead.head1.tril', 'blocks.2.multihead.head1.k.weight', 'blocks.2.multihead.head1.q.weight', 'blocks.2.multihead.head1.v.weight', 'blocks.2.multihead.head2.tril', 'blocks.2.multihead.head2.k.weight', 'blocks.2.multihead.head2.q.weight', 'blocks.2.multihead.head2.v.weight', 'blocks.2.multihead.mh_lin.weight', 'blocks.2.l_norm_1.weight', 'blocks.2.l_norm_1.bias', 'blocks.2.l_norm_2.weight', 'blocks.2.l_norm_2.bias', 'blocks.2.ffn.weight', 'blocks.2.ffn.bias', 'blocks.3.multihead.head1.tril', 'blocks.3.multihead.head1.k.weight', 'blocks.3.multihead.head1.q.weight', 'blocks.3.multihead.head1.v.weight', 'blocks.3.multihead.head2.tril', 'blocks.3.multihead.head2.k.weight', 'blocks.3.multihead.head2.q.weight', 'blocks.3.multihead.head2.v.weight', 'blocks.3.multihead.mh_lin.weight', 'blocks.3.l_norm_1.weight', 'blocks.3.l_norm_1.bias', 'blocks.3.l_norm_2.weight', 'blocks.3.l_norm_2.bias', 'blocks.3.ffn.weight', 'blocks.3.ffn.bias', 'f_lin.weight', 'f_lin.bias'])\n",
      "\n",
      "Small Model 2 Parameters:\n",
      "dict_keys(['embedding.weight', 'pe.pe', 'blocks.0.multihead.head1.tril', 'blocks.0.multihead.head1.k.weight', 'blocks.0.multihead.head1.q.weight', 'blocks.0.multihead.head1.v.weight', 'blocks.0.multihead.head2.tril', 'blocks.0.multihead.head2.k.weight', 'blocks.0.multihead.head2.q.weight', 'blocks.0.multihead.head2.v.weight', 'blocks.0.multihead.mh_lin.weight', 'blocks.0.l_norm_1.weight', 'blocks.0.l_norm_1.bias', 'blocks.0.l_norm_2.weight', 'blocks.0.l_norm_2.bias', 'blocks.0.ffn.weight', 'blocks.0.ffn.bias', 'blocks.1.multihead.head1.tril', 'blocks.1.multihead.head1.k.weight', 'blocks.1.multihead.head1.q.weight', 'blocks.1.multihead.head1.v.weight', 'blocks.1.multihead.head2.tril', 'blocks.1.multihead.head2.k.weight', 'blocks.1.multihead.head2.q.weight', 'blocks.1.multihead.head2.v.weight', 'blocks.1.multihead.mh_lin.weight', 'blocks.1.l_norm_1.weight', 'blocks.1.l_norm_1.bias', 'blocks.1.l_norm_2.weight', 'blocks.1.l_norm_2.bias', 'blocks.1.ffn.weight', 'blocks.1.ffn.bias', 'blocks.2.multihead.head1.tril', 'blocks.2.multihead.head1.k.weight', 'blocks.2.multihead.head1.q.weight', 'blocks.2.multihead.head1.v.weight', 'blocks.2.multihead.head2.tril', 'blocks.2.multihead.head2.k.weight', 'blocks.2.multihead.head2.q.weight', 'blocks.2.multihead.head2.v.weight', 'blocks.2.multihead.mh_lin.weight', 'blocks.2.l_norm_1.weight', 'blocks.2.l_norm_1.bias', 'blocks.2.l_norm_2.weight', 'blocks.2.l_norm_2.bias', 'blocks.2.ffn.weight', 'blocks.2.ffn.bias', 'blocks.3.multihead.head1.tril', 'blocks.3.multihead.head1.k.weight', 'blocks.3.multihead.head1.q.weight', 'blocks.3.multihead.head1.v.weight', 'blocks.3.multihead.head2.tril', 'blocks.3.multihead.head2.k.weight', 'blocks.3.multihead.head2.q.weight', 'blocks.3.multihead.head2.v.weight', 'blocks.3.multihead.mh_lin.weight', 'blocks.3.l_norm_1.weight', 'blocks.3.l_norm_1.bias', 'blocks.3.l_norm_2.weight', 'blocks.3.l_norm_2.bias', 'blocks.3.ffn.weight', 'blocks.3.ffn.bias', 'f_lin.weight', 'f_lin.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Large Model Parameters:\")\n",
    "print(large_model_params.keys())\n",
    "\n",
    "print(\"\\nSmall Model 1 Parameters:\")\n",
    "print(small_model_1_params.keys())\n",
    "\n",
    "print(\"\\nSmall Model 2 Parameters:\")\n",
    "print(small_model_2_params.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if model params remain the same after training small models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models still match their original states\n"
     ]
    }
   ],
   "source": [
    "if comparator.models_match(model1, model2):\n",
    "    print(\"Both models still match their original states\")\n",
    "else:\n",
    "    print(\"At least one model has changed from its original state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 7.1616, Val Loss: 5.3931\n",
      "Epoch 2, Train Loss: 5.8056, Val Loss: 4.7493\n",
      "Epoch 3, Train Loss: 5.2940, Val Loss: 4.3286\n",
      "Epoch 4, Train Loss: 4.9952, Val Loss: 4.1929\n",
      "Epoch 5, Train Loss: 4.8021, Val Loss: 3.8041\n",
      "Epoch 6, Train Loss: 4.6233, Val Loss: 3.7974\n",
      "Epoch 7, Train Loss: 4.4626, Val Loss: 3.6284\n",
      "Epoch 8, Train Loss: 4.2628, Val Loss: 3.5452\n",
      "Epoch 9, Train Loss: 4.1545, Val Loss: 3.3024\n",
      "Epoch 10, Train Loss: 4.0253, Val Loss: 3.1907\n",
      "Epoch 11, Train Loss: 3.9106, Val Loss: 3.0890\n",
      "Epoch 12, Train Loss: 3.7912, Val Loss: 3.0182\n",
      "Epoch 13, Train Loss: 3.6835, Val Loss: 2.9465\n",
      "Epoch 14, Train Loss: 3.5633, Val Loss: 2.7625\n",
      "Epoch 15, Train Loss: 3.4569, Val Loss: 2.6271\n",
      "Epoch 16, Train Loss: 3.3485, Val Loss: 2.6010\n",
      "Epoch 17, Train Loss: 3.2460, Val Loss: 2.4854\n",
      "Epoch 18, Train Loss: 3.1833, Val Loss: 2.4007\n",
      "Epoch 19, Train Loss: 3.0877, Val Loss: 2.3258\n",
      "Epoch 20, Train Loss: 2.9899, Val Loss: 2.3108\n",
      "Epoch 21, Train Loss: 2.9009, Val Loss: 2.1958\n",
      "Epoch 22, Train Loss: 2.8065, Val Loss: 2.0977\n",
      "Epoch 23, Train Loss: 2.7372, Val Loss: 2.0608\n",
      "Epoch 24, Train Loss: 2.6562, Val Loss: 1.9667\n",
      "Epoch 25, Train Loss: 2.5788, Val Loss: 1.8882\n",
      "Epoch 26, Train Loss: 2.4955, Val Loss: 1.8426\n",
      "Epoch 27, Train Loss: 2.4336, Val Loss: 1.7550\n",
      "Epoch 28, Train Loss: 2.3546, Val Loss: 1.7569\n",
      "Epoch 29, Train Loss: 2.3107, Val Loss: 1.6871\n",
      "Epoch 30, Train Loss: 2.2298, Val Loss: 1.5600\n",
      "Epoch 31, Train Loss: 2.1704, Val Loss: 1.5965\n",
      "Epoch 32, Train Loss: 2.1249, Val Loss: 1.5253\n",
      "Epoch 33, Train Loss: 2.0542, Val Loss: 1.4464\n",
      "Epoch 34, Train Loss: 1.9877, Val Loss: 1.4058\n",
      "Epoch 35, Train Loss: 1.9362, Val Loss: 1.3922\n",
      "Epoch 36, Train Loss: 1.8779, Val Loss: 1.3528\n",
      "Epoch 37, Train Loss: 1.8374, Val Loss: 1.2784\n",
      "Epoch 38, Train Loss: 1.7812, Val Loss: 1.2277\n",
      "Epoch 39, Train Loss: 1.7063, Val Loss: 1.2414\n",
      "Epoch 40, Train Loss: 1.6632, Val Loss: 1.1876\n",
      "Epoch 41, Train Loss: 1.6360, Val Loss: 1.1410\n",
      "Epoch 42, Train Loss: 1.6024, Val Loss: 1.1265\n",
      "Epoch 43, Train Loss: 1.5280, Val Loss: 1.0484\n",
      "Epoch 44, Train Loss: 1.5015, Val Loss: 1.0724\n",
      "Epoch 45, Train Loss: 1.4775, Val Loss: 1.0176\n",
      "Epoch 46, Train Loss: 1.4319, Val Loss: 0.9696\n",
      "Epoch 47, Train Loss: 1.3965, Val Loss: 0.9652\n",
      "Epoch 48, Train Loss: 1.3669, Val Loss: 0.9185\n",
      "Epoch 49, Train Loss: 1.3125, Val Loss: 0.9279\n",
      "Epoch 50, Train Loss: 1.2576, Val Loss: 0.9209\n",
      "Epoch 51, Train Loss: 1.2314, Val Loss: 0.8798\n",
      "Epoch 52, Train Loss: 1.1916, Val Loss: 0.8332\n",
      "Epoch 53, Train Loss: 1.1828, Val Loss: 0.7859\n",
      "Epoch 54, Train Loss: 1.1404, Val Loss: 0.8147\n",
      "Epoch 55, Train Loss: 1.1272, Val Loss: 0.7730\n",
      "Epoch 56, Train Loss: 1.0927, Val Loss: 0.7360\n",
      "Epoch 57, Train Loss: 1.0304, Val Loss: 0.7313\n",
      "Epoch 58, Train Loss: 1.0387, Val Loss: 0.7055\n",
      "Epoch 59, Train Loss: 1.0288, Val Loss: 0.6807\n",
      "Epoch 60, Train Loss: 0.9786, Val Loss: 0.6975\n",
      "Epoch 61, Train Loss: 0.9511, Val Loss: 0.6789\n",
      "Epoch 62, Train Loss: 0.9311, Val Loss: 0.6380\n",
      "Epoch 63, Train Loss: 0.8986, Val Loss: 0.6247\n",
      "Epoch 64, Train Loss: 0.8645, Val Loss: 0.6287\n",
      "Epoch 65, Train Loss: 0.8591, Val Loss: 0.5817\n",
      "Epoch 66, Train Loss: 0.8408, Val Loss: 0.5741\n",
      "Epoch 67, Train Loss: 0.8160, Val Loss: 0.5665\n",
      "Epoch 68, Train Loss: 0.8016, Val Loss: 0.5220\n",
      "Epoch 69, Train Loss: 0.7748, Val Loss: 0.5533\n",
      "Epoch 70, Train Loss: 0.7480, Val Loss: 0.5205\n"
     ]
    }
   ],
   "source": [
    "stack_curve = train(stacked_model, optimizer_stacked, 70, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_model_avg = StackedModelAVG([model1, model2], num_models = 2)\n",
    "optimizer_stacked_avg = torch.optim.Adam(stacked_model_avg.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models still match their original states\n"
     ]
    }
   ],
   "source": [
    "if comparator.models_match(model1, model2):\n",
    "    print(\"Both models still match their original states\")\n",
    "else:\n",
    "    print(\"At least one model has changed from its original state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 5.2629, Val Loss: 3.8841\n",
      "Epoch 2, Train Loss: 4.5306, Val Loss: 3.4248\n",
      "Epoch 3, Train Loss: 4.1479, Val Loss: 3.2220\n",
      "Epoch 4, Train Loss: 3.8691, Val Loss: 2.9943\n",
      "Epoch 5, Train Loss: 3.6840, Val Loss: 2.8030\n",
      "Epoch 6, Train Loss: 3.4987, Val Loss: 2.6475\n",
      "Epoch 7, Train Loss: 3.3423, Val Loss: 2.5795\n",
      "Epoch 8, Train Loss: 3.2203, Val Loss: 2.4466\n",
      "Epoch 9, Train Loss: 3.0888, Val Loss: 2.3206\n",
      "Epoch 10, Train Loss: 2.9617, Val Loss: 2.2651\n",
      "Epoch 11, Train Loss: 2.8695, Val Loss: 2.1298\n",
      "Epoch 12, Train Loss: 2.8020, Val Loss: 2.0734\n",
      "Epoch 13, Train Loss: 2.6611, Val Loss: 1.9530\n",
      "Epoch 14, Train Loss: 2.5773, Val Loss: 1.8953\n",
      "Epoch 15, Train Loss: 2.5167, Val Loss: 1.8846\n",
      "Epoch 16, Train Loss: 2.4150, Val Loss: 1.7614\n",
      "Epoch 17, Train Loss: 2.3438, Val Loss: 1.6976\n",
      "Epoch 18, Train Loss: 2.2536, Val Loss: 1.6527\n",
      "Epoch 19, Train Loss: 2.1863, Val Loss: 1.5861\n",
      "Epoch 20, Train Loss: 2.1183, Val Loss: 1.5477\n",
      "Epoch 21, Train Loss: 2.0511, Val Loss: 1.4819\n",
      "Epoch 22, Train Loss: 1.9985, Val Loss: 1.4199\n",
      "Epoch 23, Train Loss: 1.9208, Val Loss: 1.4007\n",
      "Epoch 24, Train Loss: 1.8660, Val Loss: 1.2882\n",
      "Epoch 25, Train Loss: 1.7981, Val Loss: 1.3019\n",
      "Epoch 26, Train Loss: 1.7659, Val Loss: 1.1995\n",
      "Epoch 27, Train Loss: 1.6946, Val Loss: 1.2723\n",
      "Epoch 28, Train Loss: 1.6361, Val Loss: 1.1539\n",
      "Epoch 29, Train Loss: 1.5919, Val Loss: 1.0981\n",
      "Epoch 30, Train Loss: 1.5742, Val Loss: 1.0892\n",
      "Epoch 31, Train Loss: 1.5104, Val Loss: 1.0064\n",
      "Epoch 32, Train Loss: 1.4516, Val Loss: 0.9804\n",
      "Epoch 33, Train Loss: 1.4337, Val Loss: 0.9774\n",
      "Epoch 34, Train Loss: 1.3904, Val Loss: 0.9767\n",
      "Epoch 35, Train Loss: 1.3393, Val Loss: 0.9615\n",
      "Epoch 36, Train Loss: 1.3225, Val Loss: 0.8888\n",
      "Epoch 37, Train Loss: 1.2514, Val Loss: 0.9153\n",
      "Epoch 38, Train Loss: 1.2220, Val Loss: 0.8071\n",
      "Epoch 39, Train Loss: 1.1895, Val Loss: 0.8334\n",
      "Epoch 40, Train Loss: 1.1576, Val Loss: 0.8273\n",
      "Epoch 41, Train Loss: 1.1343, Val Loss: 0.8020\n",
      "Epoch 42, Train Loss: 1.0849, Val Loss: 0.7772\n",
      "Epoch 43, Train Loss: 1.0650, Val Loss: 0.7509\n",
      "Epoch 44, Train Loss: 1.0420, Val Loss: 0.7126\n",
      "Epoch 45, Train Loss: 1.0094, Val Loss: 0.6831\n",
      "Epoch 46, Train Loss: 0.9764, Val Loss: 0.6579\n",
      "Epoch 47, Train Loss: 0.9596, Val Loss: 0.7222\n",
      "Epoch 48, Train Loss: 0.9263, Val Loss: 0.6471\n",
      "Epoch 49, Train Loss: 0.9165, Val Loss: 0.6117\n",
      "Epoch 50, Train Loss: 0.8959, Val Loss: 0.6205\n",
      "Epoch 51, Train Loss: 0.8465, Val Loss: 0.5773\n",
      "Epoch 52, Train Loss: 0.8230, Val Loss: 0.5926\n",
      "Epoch 53, Train Loss: 0.8159, Val Loss: 0.5605\n",
      "Epoch 54, Train Loss: 0.8025, Val Loss: 0.6043\n",
      "Epoch 55, Train Loss: 0.7668, Val Loss: 0.5087\n",
      "Epoch 56, Train Loss: 0.7518, Val Loss: 0.5311\n",
      "Epoch 57, Train Loss: 0.7355, Val Loss: 0.5389\n",
      "Epoch 58, Train Loss: 0.7133, Val Loss: 0.4491\n",
      "Epoch 59, Train Loss: 0.7056, Val Loss: 0.5120\n",
      "Epoch 60, Train Loss: 0.6759, Val Loss: 0.4905\n",
      "Epoch 61, Train Loss: 0.6698, Val Loss: 0.4679\n",
      "Epoch 62, Train Loss: 0.6558, Val Loss: 0.4411\n",
      "Epoch 63, Train Loss: 0.6308, Val Loss: 0.4346\n",
      "Epoch 64, Train Loss: 0.6145, Val Loss: 0.4463\n",
      "Epoch 65, Train Loss: 0.6162, Val Loss: 0.4539\n",
      "Epoch 66, Train Loss: 0.5866, Val Loss: 0.3819\n",
      "Epoch 67, Train Loss: 0.5734, Val Loss: 0.3992\n",
      "Epoch 68, Train Loss: 0.5575, Val Loss: 0.4067\n",
      "Epoch 69, Train Loss: 0.5421, Val Loss: 0.3870\n",
      "Epoch 70, Train Loss: 0.5341, Val Loss: 0.3881\n"
     ]
    }
   ],
   "source": [
    "stack_avg_curve = train(stacked_model_avg, optimizer_stacked_avg, 70, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_model_same = StackedModel([model1, model2], num_models = 2, same=True)\n",
    "optimizer_stacked_same = torch.optim.Adam(stacked_model_same.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models still match their original states\n"
     ]
    }
   ],
   "source": [
    "if comparator.models_match(model1, model2):\n",
    "    print(\"Both models still match their original states\")\n",
    "else:\n",
    "    print(\"At least one model has changed from its original state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 3.7087, Val Loss: 2.5053\n",
      "Epoch 2, Train Loss: 3.1418, Val Loss: 2.2677\n",
      "Epoch 3, Train Loss: 2.8825, Val Loss: 2.0843\n",
      "Epoch 4, Train Loss: 2.7075, Val Loss: 2.0223\n",
      "Epoch 5, Train Loss: 2.5966, Val Loss: 1.8837\n",
      "Epoch 6, Train Loss: 2.4911, Val Loss: 1.8218\n",
      "Epoch 7, Train Loss: 2.3864, Val Loss: 1.7470\n",
      "Epoch 8, Train Loss: 2.3155, Val Loss: 1.6743\n",
      "Epoch 9, Train Loss: 2.2066, Val Loss: 1.6205\n",
      "Epoch 10, Train Loss: 2.1435, Val Loss: 1.5583\n",
      "Epoch 11, Train Loss: 2.0763, Val Loss: 1.5329\n",
      "Epoch 12, Train Loss: 1.9815, Val Loss: 1.4235\n",
      "Epoch 13, Train Loss: 1.9499, Val Loss: 1.3751\n",
      "Epoch 14, Train Loss: 1.8835, Val Loss: 1.3772\n",
      "Epoch 15, Train Loss: 1.8131, Val Loss: 1.2915\n",
      "Epoch 16, Train Loss: 1.7608, Val Loss: 1.2813\n",
      "Epoch 17, Train Loss: 1.6999, Val Loss: 1.2343\n",
      "Epoch 18, Train Loss: 1.6637, Val Loss: 1.1833\n",
      "Epoch 19, Train Loss: 1.6104, Val Loss: 1.1186\n",
      "Epoch 20, Train Loss: 1.5598, Val Loss: 1.1322\n",
      "Epoch 21, Train Loss: 1.5346, Val Loss: 1.0706\n",
      "Epoch 22, Train Loss: 1.4935, Val Loss: 1.0330\n",
      "Epoch 23, Train Loss: 1.4250, Val Loss: 1.0782\n",
      "Epoch 24, Train Loss: 1.3787, Val Loss: 0.9957\n",
      "Epoch 25, Train Loss: 1.3419, Val Loss: 0.9340\n",
      "Epoch 26, Train Loss: 1.3002, Val Loss: 0.9538\n",
      "Epoch 27, Train Loss: 1.2663, Val Loss: 0.8865\n",
      "Epoch 28, Train Loss: 1.2508, Val Loss: 0.8960\n",
      "Epoch 29, Train Loss: 1.1951, Val Loss: 0.8569\n",
      "Epoch 30, Train Loss: 1.1650, Val Loss: 0.8239\n",
      "Epoch 31, Train Loss: 1.1532, Val Loss: 0.8145\n",
      "Epoch 32, Train Loss: 1.1436, Val Loss: 0.8495\n",
      "Epoch 33, Train Loss: 1.0767, Val Loss: 0.7773\n",
      "Epoch 34, Train Loss: 1.0642, Val Loss: 0.7523\n",
      "Epoch 35, Train Loss: 1.0200, Val Loss: 0.7254\n",
      "Epoch 36, Train Loss: 0.9925, Val Loss: 0.7172\n",
      "Epoch 37, Train Loss: 0.9770, Val Loss: 0.7142\n",
      "Epoch 38, Train Loss: 0.9600, Val Loss: 0.6700\n",
      "Epoch 39, Train Loss: 0.9147, Val Loss: 0.6477\n",
      "Epoch 40, Train Loss: 0.9114, Val Loss: 0.6683\n",
      "Epoch 41, Train Loss: 0.8710, Val Loss: 0.6411\n",
      "Epoch 42, Train Loss: 0.8623, Val Loss: 0.6076\n",
      "Epoch 43, Train Loss: 0.8326, Val Loss: 0.5436\n",
      "Epoch 44, Train Loss: 0.8038, Val Loss: 0.5895\n",
      "Epoch 45, Train Loss: 0.7736, Val Loss: 0.5238\n",
      "Epoch 46, Train Loss: 0.7588, Val Loss: 0.5478\n",
      "Epoch 47, Train Loss: 0.7331, Val Loss: 0.5415\n",
      "Epoch 48, Train Loss: 0.7416, Val Loss: 0.5732\n",
      "Epoch 49, Train Loss: 0.7222, Val Loss: 0.5067\n",
      "Epoch 50, Train Loss: 0.7070, Val Loss: 0.5195\n",
      "Epoch 51, Train Loss: 0.6668, Val Loss: 0.5353\n",
      "Epoch 52, Train Loss: 0.6646, Val Loss: 0.4980\n",
      "Epoch 53, Train Loss: 0.6442, Val Loss: 0.4478\n",
      "Epoch 54, Train Loss: 0.6276, Val Loss: 0.4422\n",
      "Epoch 55, Train Loss: 0.6069, Val Loss: 0.4245\n",
      "Epoch 56, Train Loss: 0.5901, Val Loss: 0.4495\n",
      "Epoch 57, Train Loss: 0.5943, Val Loss: 0.4360\n",
      "Epoch 58, Train Loss: 0.5816, Val Loss: 0.3958\n",
      "Epoch 59, Train Loss: 0.5566, Val Loss: 0.4113\n",
      "Epoch 60, Train Loss: 0.5364, Val Loss: 0.3714\n",
      "Epoch 61, Train Loss: 0.5448, Val Loss: 0.3955\n",
      "Epoch 62, Train Loss: 0.5219, Val Loss: 0.3647\n",
      "Epoch 63, Train Loss: 0.5009, Val Loss: 0.3906\n",
      "Epoch 64, Train Loss: 0.5029, Val Loss: 0.3429\n",
      "Epoch 65, Train Loss: 0.4828, Val Loss: 0.3498\n",
      "Epoch 66, Train Loss: 0.4763, Val Loss: 0.3813\n",
      "Epoch 67, Train Loss: 0.4559, Val Loss: 0.3541\n",
      "Epoch 68, Train Loss: 0.4515, Val Loss: 0.2978\n",
      "Epoch 69, Train Loss: 0.4434, Val Loss: 0.3392\n",
      "Epoch 70, Train Loss: 0.4301, Val Loss: 0.3139\n"
     ]
    }
   ],
   "source": [
    "stack_same_curve = train(stacked_model_same, optimizer_stacked_same, 70, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Stack Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterWeavedModel(torch.nn.Module):\n",
    "    def __init__(self, models, num_models, same=False):\n",
    "        super(InterWeavedModel, self).__init__()\n",
    "        \n",
    "        if len(models) != num_models:\n",
    "            raise ValueError(f\"Expected {num_models} models, but got {len(models)}\")\n",
    "        \n",
    "        # Randomize the order of models\n",
    "        random.shuffle(models)\n",
    "        \n",
    "        self.embedding = copy.deepcopy(models[0].embedding)\n",
    "        self.pe = copy.deepcopy(models[0].pe)\n",
    "        \n",
    "        # Interweave the blocks from all models\n",
    "        self.blocks = torch.nn.ModuleList()\n",
    "        num_blocks_per_model = len(models[0].blocks)\n",
    "        \n",
    "        for i in range(num_blocks_per_model):\n",
    "            for model in models:\n",
    "                self.blocks.append(copy.deepcopy(model.blocks[i]))\n",
    "        \n",
    "        if same:\n",
    "            model_rand = random.choice(models)\n",
    "            self.embedding = copy.deepcopy(model_rand.embedding)\n",
    "            self.pe = copy.deepcopy(model_rand.pe)\n",
    "            self.f_lin = copy.deepcopy(model_rand.f_lin)\n",
    "        else:\n",
    "            self.f_lin = copy.deepcopy(models[-1].f_lin)\n",
    "        \n",
    "        self.drop = torch.nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        e = self.embedding(inp)\n",
    "        e = self.pe(e)\n",
    "        \n",
    "        # Pass through all interweaved blocks\n",
    "        for block in self.blocks:\n",
    "            e = block(e)\n",
    "        \n",
    "        r = self.f_lin(self.drop(e))\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterWeavedModelAVG(torch.nn.Module):\n",
    "    def __init__(self, models, num_models, same=False):\n",
    "        super(InterWeavedModelAVG, self).__init__()\n",
    "        \n",
    "        if len(models) != num_models:\n",
    "            raise ValueError(f\"Expected {num_models} models, but got {len(models)}\")\n",
    "        \n",
    "        # Randomize the order of models\n",
    "        random.shuffle(models)\n",
    "        \n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, emb_size)\n",
    "        \n",
    "        embedding_weights = [copy.deepcopy(model.embedding.weight) for model in models]\n",
    "        avg_embedding_weight = sum(embedding_weights) / num_models\n",
    "\n",
    "        self.embedding.weight = torch.nn.Parameter(avg_embedding_weight, requires_grad=True)\n",
    "\n",
    "        self.pe = copy.deepcopy(models[0].pe)\n",
    "        \n",
    "        # Interweave the blocks from all models\n",
    "        self.blocks = torch.nn.ModuleList()\n",
    "        num_blocks_per_model = len(models[0].blocks)\n",
    "        \n",
    "        for i in range(num_blocks_per_model):\n",
    "            for model in models:\n",
    "                self.blocks.append(copy.deepcopy(model.blocks[i]))\n",
    "                \n",
    "        self.f_lin = torch.nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "        linear_weights = [copy.deepcopy(model.f_lin.weight) for model in models]\n",
    "        linear_biases = [copy.deepcopy(model.f_lin.bias) for model in models]\n",
    "\n",
    "        avg_linear_weight = sum(linear_weights) / num_models\n",
    "        avg_linear_bias = sum(linear_biases) / num_models\n",
    "\n",
    "        self.f_lin.weight = torch.nn.Parameter(avg_linear_weight, requires_grad=True)\n",
    "        self.f_lin.bias = torch.nn.Parameter(avg_linear_bias, requires_grad=True)\n",
    "        \n",
    "        self.drop = torch.nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        e = self.embedding(inp)\n",
    "        e = self.pe(e)\n",
    "        \n",
    "        # Pass through all interweaved blocks\n",
    "        for block in self.blocks:\n",
    "            e = block(e)\n",
    "        \n",
    "        r = self.f_lin(self.drop(e))\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "interweaved_model = InterWeavedModel([model1, model2], num_models = 2)\n",
    "optimizer_interweaved = torch.optim.Adam(interweaved_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models still match their original states\n"
     ]
    }
   ],
   "source": [
    "if comparator.models_match(model1, model2):\n",
    "    print(\"Both models still match their original states\")\n",
    "else:\n",
    "    print(\"At least one model has changed from its original state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 7.2712, Val Loss: 5.4762\n",
      "Epoch 2, Train Loss: 5.9126, Val Loss: 4.7575\n",
      "Epoch 3, Train Loss: 5.3969, Val Loss: 4.3743\n",
      "Epoch 4, Train Loss: 5.0463, Val Loss: 4.1664\n",
      "Epoch 5, Train Loss: 4.8512, Val Loss: 3.9599\n",
      "Epoch 6, Train Loss: 4.6568, Val Loss: 3.7864\n",
      "Epoch 7, Train Loss: 4.4532, Val Loss: 3.5364\n",
      "Epoch 8, Train Loss: 4.2655, Val Loss: 3.4932\n",
      "Epoch 9, Train Loss: 4.1588, Val Loss: 3.3351\n",
      "Epoch 10, Train Loss: 4.0073, Val Loss: 3.1542\n",
      "Epoch 11, Train Loss: 3.9168, Val Loss: 3.1073\n",
      "Epoch 12, Train Loss: 3.7942, Val Loss: 2.9611\n",
      "Epoch 13, Train Loss: 3.6878, Val Loss: 2.9201\n",
      "Epoch 14, Train Loss: 3.5656, Val Loss: 2.7929\n",
      "Epoch 15, Train Loss: 3.4725, Val Loss: 2.6527\n",
      "Epoch 16, Train Loss: 3.3685, Val Loss: 2.6378\n",
      "Epoch 17, Train Loss: 3.2435, Val Loss: 2.4664\n",
      "Epoch 18, Train Loss: 3.1657, Val Loss: 2.4109\n",
      "Epoch 19, Train Loss: 3.0753, Val Loss: 2.3797\n",
      "Epoch 20, Train Loss: 2.9793, Val Loss: 2.2493\n",
      "Epoch 21, Train Loss: 2.8742, Val Loss: 2.1853\n",
      "Epoch 22, Train Loss: 2.7937, Val Loss: 2.0638\n",
      "Epoch 23, Train Loss: 2.7255, Val Loss: 2.0293\n",
      "Epoch 24, Train Loss: 2.6455, Val Loss: 1.9431\n",
      "Epoch 25, Train Loss: 2.5776, Val Loss: 1.9186\n",
      "Epoch 26, Train Loss: 2.4937, Val Loss: 1.8423\n",
      "Epoch 27, Train Loss: 2.4277, Val Loss: 1.7277\n",
      "Epoch 28, Train Loss: 2.3320, Val Loss: 1.7087\n",
      "Epoch 29, Train Loss: 2.2726, Val Loss: 1.6526\n",
      "Epoch 30, Train Loss: 2.1942, Val Loss: 1.5584\n",
      "Epoch 31, Train Loss: 2.1562, Val Loss: 1.5386\n",
      "Epoch 32, Train Loss: 2.0965, Val Loss: 1.5314\n",
      "Epoch 33, Train Loss: 2.0478, Val Loss: 1.4365\n",
      "Epoch 34, Train Loss: 1.9542, Val Loss: 1.3971\n",
      "Epoch 35, Train Loss: 1.9280, Val Loss: 1.3829\n",
      "Epoch 36, Train Loss: 1.8684, Val Loss: 1.3918\n",
      "Epoch 37, Train Loss: 1.8022, Val Loss: 1.2967\n",
      "Epoch 38, Train Loss: 1.7684, Val Loss: 1.2659\n",
      "Epoch 39, Train Loss: 1.6949, Val Loss: 1.1798\n",
      "Epoch 40, Train Loss: 1.6666, Val Loss: 1.2157\n",
      "Epoch 41, Train Loss: 1.6046, Val Loss: 1.1794\n",
      "Epoch 42, Train Loss: 1.5722, Val Loss: 1.1261\n",
      "Epoch 43, Train Loss: 1.5262, Val Loss: 1.0604\n",
      "Epoch 44, Train Loss: 1.5059, Val Loss: 1.0553\n",
      "Epoch 45, Train Loss: 1.4472, Val Loss: 1.0255\n",
      "Epoch 46, Train Loss: 1.4285, Val Loss: 0.9887\n",
      "Epoch 47, Train Loss: 1.3791, Val Loss: 0.9779\n",
      "Epoch 48, Train Loss: 1.3239, Val Loss: 0.9250\n",
      "Epoch 49, Train Loss: 1.3043, Val Loss: 0.8706\n",
      "Epoch 50, Train Loss: 1.2698, Val Loss: 0.8914\n",
      "Epoch 51, Train Loss: 1.2329, Val Loss: 0.8479\n",
      "Epoch 52, Train Loss: 1.2006, Val Loss: 0.8299\n",
      "Epoch 53, Train Loss: 1.1646, Val Loss: 0.7805\n",
      "Epoch 54, Train Loss: 1.1312, Val Loss: 0.8267\n",
      "Epoch 55, Train Loss: 1.1141, Val Loss: 0.7442\n",
      "Epoch 56, Train Loss: 1.0619, Val Loss: 0.7169\n",
      "Epoch 57, Train Loss: 1.0478, Val Loss: 0.7307\n",
      "Epoch 58, Train Loss: 1.0204, Val Loss: 0.6955\n",
      "Epoch 59, Train Loss: 1.0034, Val Loss: 0.7242\n",
      "Epoch 60, Train Loss: 0.9536, Val Loss: 0.6611\n",
      "Epoch 61, Train Loss: 0.9399, Val Loss: 0.6343\n",
      "Epoch 62, Train Loss: 0.9191, Val Loss: 0.6734\n",
      "Epoch 63, Train Loss: 0.8847, Val Loss: 0.5763\n",
      "Epoch 64, Train Loss: 0.8689, Val Loss: 0.6103\n",
      "Epoch 65, Train Loss: 0.8413, Val Loss: 0.6115\n",
      "Epoch 66, Train Loss: 0.8373, Val Loss: 0.5691\n",
      "Epoch 67, Train Loss: 0.8018, Val Loss: 0.5832\n",
      "Epoch 68, Train Loss: 0.7856, Val Loss: 0.5299\n",
      "Epoch 69, Train Loss: 0.7615, Val Loss: 0.5444\n",
      "Epoch 70, Train Loss: 0.7602, Val Loss: 0.5137\n"
     ]
    }
   ],
   "source": [
    "interweaved_curve = train(interweaved_model, optimizer_interweaved, 70, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "interweaving_model_avg = InterWeavedModelAVG([model1, model2], num_models = 2)\n",
    "optimizer_interweaving_avg = torch.optim.Adam(interweaving_model_avg.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models still match their original states\n"
     ]
    }
   ],
   "source": [
    "if comparator.models_match(model1, model2):\n",
    "    print(\"Both models still match their original states\")\n",
    "else:\n",
    "    print(\"At least one model has changed from its original state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 5.0945, Val Loss: 3.7490\n",
      "Epoch 2, Train Loss: 4.4329, Val Loss: 3.3550\n",
      "Epoch 3, Train Loss: 4.0715, Val Loss: 3.1424\n",
      "Epoch 4, Train Loss: 3.8389, Val Loss: 2.9098\n",
      "Epoch 5, Train Loss: 3.6081, Val Loss: 2.7734\n",
      "Epoch 6, Train Loss: 3.4580, Val Loss: 2.6471\n",
      "Epoch 7, Train Loss: 3.3045, Val Loss: 2.5051\n",
      "Epoch 8, Train Loss: 3.1819, Val Loss: 2.3864\n",
      "Epoch 9, Train Loss: 3.0700, Val Loss: 2.3011\n",
      "Epoch 10, Train Loss: 2.9341, Val Loss: 2.1385\n",
      "Epoch 11, Train Loss: 2.8340, Val Loss: 2.1100\n",
      "Epoch 12, Train Loss: 2.7572, Val Loss: 2.0571\n",
      "Epoch 13, Train Loss: 2.6443, Val Loss: 1.9606\n",
      "Epoch 14, Train Loss: 2.5564, Val Loss: 1.8679\n",
      "Epoch 15, Train Loss: 2.4753, Val Loss: 1.8092\n",
      "Epoch 16, Train Loss: 2.3767, Val Loss: 1.7046\n",
      "Epoch 17, Train Loss: 2.3227, Val Loss: 1.6403\n",
      "Epoch 18, Train Loss: 2.2153, Val Loss: 1.5718\n",
      "Epoch 19, Train Loss: 2.1850, Val Loss: 1.5394\n",
      "Epoch 20, Train Loss: 2.0848, Val Loss: 1.4986\n",
      "Epoch 21, Train Loss: 2.0226, Val Loss: 1.4457\n",
      "Epoch 22, Train Loss: 1.9477, Val Loss: 1.3922\n",
      "Epoch 23, Train Loss: 1.8925, Val Loss: 1.3561\n",
      "Epoch 24, Train Loss: 1.8333, Val Loss: 1.3092\n",
      "Epoch 25, Train Loss: 1.7785, Val Loss: 1.2882\n",
      "Epoch 26, Train Loss: 1.7268, Val Loss: 1.2076\n",
      "Epoch 27, Train Loss: 1.6860, Val Loss: 1.1975\n",
      "Epoch 28, Train Loss: 1.6263, Val Loss: 1.1197\n",
      "Epoch 29, Train Loss: 1.5792, Val Loss: 1.1266\n",
      "Epoch 30, Train Loss: 1.5202, Val Loss: 1.0703\n",
      "Epoch 31, Train Loss: 1.4762, Val Loss: 1.0251\n",
      "Epoch 32, Train Loss: 1.4541, Val Loss: 0.9732\n",
      "Epoch 33, Train Loss: 1.3940, Val Loss: 0.9806\n",
      "Epoch 34, Train Loss: 1.3706, Val Loss: 0.9522\n",
      "Epoch 35, Train Loss: 1.3032, Val Loss: 0.9033\n",
      "Epoch 36, Train Loss: 1.2798, Val Loss: 0.8869\n",
      "Epoch 37, Train Loss: 1.2493, Val Loss: 0.8092\n",
      "Epoch 38, Train Loss: 1.2139, Val Loss: 0.8241\n",
      "Epoch 39, Train Loss: 1.1647, Val Loss: 0.8025\n",
      "Epoch 40, Train Loss: 1.1457, Val Loss: 0.8023\n",
      "Epoch 41, Train Loss: 1.0996, Val Loss: 0.8034\n",
      "Epoch 42, Train Loss: 1.0906, Val Loss: 0.7561\n",
      "Epoch 43, Train Loss: 1.0418, Val Loss: 0.7674\n",
      "Epoch 44, Train Loss: 1.0046, Val Loss: 0.6928\n",
      "Epoch 45, Train Loss: 0.9893, Val Loss: 0.6784\n",
      "Epoch 46, Train Loss: 0.9554, Val Loss: 0.6765\n",
      "Epoch 47, Train Loss: 0.9328, Val Loss: 0.6079\n",
      "Epoch 48, Train Loss: 0.9228, Val Loss: 0.6385\n",
      "Epoch 49, Train Loss: 0.8824, Val Loss: 0.6494\n",
      "Epoch 50, Train Loss: 0.8725, Val Loss: 0.6395\n",
      "Epoch 51, Train Loss: 0.8355, Val Loss: 0.5666\n",
      "Epoch 52, Train Loss: 0.8071, Val Loss: 0.5717\n",
      "Epoch 53, Train Loss: 0.7910, Val Loss: 0.5044\n",
      "Epoch 54, Train Loss: 0.7692, Val Loss: 0.5323\n",
      "Epoch 55, Train Loss: 0.7621, Val Loss: 0.5156\n",
      "Epoch 56, Train Loss: 0.7575, Val Loss: 0.5618\n",
      "Epoch 57, Train Loss: 0.7109, Val Loss: 0.5246\n",
      "Epoch 58, Train Loss: 0.7164, Val Loss: 0.4769\n",
      "Epoch 59, Train Loss: 0.6877, Val Loss: 0.4950\n",
      "Epoch 60, Train Loss: 0.6612, Val Loss: 0.4881\n",
      "Epoch 61, Train Loss: 0.6473, Val Loss: 0.4581\n",
      "Epoch 62, Train Loss: 0.6580, Val Loss: 0.4496\n",
      "Epoch 63, Train Loss: 0.6469, Val Loss: 0.4372\n",
      "Epoch 64, Train Loss: 0.6077, Val Loss: 0.3963\n",
      "Epoch 65, Train Loss: 0.5776, Val Loss: 0.4276\n",
      "Epoch 66, Train Loss: 0.5704, Val Loss: 0.4332\n",
      "Epoch 67, Train Loss: 0.5467, Val Loss: 0.4259\n",
      "Epoch 68, Train Loss: 0.5458, Val Loss: 0.4579\n",
      "Epoch 69, Train Loss: 0.5341, Val Loss: 0.4095\n",
      "Epoch 70, Train Loss: 0.5301, Val Loss: 0.3780\n"
     ]
    }
   ],
   "source": [
    "interweaved_avg_curve = train(interweaving_model_avg, optimizer_interweaving_avg, 70, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "interweaving_model_same = InterWeavedModel([model1, model2], num_models = 2, same=True)\n",
    "optimizer_interweaving_same = torch.optim.Adam(interweaving_model_same.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models still match their original states\n"
     ]
    }
   ],
   "source": [
    "if comparator.models_match(model1, model2):\n",
    "    print(\"Both models still match their original states\")\n",
    "else:\n",
    "    print(\"At least one model has changed from its original state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 3.4692, Val Loss: 2.4170\n",
      "Epoch 2, Train Loss: 3.0388, Val Loss: 2.2140\n",
      "Epoch 3, Train Loss: 2.8296, Val Loss: 2.0971\n",
      "Epoch 4, Train Loss: 2.6762, Val Loss: 1.9398\n",
      "Epoch 5, Train Loss: 2.5731, Val Loss: 1.8172\n",
      "Epoch 6, Train Loss: 2.4348, Val Loss: 1.7893\n",
      "Epoch 7, Train Loss: 2.3581, Val Loss: 1.6802\n",
      "Epoch 8, Train Loss: 2.2539, Val Loss: 1.6605\n",
      "Epoch 9, Train Loss: 2.1948, Val Loss: 1.6509\n",
      "Epoch 10, Train Loss: 2.1341, Val Loss: 1.5485\n",
      "Epoch 11, Train Loss: 2.0318, Val Loss: 1.4361\n",
      "Epoch 12, Train Loss: 1.9712, Val Loss: 1.4235\n",
      "Epoch 13, Train Loss: 1.9247, Val Loss: 1.3490\n",
      "Epoch 14, Train Loss: 1.8487, Val Loss: 1.3288\n",
      "Epoch 15, Train Loss: 1.7722, Val Loss: 1.2867\n",
      "Epoch 16, Train Loss: 1.7385, Val Loss: 1.2725\n",
      "Epoch 17, Train Loss: 1.6652, Val Loss: 1.2008\n",
      "Epoch 18, Train Loss: 1.6382, Val Loss: 1.1904\n",
      "Epoch 19, Train Loss: 1.5521, Val Loss: 1.1283\n",
      "Epoch 20, Train Loss: 1.5106, Val Loss: 1.0895\n",
      "Epoch 21, Train Loss: 1.4927, Val Loss: 1.1063\n",
      "Epoch 22, Train Loss: 1.4440, Val Loss: 1.0489\n",
      "Epoch 23, Train Loss: 1.4252, Val Loss: 0.9563\n",
      "Epoch 24, Train Loss: 1.3715, Val Loss: 0.9479\n",
      "Epoch 25, Train Loss: 1.3270, Val Loss: 0.9690\n",
      "Epoch 26, Train Loss: 1.2590, Val Loss: 0.9422\n",
      "Epoch 27, Train Loss: 1.2673, Val Loss: 0.8871\n",
      "Epoch 28, Train Loss: 1.2163, Val Loss: 0.9116\n",
      "Epoch 29, Train Loss: 1.1932, Val Loss: 0.8679\n",
      "Epoch 30, Train Loss: 1.1494, Val Loss: 0.8121\n",
      "Epoch 31, Train Loss: 1.1390, Val Loss: 0.8012\n",
      "Epoch 32, Train Loss: 1.0866, Val Loss: 0.8108\n",
      "Epoch 33, Train Loss: 1.0957, Val Loss: 0.7413\n",
      "Epoch 34, Train Loss: 1.0342, Val Loss: 0.7340\n",
      "Epoch 35, Train Loss: 1.0129, Val Loss: 0.6908\n",
      "Epoch 36, Train Loss: 0.9559, Val Loss: 0.6983\n",
      "Epoch 37, Train Loss: 0.9676, Val Loss: 0.6425\n",
      "Epoch 38, Train Loss: 0.9211, Val Loss: 0.6786\n",
      "Epoch 39, Train Loss: 0.8922, Val Loss: 0.6640\n",
      "Epoch 40, Train Loss: 0.8767, Val Loss: 0.6506\n",
      "Epoch 41, Train Loss: 0.8513, Val Loss: 0.6031\n",
      "Epoch 42, Train Loss: 0.8410, Val Loss: 0.6099\n",
      "Epoch 43, Train Loss: 0.8044, Val Loss: 0.6011\n",
      "Epoch 44, Train Loss: 0.7835, Val Loss: 0.6082\n",
      "Epoch 45, Train Loss: 0.7812, Val Loss: 0.5532\n",
      "Epoch 46, Train Loss: 0.7750, Val Loss: 0.5518\n",
      "Epoch 47, Train Loss: 0.7342, Val Loss: 0.5377\n",
      "Epoch 48, Train Loss: 0.7155, Val Loss: 0.5070\n",
      "Epoch 49, Train Loss: 0.7035, Val Loss: 0.5302\n",
      "Epoch 50, Train Loss: 0.6927, Val Loss: 0.4628\n",
      "Epoch 51, Train Loss: 0.6581, Val Loss: 0.4898\n",
      "Epoch 52, Train Loss: 0.6576, Val Loss: 0.4462\n",
      "Epoch 53, Train Loss: 0.6216, Val Loss: 0.4746\n",
      "Epoch 54, Train Loss: 0.6249, Val Loss: 0.4662\n",
      "Epoch 55, Train Loss: 0.6098, Val Loss: 0.4396\n",
      "Epoch 56, Train Loss: 0.5846, Val Loss: 0.4355\n",
      "Epoch 57, Train Loss: 0.5684, Val Loss: 0.4356\n",
      "Epoch 58, Train Loss: 0.5789, Val Loss: 0.3849\n",
      "Epoch 59, Train Loss: 0.5464, Val Loss: 0.4017\n",
      "Epoch 60, Train Loss: 0.5270, Val Loss: 0.3966\n",
      "Epoch 61, Train Loss: 0.5305, Val Loss: 0.3715\n",
      "Epoch 62, Train Loss: 0.5003, Val Loss: 0.3610\n",
      "Epoch 63, Train Loss: 0.5052, Val Loss: 0.3708\n",
      "Epoch 64, Train Loss: 0.4949, Val Loss: 0.3907\n",
      "Epoch 65, Train Loss: 0.4912, Val Loss: 0.3368\n",
      "Epoch 66, Train Loss: 0.4639, Val Loss: 0.3218\n",
      "Epoch 67, Train Loss: 0.4645, Val Loss: 0.3320\n",
      "Epoch 68, Train Loss: 0.4533, Val Loss: 0.3112\n",
      "Epoch 69, Train Loss: 0.4341, Val Loss: 0.3337\n",
      "Epoch 70, Train Loss: 0.4248, Val Loss: 0.3335\n"
     ]
    }
   ],
   "source": [
    "interweaved_same_curve = train(interweaving_model_same, optimizer_interweaving_same, 70, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Single Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "Single_Stacked_Model = StackedModel([model1, model1], num_models = 2)\n",
    "optimizer_single_stacked = torch.optim.Adam(Single_Stacked_Model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models still match their original states\n"
     ]
    }
   ],
   "source": [
    "if comparator.models_match(model1, model2):\n",
    "    print(\"Both models still match their original states\")\n",
    "else:\n",
    "    print(\"At least one model has changed from its original state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 3.0280, Val Loss: 2.1997\n",
      "Epoch 2, Train Loss: 2.7442, Val Loss: 2.0250\n",
      "Epoch 3, Train Loss: 2.6021, Val Loss: 1.9257\n",
      "Epoch 4, Train Loss: 2.5093, Val Loss: 1.8857\n",
      "Epoch 5, Train Loss: 2.3953, Val Loss: 1.8035\n",
      "Epoch 6, Train Loss: 2.3309, Val Loss: 1.7277\n",
      "Epoch 7, Train Loss: 2.2451, Val Loss: 1.6487\n",
      "Epoch 8, Train Loss: 2.1786, Val Loss: 1.6139\n",
      "Epoch 9, Train Loss: 2.0940, Val Loss: 1.5283\n",
      "Epoch 10, Train Loss: 2.0314, Val Loss: 1.4830\n",
      "Epoch 11, Train Loss: 1.9619, Val Loss: 1.4196\n",
      "Epoch 12, Train Loss: 1.8984, Val Loss: 1.4524\n",
      "Epoch 13, Train Loss: 1.8257, Val Loss: 1.3424\n",
      "Epoch 14, Train Loss: 1.7861, Val Loss: 1.3666\n",
      "Epoch 15, Train Loss: 1.7319, Val Loss: 1.2573\n",
      "Epoch 16, Train Loss: 1.6779, Val Loss: 1.1990\n",
      "Epoch 17, Train Loss: 1.6325, Val Loss: 1.1588\n",
      "Epoch 18, Train Loss: 1.5819, Val Loss: 1.1454\n",
      "Epoch 19, Train Loss: 1.5542, Val Loss: 1.1391\n",
      "Epoch 20, Train Loss: 1.4772, Val Loss: 1.0983\n",
      "Epoch 21, Train Loss: 1.4563, Val Loss: 1.0133\n",
      "Epoch 22, Train Loss: 1.4045, Val Loss: 1.0442\n",
      "Epoch 23, Train Loss: 1.3663, Val Loss: 0.9951\n",
      "Epoch 24, Train Loss: 1.3216, Val Loss: 0.9577\n",
      "Epoch 25, Train Loss: 1.3106, Val Loss: 0.9171\n",
      "Epoch 26, Train Loss: 1.2606, Val Loss: 0.9368\n",
      "Epoch 27, Train Loss: 1.2184, Val Loss: 0.8625\n",
      "Epoch 28, Train Loss: 1.1953, Val Loss: 0.8189\n",
      "Epoch 29, Train Loss: 1.1498, Val Loss: 0.8522\n",
      "Epoch 30, Train Loss: 1.1206, Val Loss: 0.8110\n",
      "Epoch 31, Train Loss: 1.1147, Val Loss: 0.7871\n",
      "Epoch 32, Train Loss: 1.0533, Val Loss: 0.7874\n",
      "Epoch 33, Train Loss: 1.0432, Val Loss: 0.7785\n",
      "Epoch 34, Train Loss: 1.0092, Val Loss: 0.7257\n",
      "Epoch 35, Train Loss: 0.9808, Val Loss: 0.6714\n",
      "Epoch 36, Train Loss: 0.9646, Val Loss: 0.7056\n",
      "Epoch 37, Train Loss: 0.9428, Val Loss: 0.6718\n",
      "Epoch 38, Train Loss: 0.9146, Val Loss: 0.6206\n",
      "Epoch 39, Train Loss: 0.8766, Val Loss: 0.6703\n",
      "Epoch 40, Train Loss: 0.8536, Val Loss: 0.6630\n",
      "Epoch 41, Train Loss: 0.8609, Val Loss: 0.5816\n",
      "Epoch 42, Train Loss: 0.8333, Val Loss: 0.5833\n",
      "Epoch 43, Train Loss: 0.8034, Val Loss: 0.5894\n",
      "Epoch 44, Train Loss: 0.7829, Val Loss: 0.5423\n",
      "Epoch 45, Train Loss: 0.7517, Val Loss: 0.5567\n",
      "Epoch 46, Train Loss: 0.7537, Val Loss: 0.5423\n",
      "Epoch 47, Train Loss: 0.7396, Val Loss: 0.5218\n",
      "Epoch 48, Train Loss: 0.7074, Val Loss: 0.5143\n",
      "Epoch 49, Train Loss: 0.6944, Val Loss: 0.5304\n",
      "Epoch 50, Train Loss: 0.6735, Val Loss: 0.5074\n",
      "Epoch 51, Train Loss: 0.6349, Val Loss: 0.4873\n",
      "Epoch 52, Train Loss: 0.6584, Val Loss: 0.4803\n",
      "Epoch 53, Train Loss: 0.6090, Val Loss: 0.4453\n",
      "Epoch 54, Train Loss: 0.6116, Val Loss: 0.4240\n",
      "Epoch 55, Train Loss: 0.5921, Val Loss: 0.4259\n",
      "Epoch 56, Train Loss: 0.5653, Val Loss: 0.4359\n",
      "Epoch 57, Train Loss: 0.5620, Val Loss: 0.3913\n",
      "Epoch 58, Train Loss: 0.5482, Val Loss: 0.3979\n",
      "Epoch 59, Train Loss: 0.5336, Val Loss: 0.4045\n",
      "Epoch 60, Train Loss: 0.5359, Val Loss: 0.3836\n",
      "Epoch 61, Train Loss: 0.5126, Val Loss: 0.3570\n",
      "Epoch 62, Train Loss: 0.5028, Val Loss: 0.3644\n",
      "Epoch 63, Train Loss: 0.4738, Val Loss: 0.3739\n",
      "Epoch 64, Train Loss: 0.4852, Val Loss: 0.3866\n",
      "Epoch 65, Train Loss: 0.4783, Val Loss: 0.3463\n",
      "Epoch 66, Train Loss: 0.4608, Val Loss: 0.3298\n",
      "Epoch 67, Train Loss: 0.4539, Val Loss: 0.3315\n",
      "Epoch 68, Train Loss: 0.4433, Val Loss: 0.3483\n",
      "Epoch 69, Train Loss: 0.4375, Val Loss: 0.3425\n",
      "Epoch 70, Train Loss: 0.4058, Val Loss: 0.3572\n"
     ]
    }
   ],
   "source": [
    "single_stacked_curve = train(Single_Stacked_Model, optimizer_single_stacked, 70, train_dataloader, val_dataloader)\n",
    "torch.save(Single_Stacked_Model.state_dict(), 'models/Single_Stacked_Model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "Single_InterWeaved_Model = InterWeavedModel([model1, model1], num_models = 2)\n",
    "optimizer_single_interweaved = torch.optim.Adam(Single_InterWeaved_Model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models still match their original states\n"
     ]
    }
   ],
   "source": [
    "if comparator.models_match(model1, model2):\n",
    "    print(\"Both models still match their original states\")\n",
    "else:\n",
    "    print(\"At least one model has changed from its original state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 3.0863, Val Loss: 2.2281\n",
      "Epoch 2, Train Loss: 2.7963, Val Loss: 2.0849\n",
      "Epoch 3, Train Loss: 2.6595, Val Loss: 1.9941\n",
      "Epoch 4, Train Loss: 2.5416, Val Loss: 1.9157\n",
      "Epoch 5, Train Loss: 2.4308, Val Loss: 1.7856\n",
      "Epoch 6, Train Loss: 2.3360, Val Loss: 1.7823\n",
      "Epoch 7, Train Loss: 2.2686, Val Loss: 1.6642\n",
      "Epoch 8, Train Loss: 2.1849, Val Loss: 1.6373\n",
      "Epoch 9, Train Loss: 2.1150, Val Loss: 1.5372\n",
      "Epoch 10, Train Loss: 2.0421, Val Loss: 1.4596\n",
      "Epoch 11, Train Loss: 1.9943, Val Loss: 1.4406\n",
      "Epoch 12, Train Loss: 1.9134, Val Loss: 1.4272\n",
      "Epoch 13, Train Loss: 1.8422, Val Loss: 1.3491\n",
      "Epoch 14, Train Loss: 1.7856, Val Loss: 1.3239\n",
      "Epoch 15, Train Loss: 1.7182, Val Loss: 1.2695\n",
      "Epoch 16, Train Loss: 1.6916, Val Loss: 1.2118\n",
      "Epoch 17, Train Loss: 1.6426, Val Loss: 1.1472\n",
      "Epoch 18, Train Loss: 1.6056, Val Loss: 1.1558\n",
      "Epoch 19, Train Loss: 1.5409, Val Loss: 1.1271\n",
      "Epoch 20, Train Loss: 1.5141, Val Loss: 1.0885\n",
      "Epoch 21, Train Loss: 1.4446, Val Loss: 1.0679\n",
      "Epoch 22, Train Loss: 1.4115, Val Loss: 1.0012\n",
      "Epoch 23, Train Loss: 1.3925, Val Loss: 1.0071\n",
      "Epoch 24, Train Loss: 1.3338, Val Loss: 1.0054\n",
      "Epoch 25, Train Loss: 1.3030, Val Loss: 0.9620\n",
      "Epoch 26, Train Loss: 1.2799, Val Loss: 0.9172\n",
      "Epoch 27, Train Loss: 1.2334, Val Loss: 0.8768\n",
      "Epoch 28, Train Loss: 1.1912, Val Loss: 0.8304\n",
      "Epoch 29, Train Loss: 1.1610, Val Loss: 0.7871\n",
      "Epoch 30, Train Loss: 1.1324, Val Loss: 0.8206\n",
      "Epoch 31, Train Loss: 1.0937, Val Loss: 0.8221\n",
      "Epoch 32, Train Loss: 1.0666, Val Loss: 0.7823\n",
      "Epoch 33, Train Loss: 1.0396, Val Loss: 0.7423\n",
      "Epoch 34, Train Loss: 1.0182, Val Loss: 0.7523\n",
      "Epoch 35, Train Loss: 0.9965, Val Loss: 0.6968\n",
      "Epoch 36, Train Loss: 0.9610, Val Loss: 0.7134\n",
      "Epoch 37, Train Loss: 0.9421, Val Loss: 0.6637\n",
      "Epoch 38, Train Loss: 0.9052, Val Loss: 0.6699\n",
      "Epoch 39, Train Loss: 0.8775, Val Loss: 0.6241\n",
      "Epoch 40, Train Loss: 0.8593, Val Loss: 0.6037\n",
      "Epoch 41, Train Loss: 0.8471, Val Loss: 0.6039\n",
      "Epoch 42, Train Loss: 0.8158, Val Loss: 0.5881\n",
      "Epoch 43, Train Loss: 0.8002, Val Loss: 0.5622\n",
      "Epoch 44, Train Loss: 0.7694, Val Loss: 0.6212\n",
      "Epoch 45, Train Loss: 0.7599, Val Loss: 0.5180\n",
      "Epoch 46, Train Loss: 0.7399, Val Loss: 0.5267\n",
      "Epoch 47, Train Loss: 0.7143, Val Loss: 0.4967\n",
      "Epoch 48, Train Loss: 0.6959, Val Loss: 0.5431\n",
      "Epoch 49, Train Loss: 0.6821, Val Loss: 0.4883\n",
      "Epoch 50, Train Loss: 0.6638, Val Loss: 0.4806\n",
      "Epoch 51, Train Loss: 0.6584, Val Loss: 0.4606\n",
      "Epoch 52, Train Loss: 0.6327, Val Loss: 0.4644\n",
      "Epoch 53, Train Loss: 0.6119, Val Loss: 0.4402\n",
      "Epoch 54, Train Loss: 0.6224, Val Loss: 0.4553\n",
      "Epoch 55, Train Loss: 0.5952, Val Loss: 0.4593\n",
      "Epoch 56, Train Loss: 0.5789, Val Loss: 0.3998\n",
      "Epoch 57, Train Loss: 0.5691, Val Loss: 0.4288\n",
      "Epoch 58, Train Loss: 0.5419, Val Loss: 0.4074\n",
      "Epoch 59, Train Loss: 0.5250, Val Loss: 0.4084\n",
      "Epoch 60, Train Loss: 0.5242, Val Loss: 0.3373\n",
      "Epoch 61, Train Loss: 0.4977, Val Loss: 0.3870\n",
      "Epoch 62, Train Loss: 0.5020, Val Loss: 0.3712\n",
      "Epoch 63, Train Loss: 0.4835, Val Loss: 0.3686\n",
      "Epoch 64, Train Loss: 0.4665, Val Loss: 0.3577\n",
      "Epoch 65, Train Loss: 0.4711, Val Loss: 0.3596\n",
      "Epoch 66, Train Loss: 0.4445, Val Loss: 0.3288\n",
      "Epoch 67, Train Loss: 0.4516, Val Loss: 0.3295\n",
      "Epoch 68, Train Loss: 0.4196, Val Loss: 0.3415\n",
      "Epoch 69, Train Loss: 0.4272, Val Loss: 0.3373\n",
      "Epoch 70, Train Loss: 0.4114, Val Loss: 0.3347\n"
     ]
    }
   ],
   "source": [
    "single_interweaved_curve = train(Single_InterWeaved_Model, optimizer_single_interweaved, 70, train_dataloader, val_dataloader)\n",
    "torch.save(Single_InterWeaved_Model.state_dict(), 'models/Single_InterWeaved_Model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame({'CrossLiGOv2 depth+width':clv2_depth_width_curve,'StackLiGOv2 depth+width':slv2_depth_width_curve,'CrossLiGO Depth+Width':cl_depth_width_curve, 'CrossLiGO depth':cl_depth_curve,'Big Model Depth+Width': scratch_depth_width, 'Big Model Depth': scratch_depth,'StackLiGO depth+width': sl_depth_width_curve, 'StackLiGO depth only': sl_depth_curve,'LiGO width only': ligo_width_curve, 'LiGO depth only': ligo_depth_curve, 'LiGO depth + width': ligo_depth_width_curve})\n",
    "\n",
    "# Create a DataFrame with the data\n",
    "df = pd.DataFrame({'Scratch': scratch_curve, 'Stacked Model': stack_curve, 'Stacked Model AVG': stack_avg_curve, 'Stacked Model Same': stack_same_curve, 'Interweaved Model': interweaved_curve, 'Interweaved Model AVG': interweaved_avg_curve, 'Interweaved Model Same': interweaved_same_curve, 'Single Stacked Model': single_stacked_curve, 'Single Interweaved Model': single_interweaved_curve})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('Wiki2_multi_model_curves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the split points\n",
    "train_split_3 = len(train_dataset) // 3\n",
    "train_split_2_3 = 2 * len(train_dataset) // 3\n",
    "val_split_3 = len(val_dataset) // 3\n",
    "val_split_2_3 = 2 * len(val_dataset) // 3\n",
    "\n",
    "# Create subsets for each third\n",
    "train_dataset_1_3 = Subset(train_dataset, range(train_split_3))\n",
    "train_dataset_2_3 = Subset(train_dataset, range(train_split_3, train_split_2_3))\n",
    "train_dataset_3_3 = Subset(train_dataset, range(train_split_2_3, len(train_dataset)))\n",
    "\n",
    "val_dataset_1_3 = Subset(val_dataset, range(val_split_3))\n",
    "val_dataset_2_3 = Subset(val_dataset, range(val_split_3, val_split_2_3))\n",
    "val_dataset_3_3 = Subset(val_dataset, range(val_split_2_3, len(val_dataset)))\n",
    "\n",
    "# Create DataLoaders for each third\n",
    "train_dataloader_1_3 = DataLoader(train_dataset_1_3, shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "train_dataloader_2_3 = DataLoader(train_dataset_2_3, shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "train_dataloader_3_3 = DataLoader(train_dataset_3_3, shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "val_dataloader_1_3 = DataLoader(val_dataset_1_3, shuffle=False, batch_size=batch_size, collate_fn=data_collator)\n",
    "val_dataloader_2_3 = DataLoader(val_dataset_2_3, shuffle=False, batch_size=batch_size, collate_fn=data_collator)\n",
    "val_dataloader_3_3 = DataLoader(val_dataset_3_3, shuffle=False, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "# Convert to lists if necessary\n",
    "train_dataloader_1_3 = list(train_dataloader_1_3)\n",
    "train_dataloader_2_3 = list(train_dataloader_2_3)\n",
    "train_dataloader_3_3 = list(train_dataloader_3_3)\n",
    "\n",
    "val_dataloader_1_3 = list(val_dataloader_1_3)\n",
    "val_dataloader_2_3 = list(val_dataloader_2_3)\n",
    "val_dataloader_3_3 = list(val_dataloader_3_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models on each third\n",
    "model3_1 = Model(4).to(device)\n",
    "optimizer3_1 = torch.optim.Adam(model3_1.parameters(), lr=learning_rate)\n",
    "\n",
    "model3_2 = Model(4).to(device)\n",
    "optimizer3_2 = torch.optim.Adam(model3_2.parameters(), lr=learning_rate)\n",
    "\n",
    "model3_3 = Model(4).to(device)\n",
    "optimizer3_3 = torch.optim.Adam(model3_3.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 10.6026, Val Loss: 9.9560\n",
      "Epoch 2, Train Loss: 9.8454, Val Loss: 9.1769\n",
      "Epoch 3, Train Loss: 9.1847, Val Loss: 8.4479\n",
      "Epoch 4, Train Loss: 8.5373, Val Loss: 7.6288\n",
      "Epoch 5, Train Loss: 7.9325, Val Loss: 7.0317\n",
      "Epoch 6, Train Loss: 7.3871, Val Loss: 6.4966\n",
      "Epoch 7, Train Loss: 6.8901, Val Loss: 5.9738\n",
      "Epoch 8, Train Loss: 6.4270, Val Loss: 5.6054\n",
      "Epoch 9, Train Loss: 6.0248, Val Loss: 5.1042\n",
      "Epoch 10, Train Loss: 5.6537, Val Loss: 4.8712\n",
      "Epoch 11, Train Loss: 5.3430, Val Loss: 4.4598\n",
      "Epoch 12, Train Loss: 5.0551, Val Loss: 4.2213\n",
      "Epoch 13, Train Loss: 4.7930, Val Loss: 3.9630\n",
      "Epoch 14, Train Loss: 4.6064, Val Loss: 3.8179\n",
      "Epoch 15, Train Loss: 4.3880, Val Loss: 3.6199\n",
      "Epoch 16, Train Loss: 4.2019, Val Loss: 3.4821\n",
      "Epoch 17, Train Loss: 4.0214, Val Loss: 3.3583\n",
      "Epoch 18, Train Loss: 3.8823, Val Loss: 3.1871\n",
      "Epoch 19, Train Loss: 3.7296, Val Loss: 3.0652\n",
      "Epoch 20, Train Loss: 3.5753, Val Loss: 2.9288\n",
      "Epoch 21, Train Loss: 3.4352, Val Loss: 2.8332\n",
      "Epoch 22, Train Loss: 3.3060, Val Loss: 2.6882\n",
      "Epoch 23, Train Loss: 3.1841, Val Loss: 2.5915\n",
      "Epoch 24, Train Loss: 3.0948, Val Loss: 2.4986\n",
      "Epoch 25, Train Loss: 2.9366, Val Loss: 2.4064\n",
      "Epoch 26, Train Loss: 2.8218, Val Loss: 2.3005\n",
      "Epoch 27, Train Loss: 2.7424, Val Loss: 2.2471\n",
      "Epoch 28, Train Loss: 2.6124, Val Loss: 2.1554\n",
      "Epoch 29, Train Loss: 2.5181, Val Loss: 2.0938\n",
      "Epoch 30, Train Loss: 2.4140, Val Loss: 1.9857\n",
      "Epoch 1, Train Loss: 10.7145, Val Loss: 10.1260\n",
      "Epoch 2, Train Loss: 9.9318, Val Loss: 9.3770\n",
      "Epoch 3, Train Loss: 9.2949, Val Loss: 8.6480\n",
      "Epoch 4, Train Loss: 8.6475, Val Loss: 7.9935\n",
      "Epoch 5, Train Loss: 8.0608, Val Loss: 7.3636\n",
      "Epoch 6, Train Loss: 7.5091, Val Loss: 6.7933\n",
      "Epoch 7, Train Loss: 6.9861, Val Loss: 6.2374\n",
      "Epoch 8, Train Loss: 6.5379, Val Loss: 5.7768\n",
      "Epoch 9, Train Loss: 6.1204, Val Loss: 5.4193\n",
      "Epoch 10, Train Loss: 5.7539, Val Loss: 5.0519\n",
      "Epoch 11, Train Loss: 5.4246, Val Loss: 4.7316\n",
      "Epoch 12, Train Loss: 5.1492, Val Loss: 4.4815\n",
      "Epoch 13, Train Loss: 4.8878, Val Loss: 4.2466\n",
      "Epoch 14, Train Loss: 4.6525, Val Loss: 4.0281\n",
      "Epoch 15, Train Loss: 4.4479, Val Loss: 3.8830\n",
      "Epoch 16, Train Loss: 4.2747, Val Loss: 3.6957\n",
      "Epoch 17, Train Loss: 4.0706, Val Loss: 3.5107\n",
      "Epoch 18, Train Loss: 3.9258, Val Loss: 3.4446\n",
      "Epoch 19, Train Loss: 3.7738, Val Loss: 3.2783\n",
      "Epoch 20, Train Loss: 3.6212, Val Loss: 3.0978\n",
      "Epoch 21, Train Loss: 3.5040, Val Loss: 2.9836\n",
      "Epoch 22, Train Loss: 3.3743, Val Loss: 2.8410\n",
      "Epoch 23, Train Loss: 3.2216, Val Loss: 2.7089\n",
      "Epoch 24, Train Loss: 3.0776, Val Loss: 2.6504\n",
      "Epoch 25, Train Loss: 2.9595, Val Loss: 2.5435\n",
      "Epoch 26, Train Loss: 2.8629, Val Loss: 2.4157\n",
      "Epoch 27, Train Loss: 2.7232, Val Loss: 2.3237\n",
      "Epoch 28, Train Loss: 2.6416, Val Loss: 2.2124\n",
      "Epoch 29, Train Loss: 2.5450, Val Loss: 2.1607\n",
      "Epoch 30, Train Loss: 2.4150, Val Loss: 2.0801\n",
      "Epoch 1, Train Loss: 10.5871, Val Loss: 9.9017\n",
      "Epoch 2, Train Loss: 9.8108, Val Loss: 9.1686\n",
      "Epoch 3, Train Loss: 9.1600, Val Loss: 8.4348\n",
      "Epoch 4, Train Loss: 8.5379, Val Loss: 7.7638\n",
      "Epoch 5, Train Loss: 7.9390, Val Loss: 7.2119\n",
      "Epoch 6, Train Loss: 7.4212, Val Loss: 6.6058\n",
      "Epoch 7, Train Loss: 6.9033, Val Loss: 6.1053\n",
      "Epoch 8, Train Loss: 6.4827, Val Loss: 5.6539\n",
      "Epoch 9, Train Loss: 6.0866, Val Loss: 5.3021\n",
      "Epoch 10, Train Loss: 5.7176, Val Loss: 4.9098\n",
      "Epoch 11, Train Loss: 5.4360, Val Loss: 4.6298\n",
      "Epoch 12, Train Loss: 5.1191, Val Loss: 4.2818\n",
      "Epoch 13, Train Loss: 4.8725, Val Loss: 4.1501\n",
      "Epoch 14, Train Loss: 4.6240, Val Loss: 3.8843\n",
      "Epoch 15, Train Loss: 4.4470, Val Loss: 3.6857\n",
      "Epoch 16, Train Loss: 4.2576, Val Loss: 3.5993\n",
      "Epoch 17, Train Loss: 4.0449, Val Loss: 3.4329\n",
      "Epoch 18, Train Loss: 3.8739, Val Loss: 3.2784\n",
      "Epoch 19, Train Loss: 3.7071, Val Loss: 3.1522\n",
      "Epoch 20, Train Loss: 3.5841, Val Loss: 3.0285\n",
      "Epoch 21, Train Loss: 3.4636, Val Loss: 2.8866\n",
      "Epoch 22, Train Loss: 3.3211, Val Loss: 2.7435\n",
      "Epoch 23, Train Loss: 3.2150, Val Loss: 2.6710\n",
      "Epoch 24, Train Loss: 3.0629, Val Loss: 2.5794\n",
      "Epoch 25, Train Loss: 2.9457, Val Loss: 2.4396\n",
      "Epoch 26, Train Loss: 2.8438, Val Loss: 2.3603\n",
      "Epoch 27, Train Loss: 2.7352, Val Loss: 2.3120\n",
      "Epoch 28, Train Loss: 2.6120, Val Loss: 2.2240\n",
      "Epoch 29, Train Loss: 2.5239, Val Loss: 2.1647\n",
      "Epoch 30, Train Loss: 2.4519, Val Loss: 2.0704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.901678466796875,\n",
       " 9.16861801147461,\n",
       " 8.434764671325684,\n",
       " 7.763831710815429,\n",
       " 7.211936950683594,\n",
       " 6.605757999420166,\n",
       " 6.105336475372314,\n",
       " 5.653909206390381,\n",
       " 5.302112197875976,\n",
       " 4.909782600402832,\n",
       " 4.62977294921875,\n",
       " 4.281831550598144,\n",
       " 4.15011625289917,\n",
       " 3.884261703491211,\n",
       " 3.6857176780700684,\n",
       " 3.599269390106201,\n",
       " 3.432884931564331,\n",
       " 3.278370809555054,\n",
       " 3.1522472858428956,\n",
       " 3.0284979343414307,\n",
       " 2.8865599155426027,\n",
       " 2.7435003757476806,\n",
       " 2.67102255821228,\n",
       " 2.5794060230255127,\n",
       " 2.4396080017089843,\n",
       " 2.360327434539795,\n",
       " 2.311966562271118,\n",
       " 2.2240476608276367,\n",
       " 2.1647094249725343,\n",
       " 2.070417356491089]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model3_1, optimizer3_1, 30, train_dataloader_1_3, val_dataloader_1_3)\n",
    "train(model3_2, optimizer3_2, 30, train_dataloader_2_3, val_dataloader_2_3)\n",
    "train(model3_3, optimizer3_3, 30, train_dataloader_3_3, val_dataloader_3_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triple_Stacked_Model = StackedModel([model3_1, model3_2, model3_3], num_models = 3)\n",
    "optimizer_triple_stacked = torch.optim.Adam(Triple_Stacked_Model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelComparator3:\n",
    "    def __init__(self):\n",
    "        self.model1_state = None\n",
    "        self.model2_state = None\n",
    "        self.model3_state = None\n",
    "\n",
    "    def save_model_states(self, model1, model2, model3):\n",
    "        self.model1_state = self._get_model_state(model1)\n",
    "        self.model2_state = self._get_model_state(model2)\n",
    "        self.model3_state = self._get_model_state(model3)\n",
    "        print(\"Model states saved successfully.\")\n",
    "\n",
    "    def _get_model_state(self, model):\n",
    "        return {name: param.clone().detach().cpu() for name, param in model.named_parameters()}\n",
    "\n",
    "    def models_match(self, check_model1, check_model2, check_model3):\n",
    "        if self.model1_state is None or self.model2_state is None or self.model3_state is None:\n",
    "            raise ValueError(\"Model states have not been saved. Call save_model_states() first.\")\n",
    "\n",
    "        match1 = self._compare_model(check_model1, self.model1_state)\n",
    "        match2 = self._compare_model(check_model2, self.model2_state)\n",
    "        match3 = self._compare_model(check_model3, self.model3_state)\n",
    "        \n",
    "        return match1 and match2 and match3\n",
    "\n",
    "    def _compare_model(self, model, saved_state):\n",
    "        current_state = self._get_model_state(model)\n",
    "        \n",
    "        if current_state.keys() != saved_state.keys():\n",
    "            print(f\"Model has different parameter structure than the saved state.\")\n",
    "            return False\n",
    "\n",
    "        for name, param in current_state.items():\n",
    "            if not torch.allclose(param, saved_state[name]):\n",
    "                print(f\"Mismatch in parameter: {name}\")\n",
    "                return False\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model states saved successfully.\n"
     ]
    }
   ],
   "source": [
    "comparator3 = ModelComparator3()\n",
    "comparator3.save_model_states(model3_1, model3_2, model3_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 7.0888, Val Loss: 5.1498\n",
      "Epoch 2, Train Loss: 5.7206, Val Loss: 4.6308\n",
      "Epoch 3, Train Loss: 5.2609, Val Loss: 4.2790\n",
      "Epoch 4, Train Loss: 4.9342, Val Loss: 4.0127\n",
      "Epoch 5, Train Loss: 4.7309, Val Loss: 3.8195\n",
      "Epoch 6, Train Loss: 4.5863, Val Loss: 3.7027\n",
      "Epoch 7, Train Loss: 4.3971, Val Loss: 3.5709\n",
      "Epoch 8, Train Loss: 4.2474, Val Loss: 3.3951\n",
      "Epoch 9, Train Loss: 4.1358, Val Loss: 3.2507\n",
      "Epoch 10, Train Loss: 4.0106, Val Loss: 3.1572\n",
      "Epoch 11, Train Loss: 3.8804, Val Loss: 3.0371\n",
      "Epoch 12, Train Loss: 3.7662, Val Loss: 2.9322\n",
      "Epoch 13, Train Loss: 3.6455, Val Loss: 2.8655\n",
      "Epoch 14, Train Loss: 3.5362, Val Loss: 2.7392\n",
      "Epoch 15, Train Loss: 3.4265, Val Loss: 2.6508\n",
      "Epoch 16, Train Loss: 3.3732, Val Loss: 2.5829\n",
      "Epoch 17, Train Loss: 3.2500, Val Loss: 2.4711\n",
      "Epoch 18, Train Loss: 3.1659, Val Loss: 2.4632\n",
      "Epoch 19, Train Loss: 3.0838, Val Loss: 2.2653\n",
      "Epoch 20, Train Loss: 3.0120, Val Loss: 2.2394\n",
      "Epoch 21, Train Loss: 2.9009, Val Loss: 2.1537\n",
      "Epoch 22, Train Loss: 2.8255, Val Loss: 2.0911\n",
      "Epoch 23, Train Loss: 2.7783, Val Loss: 2.0633\n",
      "Epoch 24, Train Loss: 2.7232, Val Loss: 1.9344\n",
      "Epoch 25, Train Loss: 2.6425, Val Loss: 1.9200\n",
      "Epoch 26, Train Loss: 2.5554, Val Loss: 1.8833\n",
      "Epoch 27, Train Loss: 2.4786, Val Loss: 1.8164\n",
      "Epoch 28, Train Loss: 2.4137, Val Loss: 1.6637\n",
      "Epoch 29, Train Loss: 2.3935, Val Loss: 1.6868\n",
      "Epoch 30, Train Loss: 2.2842, Val Loss: 1.6231\n",
      "Epoch 31, Train Loss: 2.2501, Val Loss: 1.5593\n",
      "Epoch 32, Train Loss: 2.1926, Val Loss: 1.5378\n",
      "Epoch 33, Train Loss: 2.1000, Val Loss: 1.5267\n",
      "Epoch 34, Train Loss: 2.0670, Val Loss: 1.4478\n",
      "Epoch 35, Train Loss: 2.0225, Val Loss: 1.4078\n",
      "Epoch 36, Train Loss: 1.9630, Val Loss: 1.3308\n",
      "Epoch 37, Train Loss: 1.9279, Val Loss: 1.3243\n",
      "Epoch 38, Train Loss: 1.8737, Val Loss: 1.3719\n",
      "Epoch 39, Train Loss: 1.8358, Val Loss: 1.2374\n",
      "Epoch 40, Train Loss: 1.8012, Val Loss: 1.2691\n",
      "Epoch 41, Train Loss: 1.7395, Val Loss: 1.1950\n",
      "Epoch 42, Train Loss: 1.6736, Val Loss: 1.1733\n",
      "Epoch 43, Train Loss: 1.6632, Val Loss: 1.1793\n",
      "Epoch 44, Train Loss: 1.5879, Val Loss: 1.1050\n",
      "Epoch 45, Train Loss: 1.5798, Val Loss: 1.0547\n",
      "Epoch 46, Train Loss: 1.5161, Val Loss: 1.0162\n",
      "Epoch 47, Train Loss: 1.4927, Val Loss: 1.0150\n",
      "Epoch 48, Train Loss: 1.4507, Val Loss: 0.9731\n",
      "Epoch 49, Train Loss: 1.4000, Val Loss: 0.9646\n",
      "Epoch 50, Train Loss: 1.3791, Val Loss: 0.9071\n",
      "Epoch 51, Train Loss: 1.3455, Val Loss: 0.8887\n",
      "Epoch 52, Train Loss: 1.3181, Val Loss: 0.9186\n",
      "Epoch 53, Train Loss: 1.2877, Val Loss: 0.8798\n",
      "Epoch 54, Train Loss: 1.2403, Val Loss: 0.9072\n",
      "Epoch 55, Train Loss: 1.2301, Val Loss: 0.8731\n",
      "Epoch 56, Train Loss: 1.1671, Val Loss: 0.7935\n",
      "Epoch 57, Train Loss: 1.1721, Val Loss: 0.7638\n",
      "Epoch 58, Train Loss: 1.1114, Val Loss: 0.7327\n",
      "Epoch 59, Train Loss: 1.1109, Val Loss: 0.8132\n",
      "Epoch 60, Train Loss: 1.0874, Val Loss: 0.7328\n",
      "Epoch 61, Train Loss: 1.0477, Val Loss: 0.7578\n",
      "Epoch 62, Train Loss: 1.0249, Val Loss: 0.6996\n",
      "Epoch 63, Train Loss: 0.9927, Val Loss: 0.6673\n",
      "Epoch 64, Train Loss: 0.9690, Val Loss: 0.6338\n",
      "Epoch 65, Train Loss: 0.9618, Val Loss: 0.6098\n",
      "Epoch 66, Train Loss: 0.9286, Val Loss: 0.6606\n",
      "Epoch 67, Train Loss: 0.9050, Val Loss: 0.6029\n",
      "Epoch 68, Train Loss: 0.8771, Val Loss: 0.5511\n",
      "Epoch 69, Train Loss: 0.8400, Val Loss: 0.5819\n",
      "Epoch 70, Train Loss: 0.8261, Val Loss: 0.5718\n"
     ]
    }
   ],
   "source": [
    "triple_stack_curve = train(Triple_Stacked_Model, optimizer_triple_stacked, 70, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triple_Stacked_Model_avg = StackedModelAVG([model3_1, model3_2, model3_3], num_models = 3)\n",
    "optimizer_triple_stacked_avg = torch.optim.Adam(Triple_Stacked_Model_avg.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All three models still match their original states\n"
     ]
    }
   ],
   "source": [
    "if comparator3.models_match(model3_1, model3_2, model3_3):\n",
    "    print(\"All three models still match their original states\")\n",
    "else:\n",
    "    print(\"At least one model has changed from its original state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 6.8336, Val Loss: 5.4024\n",
      "Epoch 2, Train Loss: 5.8907, Val Loss: 4.7956\n",
      "Epoch 3, Train Loss: 5.3967, Val Loss: 4.2601\n",
      "Epoch 4, Train Loss: 5.0064, Val Loss: 3.9746\n",
      "Epoch 5, Train Loss: 4.6478, Val Loss: 3.6117\n",
      "Epoch 6, Train Loss: 4.3942, Val Loss: 3.4223\n",
      "Epoch 7, Train Loss: 4.1624, Val Loss: 3.1754\n",
      "Epoch 8, Train Loss: 3.9815, Val Loss: 3.0170\n",
      "Epoch 9, Train Loss: 3.8155, Val Loss: 2.8531\n",
      "Epoch 10, Train Loss: 3.6399, Val Loss: 2.7411\n",
      "Epoch 11, Train Loss: 3.5004, Val Loss: 2.5578\n",
      "Epoch 12, Train Loss: 3.3865, Val Loss: 2.5280\n",
      "Epoch 13, Train Loss: 3.2514, Val Loss: 2.4563\n",
      "Epoch 14, Train Loss: 3.1373, Val Loss: 2.2713\n",
      "Epoch 15, Train Loss: 3.0168, Val Loss: 2.2331\n",
      "Epoch 16, Train Loss: 2.9188, Val Loss: 2.1564\n",
      "Epoch 17, Train Loss: 2.8469, Val Loss: 2.0611\n",
      "Epoch 18, Train Loss: 2.7413, Val Loss: 1.9869\n",
      "Epoch 19, Train Loss: 2.6413, Val Loss: 1.9430\n",
      "Epoch 20, Train Loss: 2.5412, Val Loss: 1.8430\n",
      "Epoch 21, Train Loss: 2.4821, Val Loss: 1.7578\n",
      "Epoch 22, Train Loss: 2.4116, Val Loss: 1.7159\n",
      "Epoch 23, Train Loss: 2.3284, Val Loss: 1.6703\n",
      "Epoch 24, Train Loss: 2.2421, Val Loss: 1.6122\n",
      "Epoch 25, Train Loss: 2.2031, Val Loss: 1.5207\n",
      "Epoch 26, Train Loss: 2.1483, Val Loss: 1.5037\n",
      "Epoch 27, Train Loss: 2.0635, Val Loss: 1.4365\n",
      "Epoch 28, Train Loss: 1.9955, Val Loss: 1.3978\n",
      "Epoch 29, Train Loss: 1.9258, Val Loss: 1.3591\n",
      "Epoch 30, Train Loss: 1.8812, Val Loss: 1.3406\n",
      "Epoch 31, Train Loss: 1.8308, Val Loss: 1.2173\n",
      "Epoch 32, Train Loss: 1.7581, Val Loss: 1.2291\n",
      "Epoch 33, Train Loss: 1.7248, Val Loss: 1.1849\n",
      "Epoch 34, Train Loss: 1.6780, Val Loss: 1.2175\n",
      "Epoch 35, Train Loss: 1.6266, Val Loss: 1.1106\n",
      "Epoch 36, Train Loss: 1.5660, Val Loss: 1.0838\n",
      "Epoch 37, Train Loss: 1.5331, Val Loss: 1.0293\n",
      "Epoch 38, Train Loss: 1.4780, Val Loss: 0.9897\n",
      "Epoch 39, Train Loss: 1.4497, Val Loss: 1.0024\n",
      "Epoch 40, Train Loss: 1.4301, Val Loss: 0.9673\n",
      "Epoch 41, Train Loss: 1.3634, Val Loss: 0.9315\n",
      "Epoch 42, Train Loss: 1.3437, Val Loss: 0.9367\n",
      "Epoch 43, Train Loss: 1.2833, Val Loss: 0.8388\n",
      "Epoch 44, Train Loss: 1.2708, Val Loss: 0.8961\n",
      "Epoch 45, Train Loss: 1.2284, Val Loss: 0.7976\n",
      "Epoch 46, Train Loss: 1.2008, Val Loss: 0.8431\n",
      "Epoch 47, Train Loss: 1.1575, Val Loss: 0.7574\n",
      "Epoch 48, Train Loss: 1.1374, Val Loss: 0.7308\n",
      "Epoch 49, Train Loss: 1.0988, Val Loss: 0.7439\n",
      "Epoch 50, Train Loss: 1.0649, Val Loss: 0.7659\n",
      "Epoch 51, Train Loss: 1.0468, Val Loss: 0.7220\n",
      "Epoch 52, Train Loss: 0.9877, Val Loss: 0.6944\n",
      "Epoch 53, Train Loss: 0.9926, Val Loss: 0.6744\n",
      "Epoch 54, Train Loss: 0.9648, Val Loss: 0.6675\n",
      "Epoch 55, Train Loss: 0.9532, Val Loss: 0.6439\n",
      "Epoch 56, Train Loss: 0.9253, Val Loss: 0.5823\n",
      "Epoch 57, Train Loss: 0.9018, Val Loss: 0.6190\n",
      "Epoch 58, Train Loss: 0.8509, Val Loss: 0.6124\n",
      "Epoch 59, Train Loss: 0.8444, Val Loss: 0.5696\n",
      "Epoch 60, Train Loss: 0.8261, Val Loss: 0.5461\n",
      "Epoch 61, Train Loss: 0.8151, Val Loss: 0.5446\n",
      "Epoch 62, Train Loss: 0.7895, Val Loss: 0.5316\n",
      "Epoch 63, Train Loss: 0.7621, Val Loss: 0.5552\n",
      "Epoch 64, Train Loss: 0.7450, Val Loss: 0.5127\n",
      "Epoch 65, Train Loss: 0.7304, Val Loss: 0.4648\n",
      "Epoch 66, Train Loss: 0.7202, Val Loss: 0.5031\n",
      "Epoch 67, Train Loss: 0.7058, Val Loss: 0.5013\n",
      "Epoch 68, Train Loss: 0.6694, Val Loss: 0.4347\n",
      "Epoch 69, Train Loss: 0.6416, Val Loss: 0.4700\n",
      "Epoch 70, Train Loss: 0.6352, Val Loss: 0.4414\n"
     ]
    }
   ],
   "source": [
    "triple_stack_curve_avg = train(Triple_Stacked_Model_avg, optimizer_triple_stacked_avg, 70, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triple_Stacked_Model_same = StackedModel([model3_1, model3_2, model3_3], num_models = 3, same=True)\n",
    "optimizer_triple_stacked_same = torch.optim.Adam(Triple_Stacked_Model_same.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All three models still match their original states\n"
     ]
    }
   ],
   "source": [
    "if comparator3.models_match(model3_1, model3_2, model3_3):\n",
    "    print(\"All three models still match their original states\")\n",
    "else:\n",
    "    print(\"At least one model has changed from its original state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 4.7737, Val Loss: 2.9107\n",
      "Epoch 2, Train Loss: 3.6511, Val Loss: 2.5854\n",
      "Epoch 3, Train Loss: 3.2590, Val Loss: 2.3394\n",
      "Epoch 4, Train Loss: 3.0340, Val Loss: 2.1586\n",
      "Epoch 5, Train Loss: 2.8572, Val Loss: 2.0531\n",
      "Epoch 6, Train Loss: 2.7366, Val Loss: 1.9316\n",
      "Epoch 7, Train Loss: 2.5880, Val Loss: 1.8567\n",
      "Epoch 8, Train Loss: 2.4807, Val Loss: 1.7973\n",
      "Epoch 9, Train Loss: 2.3686, Val Loss: 1.7267\n",
      "Epoch 10, Train Loss: 2.3140, Val Loss: 1.6471\n",
      "Epoch 11, Train Loss: 2.2092, Val Loss: 1.5505\n",
      "Epoch 12, Train Loss: 2.1513, Val Loss: 1.5332\n",
      "Epoch 13, Train Loss: 2.0715, Val Loss: 1.4797\n",
      "Epoch 14, Train Loss: 1.9944, Val Loss: 1.4178\n",
      "Epoch 15, Train Loss: 1.9048, Val Loss: 1.3204\n",
      "Epoch 16, Train Loss: 1.8604, Val Loss: 1.3184\n",
      "Epoch 17, Train Loss: 1.8276, Val Loss: 1.2452\n",
      "Epoch 18, Train Loss: 1.7700, Val Loss: 1.2089\n",
      "Epoch 19, Train Loss: 1.7035, Val Loss: 1.1769\n",
      "Epoch 20, Train Loss: 1.6576, Val Loss: 1.1838\n",
      "Epoch 21, Train Loss: 1.6238, Val Loss: 1.1233\n",
      "Epoch 22, Train Loss: 1.5615, Val Loss: 1.0721\n",
      "Epoch 23, Train Loss: 1.4985, Val Loss: 1.0894\n",
      "Epoch 24, Train Loss: 1.4485, Val Loss: 1.0397\n",
      "Epoch 25, Train Loss: 1.4238, Val Loss: 1.0062\n",
      "Epoch 26, Train Loss: 1.3737, Val Loss: 0.9537\n",
      "Epoch 27, Train Loss: 1.3462, Val Loss: 0.9181\n",
      "Epoch 28, Train Loss: 1.3204, Val Loss: 0.9147\n",
      "Epoch 29, Train Loss: 1.2699, Val Loss: 0.9045\n",
      "Epoch 30, Train Loss: 1.2512, Val Loss: 0.8575\n",
      "Epoch 31, Train Loss: 1.2212, Val Loss: 0.8380\n",
      "Epoch 32, Train Loss: 1.1594, Val Loss: 0.8287\n",
      "Epoch 33, Train Loss: 1.1484, Val Loss: 0.8210\n",
      "Epoch 34, Train Loss: 1.1171, Val Loss: 0.7960\n",
      "Epoch 35, Train Loss: 1.0735, Val Loss: 0.7153\n",
      "Epoch 36, Train Loss: 1.0544, Val Loss: 0.7491\n",
      "Epoch 37, Train Loss: 1.0326, Val Loss: 0.7524\n",
      "Epoch 38, Train Loss: 1.0154, Val Loss: 0.7082\n",
      "Epoch 39, Train Loss: 0.9761, Val Loss: 0.7270\n",
      "Epoch 40, Train Loss: 0.9638, Val Loss: 0.6547\n",
      "Epoch 41, Train Loss: 0.9247, Val Loss: 0.6384\n",
      "Epoch 42, Train Loss: 0.9018, Val Loss: 0.6457\n",
      "Epoch 43, Train Loss: 0.8854, Val Loss: 0.6103\n",
      "Epoch 44, Train Loss: 0.8636, Val Loss: 0.5828\n",
      "Epoch 45, Train Loss: 0.8378, Val Loss: 0.5541\n",
      "Epoch 46, Train Loss: 0.8092, Val Loss: 0.5788\n",
      "Epoch 47, Train Loss: 0.7967, Val Loss: 0.5938\n",
      "Epoch 48, Train Loss: 0.7713, Val Loss: 0.5879\n",
      "Epoch 49, Train Loss: 0.7499, Val Loss: 0.5283\n",
      "Epoch 50, Train Loss: 0.7312, Val Loss: 0.5341\n",
      "Epoch 51, Train Loss: 0.7169, Val Loss: 0.5224\n",
      "Epoch 52, Train Loss: 0.7027, Val Loss: 0.4892\n",
      "Epoch 53, Train Loss: 0.6812, Val Loss: 0.4793\n",
      "Epoch 54, Train Loss: 0.6594, Val Loss: 0.4856\n",
      "Epoch 55, Train Loss: 0.6486, Val Loss: 0.4494\n",
      "Epoch 56, Train Loss: 0.6313, Val Loss: 0.4510\n",
      "Epoch 57, Train Loss: 0.6293, Val Loss: 0.4424\n",
      "Epoch 58, Train Loss: 0.5986, Val Loss: 0.4330\n",
      "Epoch 59, Train Loss: 0.6065, Val Loss: 0.4163\n",
      "Epoch 60, Train Loss: 0.5634, Val Loss: 0.4181\n",
      "Epoch 61, Train Loss: 0.5611, Val Loss: 0.4004\n",
      "Epoch 62, Train Loss: 0.5484, Val Loss: 0.3864\n",
      "Epoch 63, Train Loss: 0.5380, Val Loss: 0.4224\n",
      "Epoch 64, Train Loss: 0.5340, Val Loss: 0.4334\n",
      "Epoch 65, Train Loss: 0.5224, Val Loss: 0.3616\n",
      "Epoch 66, Train Loss: 0.4988, Val Loss: 0.3503\n",
      "Epoch 67, Train Loss: 0.4903, Val Loss: 0.3550\n",
      "Epoch 68, Train Loss: 0.4722, Val Loss: 0.3470\n",
      "Epoch 69, Train Loss: 0.4667, Val Loss: 0.3299\n",
      "Epoch 70, Train Loss: 0.4674, Val Loss: 0.3292\n"
     ]
    }
   ],
   "source": [
    "triple_stack_curve_same = train(Triple_Stacked_Model_same, optimizer_triple_stacked_same, 70, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triple_InterWeaved_Model = InterWeavedModel([model3_1, model3_2, model3_3], num_models = 3)\n",
    "optimizer_triple_interweaved = torch.optim.Adam(Triple_InterWeaved_Model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All three models still match their original states\n"
     ]
    }
   ],
   "source": [
    "if comparator3.models_match(model3_1, model3_2, model3_3):\n",
    "    print(\"All three models still match their original states\")\n",
    "else:\n",
    "    print(\"At least one model has changed from its original state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 7.2647, Val Loss: 5.3183\n",
      "Epoch 2, Train Loss: 5.8842, Val Loss: 4.6263\n",
      "Epoch 3, Train Loss: 5.3469, Val Loss: 4.2850\n",
      "Epoch 4, Train Loss: 5.0590, Val Loss: 4.0760\n",
      "Epoch 5, Train Loss: 4.8010, Val Loss: 3.9126\n",
      "Epoch 6, Train Loss: 4.6024, Val Loss: 3.7693\n",
      "Epoch 7, Train Loss: 4.4194, Val Loss: 3.5675\n",
      "Epoch 8, Train Loss: 4.3052, Val Loss: 3.4546\n",
      "Epoch 9, Train Loss: 4.1443, Val Loss: 3.3169\n",
      "Epoch 10, Train Loss: 4.0242, Val Loss: 3.1810\n",
      "Epoch 11, Train Loss: 3.8971, Val Loss: 3.0075\n",
      "Epoch 12, Train Loss: 3.8172, Val Loss: 2.8612\n",
      "Epoch 13, Train Loss: 3.6771, Val Loss: 2.8676\n",
      "Epoch 14, Train Loss: 3.5604, Val Loss: 2.7281\n",
      "Epoch 15, Train Loss: 3.4860, Val Loss: 2.5993\n",
      "Epoch 16, Train Loss: 3.3675, Val Loss: 2.6164\n",
      "Epoch 17, Train Loss: 3.2524, Val Loss: 2.4153\n",
      "Epoch 18, Train Loss: 3.1614, Val Loss: 2.3486\n",
      "Epoch 19, Train Loss: 3.0719, Val Loss: 2.3076\n",
      "Epoch 20, Train Loss: 3.0017, Val Loss: 2.2542\n",
      "Epoch 21, Train Loss: 2.9038, Val Loss: 2.1273\n",
      "Epoch 22, Train Loss: 2.8432, Val Loss: 2.0952\n",
      "Epoch 23, Train Loss: 2.7497, Val Loss: 1.9628\n",
      "Epoch 24, Train Loss: 2.6539, Val Loss: 1.9432\n",
      "Epoch 25, Train Loss: 2.6018, Val Loss: 1.9011\n",
      "Epoch 26, Train Loss: 2.5051, Val Loss: 1.8295\n",
      "Epoch 27, Train Loss: 2.4812, Val Loss: 1.8267\n",
      "Epoch 28, Train Loss: 2.3924, Val Loss: 1.6586\n",
      "Epoch 29, Train Loss: 2.3315, Val Loss: 1.6674\n",
      "Epoch 30, Train Loss: 2.2645, Val Loss: 1.5968\n",
      "Epoch 31, Train Loss: 2.1922, Val Loss: 1.5517\n",
      "Epoch 32, Train Loss: 2.1384, Val Loss: 1.5108\n",
      "Epoch 33, Train Loss: 2.0874, Val Loss: 1.4457\n",
      "Epoch 34, Train Loss: 2.0480, Val Loss: 1.4802\n",
      "Epoch 35, Train Loss: 1.9687, Val Loss: 1.4176\n",
      "Epoch 36, Train Loss: 1.9410, Val Loss: 1.3436\n",
      "Epoch 37, Train Loss: 1.8949, Val Loss: 1.2874\n",
      "Epoch 38, Train Loss: 1.8211, Val Loss: 1.2911\n",
      "Epoch 39, Train Loss: 1.8027, Val Loss: 1.2619\n",
      "Epoch 40, Train Loss: 1.7502, Val Loss: 1.1953\n",
      "Epoch 41, Train Loss: 1.7081, Val Loss: 1.1495\n",
      "Epoch 42, Train Loss: 1.6452, Val Loss: 1.1392\n",
      "Epoch 43, Train Loss: 1.6210, Val Loss: 1.1194\n",
      "Epoch 44, Train Loss: 1.5496, Val Loss: 1.0895\n",
      "Epoch 45, Train Loss: 1.5378, Val Loss: 1.0022\n",
      "Epoch 46, Train Loss: 1.4977, Val Loss: 1.0326\n",
      "Epoch 47, Train Loss: 1.4645, Val Loss: 1.0161\n",
      "Epoch 48, Train Loss: 1.4268, Val Loss: 0.9266\n",
      "Epoch 49, Train Loss: 1.3891, Val Loss: 0.9424\n",
      "Epoch 50, Train Loss: 1.3507, Val Loss: 0.9267\n",
      "Epoch 51, Train Loss: 1.2993, Val Loss: 0.8768\n",
      "Epoch 52, Train Loss: 1.2795, Val Loss: 0.8549\n",
      "Epoch 53, Train Loss: 1.2522, Val Loss: 0.8396\n",
      "Epoch 54, Train Loss: 1.2216, Val Loss: 0.8209\n",
      "Epoch 55, Train Loss: 1.1948, Val Loss: 0.8376\n",
      "Epoch 56, Train Loss: 1.1558, Val Loss: 0.7991\n",
      "Epoch 57, Train Loss: 1.1281, Val Loss: 0.7623\n",
      "Epoch 58, Train Loss: 1.0961, Val Loss: 0.7355\n",
      "Epoch 59, Train Loss: 1.0811, Val Loss: 0.7075\n",
      "Epoch 60, Train Loss: 1.0413, Val Loss: 0.7422\n",
      "Epoch 61, Train Loss: 1.0271, Val Loss: 0.6876\n",
      "Epoch 62, Train Loss: 1.0090, Val Loss: 0.6225\n",
      "Epoch 63, Train Loss: 0.9735, Val Loss: 0.6684\n",
      "Epoch 64, Train Loss: 0.9364, Val Loss: 0.6148\n",
      "Epoch 65, Train Loss: 0.9261, Val Loss: 0.6341\n",
      "Epoch 66, Train Loss: 0.8872, Val Loss: 0.6199\n",
      "Epoch 67, Train Loss: 0.8866, Val Loss: 0.6186\n",
      "Epoch 68, Train Loss: 0.8769, Val Loss: 0.5929\n",
      "Epoch 69, Train Loss: 0.8617, Val Loss: 0.5460\n",
      "Epoch 70, Train Loss: 0.8029, Val Loss: 0.5853\n"
     ]
    }
   ],
   "source": [
    "triple_interweaved_curve = train(Triple_InterWeaved_Model, optimizer_triple_interweaved, 70, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triple_InterWeaved_Model_avg = InterWeavedModelAVG([model3_1, model3_2, model3_3], num_models = 3)\n",
    "optimizer_triple_interweaved_avg = torch.optim.Adam(Triple_InterWeaved_Model_avg.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both models still match their original states\n"
     ]
    }
   ],
   "source": [
    "if comparator3.models_match(model3_1, model3_2, model3_3):\n",
    "    print(\"Both models still match their original states\")\n",
    "else:\n",
    "    print(\"At least one model has changed from its original state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 6.6106, Val Loss: 5.2115\n",
      "Epoch 2, Train Loss: 5.7838, Val Loss: 4.6599\n",
      "Epoch 3, Train Loss: 5.2669, Val Loss: 4.1876\n",
      "Epoch 4, Train Loss: 4.8897, Val Loss: 3.8027\n",
      "Epoch 5, Train Loss: 4.5822, Val Loss: 3.5712\n",
      "Epoch 6, Train Loss: 4.3410, Val Loss: 3.3445\n",
      "Epoch 7, Train Loss: 4.1143, Val Loss: 3.1478\n",
      "Epoch 8, Train Loss: 3.9186, Val Loss: 2.9573\n",
      "Epoch 9, Train Loss: 3.7466, Val Loss: 2.8659\n",
      "Epoch 10, Train Loss: 3.5945, Val Loss: 2.6951\n",
      "Epoch 11, Train Loss: 3.4358, Val Loss: 2.5621\n",
      "Epoch 12, Train Loss: 3.3132, Val Loss: 2.4488\n",
      "Epoch 13, Train Loss: 3.2156, Val Loss: 2.3539\n",
      "Epoch 14, Train Loss: 3.0886, Val Loss: 2.3027\n",
      "Epoch 15, Train Loss: 3.0049, Val Loss: 2.2284\n",
      "Epoch 16, Train Loss: 2.9025, Val Loss: 2.0873\n",
      "Epoch 17, Train Loss: 2.7800, Val Loss: 2.0056\n",
      "Epoch 18, Train Loss: 2.6924, Val Loss: 1.9949\n",
      "Epoch 19, Train Loss: 2.6161, Val Loss: 1.8959\n",
      "Epoch 20, Train Loss: 2.5271, Val Loss: 1.8095\n",
      "Epoch 21, Train Loss: 2.4524, Val Loss: 1.7736\n",
      "Epoch 22, Train Loss: 2.3837, Val Loss: 1.6716\n",
      "Epoch 23, Train Loss: 2.2987, Val Loss: 1.6477\n",
      "Epoch 24, Train Loss: 2.2262, Val Loss: 1.5935\n",
      "Epoch 25, Train Loss: 2.1651, Val Loss: 1.5797\n",
      "Epoch 26, Train Loss: 2.0988, Val Loss: 1.4685\n",
      "Epoch 27, Train Loss: 2.0257, Val Loss: 1.4229\n",
      "Epoch 28, Train Loss: 1.9714, Val Loss: 1.3941\n",
      "Epoch 29, Train Loss: 1.9188, Val Loss: 1.3796\n",
      "Epoch 30, Train Loss: 1.8520, Val Loss: 1.2783\n",
      "Epoch 31, Train Loss: 1.7863, Val Loss: 1.2225\n",
      "Epoch 32, Train Loss: 1.7489, Val Loss: 1.1854\n",
      "Epoch 33, Train Loss: 1.6772, Val Loss: 1.2184\n",
      "Epoch 34, Train Loss: 1.6423, Val Loss: 1.1544\n",
      "Epoch 35, Train Loss: 1.6038, Val Loss: 1.1618\n",
      "Epoch 36, Train Loss: 1.5549, Val Loss: 1.0259\n",
      "Epoch 37, Train Loss: 1.5112, Val Loss: 1.0694\n",
      "Epoch 38, Train Loss: 1.4707, Val Loss: 1.0456\n",
      "Epoch 39, Train Loss: 1.4366, Val Loss: 0.9571\n",
      "Epoch 40, Train Loss: 1.3891, Val Loss: 1.0193\n",
      "Epoch 41, Train Loss: 1.3488, Val Loss: 0.9213\n",
      "Epoch 42, Train Loss: 1.3162, Val Loss: 0.8906\n",
      "Epoch 43, Train Loss: 1.2726, Val Loss: 0.8701\n",
      "Epoch 44, Train Loss: 1.2330, Val Loss: 0.8443\n",
      "Epoch 45, Train Loss: 1.2073, Val Loss: 0.8519\n",
      "Epoch 46, Train Loss: 1.1693, Val Loss: 0.8431\n",
      "Epoch 47, Train Loss: 1.1357, Val Loss: 0.7482\n",
      "Epoch 48, Train Loss: 1.1201, Val Loss: 0.7280\n",
      "Epoch 49, Train Loss: 1.0680, Val Loss: 0.7052\n",
      "Epoch 50, Train Loss: 1.0464, Val Loss: 0.6998\n",
      "Epoch 51, Train Loss: 1.0016, Val Loss: 0.7200\n",
      "Epoch 52, Train Loss: 0.9970, Val Loss: 0.6616\n",
      "Epoch 53, Train Loss: 0.9820, Val Loss: 0.6520\n",
      "Epoch 54, Train Loss: 0.9440, Val Loss: 0.6342\n",
      "Epoch 55, Train Loss: 0.9199, Val Loss: 0.6627\n",
      "Epoch 56, Train Loss: 0.9025, Val Loss: 0.6250\n",
      "Epoch 57, Train Loss: 0.8743, Val Loss: 0.6016\n",
      "Epoch 58, Train Loss: 0.8742, Val Loss: 0.5352\n",
      "Epoch 59, Train Loss: 0.8193, Val Loss: 0.5691\n",
      "Epoch 60, Train Loss: 0.7959, Val Loss: 0.5504\n",
      "Epoch 61, Train Loss: 0.7773, Val Loss: 0.5333\n",
      "Epoch 62, Train Loss: 0.7604, Val Loss: 0.4978\n",
      "Epoch 63, Train Loss: 0.7448, Val Loss: 0.5028\n",
      "Epoch 64, Train Loss: 0.7451, Val Loss: 0.4964\n",
      "Epoch 65, Train Loss: 0.7335, Val Loss: 0.4786\n",
      "Epoch 66, Train Loss: 0.6929, Val Loss: 0.4726\n",
      "Epoch 67, Train Loss: 0.6844, Val Loss: 0.4410\n",
      "Epoch 68, Train Loss: 0.6648, Val Loss: 0.4321\n",
      "Epoch 69, Train Loss: 0.6472, Val Loss: 0.4380\n",
      "Epoch 70, Train Loss: 0.6315, Val Loss: 0.4483\n"
     ]
    }
   ],
   "source": [
    "triple_interweaved_avg = train(Triple_InterWeaved_Model_avg, optimizer_triple_interweaved_avg, 70, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triple_InterWeaved_Model_same = InterWeavedModel([model3_1, model3_2, model3_3], num_models = 3, same=True)\n",
    "optimizer_triple_interweaved_same = torch.optim.Adam(Triple_InterWeaved_Model_same.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All three models still match their original states\n"
     ]
    }
   ],
   "source": [
    "if comparator3.models_match(model3_1, model3_2, model3_3):\n",
    "    print(\"All three models still match their original states\")\n",
    "else:\n",
    "    print(\"At least one model has changed from its original state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 4.3374, Val Loss: 2.8476\n",
      "Epoch 2, Train Loss: 3.5060, Val Loss: 2.5123\n",
      "Epoch 3, Train Loss: 3.2191, Val Loss: 2.3227\n",
      "Epoch 4, Train Loss: 3.0209, Val Loss: 2.1519\n",
      "Epoch 5, Train Loss: 2.8675, Val Loss: 2.0235\n",
      "Epoch 6, Train Loss: 2.6969, Val Loss: 1.9272\n",
      "Epoch 7, Train Loss: 2.5654, Val Loss: 1.8593\n",
      "Epoch 8, Train Loss: 2.4697, Val Loss: 1.7754\n",
      "Epoch 9, Train Loss: 2.3626, Val Loss: 1.6691\n",
      "Epoch 10, Train Loss: 2.2886, Val Loss: 1.6263\n",
      "Epoch 11, Train Loss: 2.2066, Val Loss: 1.5426\n",
      "Epoch 12, Train Loss: 2.1063, Val Loss: 1.5193\n",
      "Epoch 13, Train Loss: 2.0569, Val Loss: 1.4589\n",
      "Epoch 14, Train Loss: 1.9945, Val Loss: 1.3985\n",
      "Epoch 15, Train Loss: 1.9155, Val Loss: 1.3595\n",
      "Epoch 16, Train Loss: 1.8827, Val Loss: 1.3103\n",
      "Epoch 17, Train Loss: 1.7927, Val Loss: 1.2331\n",
      "Epoch 18, Train Loss: 1.7390, Val Loss: 1.2738\n",
      "Epoch 19, Train Loss: 1.7234, Val Loss: 1.2421\n",
      "Epoch 20, Train Loss: 1.6538, Val Loss: 1.1903\n",
      "Epoch 21, Train Loss: 1.5995, Val Loss: 1.1588\n",
      "Epoch 22, Train Loss: 1.5810, Val Loss: 1.0708\n",
      "Epoch 23, Train Loss: 1.5196, Val Loss: 1.0874\n",
      "Epoch 24, Train Loss: 1.4607, Val Loss: 1.0467\n",
      "Epoch 25, Train Loss: 1.4416, Val Loss: 0.9993\n",
      "Epoch 26, Train Loss: 1.3812, Val Loss: 0.9661\n",
      "Epoch 27, Train Loss: 1.3486, Val Loss: 0.9564\n",
      "Epoch 28, Train Loss: 1.3196, Val Loss: 0.8938\n",
      "Epoch 29, Train Loss: 1.2828, Val Loss: 0.9341\n",
      "Epoch 30, Train Loss: 1.2350, Val Loss: 0.8575\n",
      "Epoch 31, Train Loss: 1.1990, Val Loss: 0.8567\n",
      "Epoch 32, Train Loss: 1.1682, Val Loss: 0.8177\n",
      "Epoch 33, Train Loss: 1.1346, Val Loss: 0.8307\n",
      "Epoch 34, Train Loss: 1.1126, Val Loss: 0.7800\n",
      "Epoch 35, Train Loss: 1.0670, Val Loss: 0.7625\n",
      "Epoch 36, Train Loss: 1.0579, Val Loss: 0.7329\n",
      "Epoch 37, Train Loss: 1.0348, Val Loss: 0.7358\n",
      "Epoch 38, Train Loss: 1.0107, Val Loss: 0.7188\n",
      "Epoch 39, Train Loss: 0.9985, Val Loss: 0.6840\n",
      "Epoch 40, Train Loss: 0.9494, Val Loss: 0.6701\n",
      "Epoch 41, Train Loss: 0.9148, Val Loss: 0.6486\n",
      "Epoch 42, Train Loss: 0.9102, Val Loss: 0.6659\n",
      "Epoch 43, Train Loss: 0.8613, Val Loss: 0.6219\n",
      "Epoch 44, Train Loss: 0.8536, Val Loss: 0.6062\n",
      "Epoch 45, Train Loss: 0.8422, Val Loss: 0.5869\n",
      "Epoch 46, Train Loss: 0.7823, Val Loss: 0.5478\n",
      "Epoch 47, Train Loss: 0.7883, Val Loss: 0.5824\n",
      "Epoch 48, Train Loss: 0.7795, Val Loss: 0.5793\n",
      "Epoch 49, Train Loss: 0.7513, Val Loss: 0.5222\n",
      "Epoch 50, Train Loss: 0.7483, Val Loss: 0.5260\n",
      "Epoch 51, Train Loss: 0.7177, Val Loss: 0.5002\n",
      "Epoch 52, Train Loss: 0.7067, Val Loss: 0.5152\n",
      "Epoch 53, Train Loss: 0.6940, Val Loss: 0.4900\n",
      "Epoch 54, Train Loss: 0.6566, Val Loss: 0.4602\n",
      "Epoch 55, Train Loss: 0.6512, Val Loss: 0.4666\n",
      "Epoch 56, Train Loss: 0.6456, Val Loss: 0.4460\n",
      "Epoch 57, Train Loss: 0.6294, Val Loss: 0.4391\n",
      "Epoch 58, Train Loss: 0.6049, Val Loss: 0.4303\n",
      "Epoch 59, Train Loss: 0.5869, Val Loss: 0.4623\n",
      "Epoch 60, Train Loss: 0.5725, Val Loss: 0.4412\n",
      "Epoch 61, Train Loss: 0.5736, Val Loss: 0.4249\n",
      "Epoch 62, Train Loss: 0.5471, Val Loss: 0.3997\n",
      "Epoch 63, Train Loss: 0.5275, Val Loss: 0.3936\n",
      "Epoch 64, Train Loss: 0.5227, Val Loss: 0.4091\n",
      "Epoch 65, Train Loss: 0.5244, Val Loss: 0.4002\n",
      "Epoch 66, Train Loss: 0.5027, Val Loss: 0.3652\n",
      "Epoch 67, Train Loss: 0.5071, Val Loss: 0.3582\n",
      "Epoch 68, Train Loss: 0.4908, Val Loss: 0.3447\n",
      "Epoch 69, Train Loss: 0.4789, Val Loss: 0.3546\n",
      "Epoch 70, Train Loss: 0.4423, Val Loss: 0.3322\n"
     ]
    }
   ],
   "source": [
    "triple_interweaved_same_curve = train(Triple_InterWeaved_Model_same, optimizer_triple_interweaved_same, 70, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triple_single_stack = StackedModel([model3_1, model3_1, model3_1], num_models = 3)\n",
    "optimizer_triple_single_stack = torch.optim.Adam(Triple_single_stack.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 3.9657, Val Loss: 2.6437\n",
      "Epoch 2, Train Loss: 3.2525, Val Loss: 2.4017\n",
      "Epoch 3, Train Loss: 3.0183, Val Loss: 2.2646\n",
      "Epoch 4, Train Loss: 2.8695, Val Loss: 2.0840\n",
      "Epoch 5, Train Loss: 2.7183, Val Loss: 1.9752\n",
      "Epoch 6, Train Loss: 2.5985, Val Loss: 1.9045\n",
      "Epoch 7, Train Loss: 2.5098, Val Loss: 1.7969\n",
      "Epoch 8, Train Loss: 2.3921, Val Loss: 1.7124\n",
      "Epoch 9, Train Loss: 2.3025, Val Loss: 1.6503\n",
      "Epoch 10, Train Loss: 2.2135, Val Loss: 1.5931\n",
      "Epoch 11, Train Loss: 2.1458, Val Loss: 1.5210\n",
      "Epoch 12, Train Loss: 2.0881, Val Loss: 1.4606\n",
      "Epoch 13, Train Loss: 2.0233, Val Loss: 1.3992\n",
      "Epoch 14, Train Loss: 1.9472, Val Loss: 1.4284\n",
      "Epoch 15, Train Loss: 1.8902, Val Loss: 1.3224\n",
      "Epoch 16, Train Loss: 1.8216, Val Loss: 1.2991\n",
      "Epoch 17, Train Loss: 1.7695, Val Loss: 1.2420\n",
      "Epoch 18, Train Loss: 1.7068, Val Loss: 1.2191\n",
      "Epoch 19, Train Loss: 1.6615, Val Loss: 1.1714\n",
      "Epoch 20, Train Loss: 1.6068, Val Loss: 1.1153\n",
      "Epoch 21, Train Loss: 1.5557, Val Loss: 1.1275\n",
      "Epoch 22, Train Loss: 1.5059, Val Loss: 1.0986\n",
      "Epoch 23, Train Loss: 1.4815, Val Loss: 1.0354\n",
      "Epoch 24, Train Loss: 1.4428, Val Loss: 1.0445\n",
      "Epoch 25, Train Loss: 1.4177, Val Loss: 0.9860\n",
      "Epoch 26, Train Loss: 1.3771, Val Loss: 0.9486\n",
      "Epoch 27, Train Loss: 1.3123, Val Loss: 0.9257\n",
      "Epoch 28, Train Loss: 1.2957, Val Loss: 0.9280\n",
      "Epoch 29, Train Loss: 1.2578, Val Loss: 0.8849\n",
      "Epoch 30, Train Loss: 1.2312, Val Loss: 0.8848\n",
      "Epoch 31, Train Loss: 1.1724, Val Loss: 0.8557\n",
      "Epoch 32, Train Loss: 1.1700, Val Loss: 0.8072\n",
      "Epoch 33, Train Loss: 1.1273, Val Loss: 0.7557\n",
      "Epoch 34, Train Loss: 1.0880, Val Loss: 0.8030\n",
      "Epoch 35, Train Loss: 1.0904, Val Loss: 0.7977\n",
      "Epoch 36, Train Loss: 1.0399, Val Loss: 0.7793\n",
      "Epoch 37, Train Loss: 1.0348, Val Loss: 0.7127\n",
      "Epoch 38, Train Loss: 0.9795, Val Loss: 0.6852\n",
      "Epoch 39, Train Loss: 0.9608, Val Loss: 0.6390\n",
      "Epoch 40, Train Loss: 0.9423, Val Loss: 0.6916\n",
      "Epoch 41, Train Loss: 0.9190, Val Loss: 0.6350\n",
      "Epoch 42, Train Loss: 0.8967, Val Loss: 0.6544\n",
      "Epoch 43, Train Loss: 0.8583, Val Loss: 0.6224\n",
      "Epoch 44, Train Loss: 0.8528, Val Loss: 0.5926\n",
      "Epoch 45, Train Loss: 0.8233, Val Loss: 0.5857\n",
      "Epoch 46, Train Loss: 0.8011, Val Loss: 0.5779\n",
      "Epoch 47, Train Loss: 0.7889, Val Loss: 0.5927\n",
      "Epoch 48, Train Loss: 0.7673, Val Loss: 0.5883\n",
      "Epoch 49, Train Loss: 0.7539, Val Loss: 0.5437\n",
      "Epoch 50, Train Loss: 0.7320, Val Loss: 0.5046\n",
      "Epoch 51, Train Loss: 0.7071, Val Loss: 0.5117\n",
      "Epoch 52, Train Loss: 0.6972, Val Loss: 0.5228\n",
      "Epoch 53, Train Loss: 0.6672, Val Loss: 0.4554\n",
      "Epoch 54, Train Loss: 0.6674, Val Loss: 0.4879\n",
      "Epoch 55, Train Loss: 0.6547, Val Loss: 0.4733\n",
      "Epoch 56, Train Loss: 0.6293, Val Loss: 0.4723\n",
      "Epoch 57, Train Loss: 0.6210, Val Loss: 0.4546\n",
      "Epoch 58, Train Loss: 0.5993, Val Loss: 0.4583\n",
      "Epoch 59, Train Loss: 0.5767, Val Loss: 0.4090\n",
      "Epoch 60, Train Loss: 0.5722, Val Loss: 0.4073\n",
      "Epoch 61, Train Loss: 0.5734, Val Loss: 0.4200\n",
      "Epoch 62, Train Loss: 0.5537, Val Loss: 0.3943\n",
      "Epoch 63, Train Loss: 0.5338, Val Loss: 0.4042\n",
      "Epoch 64, Train Loss: 0.5138, Val Loss: 0.3914\n",
      "Epoch 65, Train Loss: 0.5112, Val Loss: 0.4085\n",
      "Epoch 66, Train Loss: 0.5104, Val Loss: 0.3629\n",
      "Epoch 67, Train Loss: 0.4883, Val Loss: 0.3554\n",
      "Epoch 68, Train Loss: 0.4679, Val Loss: 0.3720\n",
      "Epoch 69, Train Loss: 0.4716, Val Loss: 0.3477\n",
      "Epoch 70, Train Loss: 0.4491, Val Loss: 0.3762\n"
     ]
    }
   ],
   "source": [
    "triple_single_stack_curve = train(Triple_single_stack, optimizer_triple_single_stack, 70, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triple_single_interweave = InterWeavedModel([model3_1, model3_1, model3_1], num_models = 3)\n",
    "optimizer_triple_single_interweave = torch.optim.Adam(Triple_single_interweave.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 4.1497, Val Loss: 2.9043\n",
      "Epoch 2, Train Loss: 3.4952, Val Loss: 2.6159\n",
      "Epoch 3, Train Loss: 3.2203, Val Loss: 2.3985\n",
      "Epoch 4, Train Loss: 3.0144, Val Loss: 2.2226\n",
      "Epoch 5, Train Loss: 2.8596, Val Loss: 2.1509\n",
      "Epoch 6, Train Loss: 2.7127, Val Loss: 2.0019\n",
      "Epoch 7, Train Loss: 2.6068, Val Loss: 1.9291\n",
      "Epoch 8, Train Loss: 2.4843, Val Loss: 1.8354\n",
      "Epoch 9, Train Loss: 2.3733, Val Loss: 1.7613\n",
      "Epoch 10, Train Loss: 2.3036, Val Loss: 1.7104\n",
      "Epoch 11, Train Loss: 2.2264, Val Loss: 1.5979\n",
      "Epoch 12, Train Loss: 2.1637, Val Loss: 1.5279\n",
      "Epoch 13, Train Loss: 2.0622, Val Loss: 1.4948\n",
      "Epoch 14, Train Loss: 1.9929, Val Loss: 1.4448\n",
      "Epoch 15, Train Loss: 1.9510, Val Loss: 1.4291\n",
      "Epoch 16, Train Loss: 1.8823, Val Loss: 1.3089\n",
      "Epoch 17, Train Loss: 1.8251, Val Loss: 1.2659\n",
      "Epoch 18, Train Loss: 1.7582, Val Loss: 1.2373\n",
      "Epoch 19, Train Loss: 1.7168, Val Loss: 1.1746\n",
      "Epoch 20, Train Loss: 1.6416, Val Loss: 1.1830\n",
      "Epoch 21, Train Loss: 1.5980, Val Loss: 1.1106\n",
      "Epoch 22, Train Loss: 1.5512, Val Loss: 1.0605\n",
      "Epoch 23, Train Loss: 1.5091, Val Loss: 1.0445\n",
      "Epoch 24, Train Loss: 1.4696, Val Loss: 1.0492\n",
      "Epoch 25, Train Loss: 1.4172, Val Loss: 1.0136\n",
      "Epoch 26, Train Loss: 1.3780, Val Loss: 0.9708\n",
      "Epoch 27, Train Loss: 1.3349, Val Loss: 0.9673\n",
      "Epoch 28, Train Loss: 1.3146, Val Loss: 0.9258\n",
      "Epoch 29, Train Loss: 1.2840, Val Loss: 0.8838\n",
      "Epoch 30, Train Loss: 1.2291, Val Loss: 0.8867\n",
      "Epoch 31, Train Loss: 1.1957, Val Loss: 0.8548\n",
      "Epoch 32, Train Loss: 1.1654, Val Loss: 0.8039\n",
      "Epoch 33, Train Loss: 1.1593, Val Loss: 0.8309\n",
      "Epoch 34, Train Loss: 1.1042, Val Loss: 0.7657\n",
      "Epoch 35, Train Loss: 1.0801, Val Loss: 0.7576\n",
      "Epoch 36, Train Loss: 1.0480, Val Loss: 0.7023\n",
      "Epoch 37, Train Loss: 1.0340, Val Loss: 0.7645\n",
      "Epoch 38, Train Loss: 1.0091, Val Loss: 0.7038\n",
      "Epoch 39, Train Loss: 0.9669, Val Loss: 0.7018\n",
      "Epoch 40, Train Loss: 0.9517, Val Loss: 0.6619\n",
      "Epoch 41, Train Loss: 0.9101, Val Loss: 0.6421\n",
      "Epoch 42, Train Loss: 0.8947, Val Loss: 0.5915\n",
      "Epoch 43, Train Loss: 0.8705, Val Loss: 0.6446\n",
      "Epoch 44, Train Loss: 0.8614, Val Loss: 0.6254\n",
      "Epoch 45, Train Loss: 0.8206, Val Loss: 0.6195\n",
      "Epoch 46, Train Loss: 0.8137, Val Loss: 0.5468\n",
      "Epoch 47, Train Loss: 0.7852, Val Loss: 0.5465\n",
      "Epoch 48, Train Loss: 0.7801, Val Loss: 0.5637\n",
      "Epoch 49, Train Loss: 0.7565, Val Loss: 0.5727\n",
      "Epoch 50, Train Loss: 0.7180, Val Loss: 0.5259\n",
      "Epoch 51, Train Loss: 0.7001, Val Loss: 0.4844\n",
      "Epoch 52, Train Loss: 0.6938, Val Loss: 0.4653\n",
      "Epoch 53, Train Loss: 0.6783, Val Loss: 0.5233\n",
      "Epoch 54, Train Loss: 0.6681, Val Loss: 0.4592\n",
      "Epoch 55, Train Loss: 0.6379, Val Loss: 0.4774\n",
      "Epoch 56, Train Loss: 0.6326, Val Loss: 0.4453\n",
      "Epoch 57, Train Loss: 0.6117, Val Loss: 0.4462\n",
      "Epoch 58, Train Loss: 0.5966, Val Loss: 0.4935\n",
      "Epoch 59, Train Loss: 0.5743, Val Loss: 0.4150\n",
      "Epoch 60, Train Loss: 0.5710, Val Loss: 0.4113\n",
      "Epoch 61, Train Loss: 0.5541, Val Loss: 0.4072\n",
      "Epoch 62, Train Loss: 0.5431, Val Loss: 0.3767\n",
      "Epoch 63, Train Loss: 0.5390, Val Loss: 0.4311\n",
      "Epoch 64, Train Loss: 0.5191, Val Loss: 0.4014\n",
      "Epoch 65, Train Loss: 0.5199, Val Loss: 0.4144\n",
      "Epoch 66, Train Loss: 0.4976, Val Loss: 0.3776\n",
      "Epoch 67, Train Loss: 0.4915, Val Loss: 0.3640\n",
      "Epoch 68, Train Loss: 0.4724, Val Loss: 0.3576\n",
      "Epoch 69, Train Loss: 0.4574, Val Loss: 0.3877\n",
      "Epoch 70, Train Loss: 0.4546, Val Loss: 0.3103\n"
     ]
    }
   ],
   "source": [
    "triple_single_interweave_curve = train(Triple_single_interweave, optimizer_triple_single_interweave, 70, train_dataloader, val_dataloader)\n",
    "torch.save(Triple_single_interweave.state_dict(), 'models/Triple_single_interweave.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigModel3(torch.nn.Module):\n",
    "    def __init__(self, big=False):\n",
    "        super(BigModel3, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, new_emb_size if big else emb_size)\n",
    "        self.pe = PositionalEncoding(d_model= new_emb_size if big else emb_size)\n",
    "        self.block1 = Block(big=big)\n",
    "        self.block2 = Block(big=big)\n",
    "        self.block3 = Block(big=big)\n",
    "        self.block4 = Block(big=big)\n",
    "        self.block5 = Block(big=big)\n",
    "        self.block6 = Block(big=big)\n",
    "        self.block7 = Block(big=big)\n",
    "        self.block8 = Block(big=big)\n",
    "        self.block9 = Block(big=big)\n",
    "        self.block10 = Block(big=big)\n",
    "        self.block11 = Block(big=big)\n",
    "        self.block12 = Block(big=big)\n",
    "        self.f_lin = torch.nn.Linear(new_emb_size if big else emb_size, vocab_size)\n",
    "        self.drop = torch.nn.Dropout(0.1)\n",
    "    def forward(self, inp):\n",
    "        e = self.embedding(inp)\n",
    "        e = self.pe(e)\n",
    "        m = self.block1(e)\n",
    "        m = self.block2(m)\n",
    "        m = self.block3(m)\n",
    "        m = self.block4(m)\n",
    "        m = self.block5(m)\n",
    "        m = self.block6(m)\n",
    "        m = self.block7(m)\n",
    "        m = self.block8(m)\n",
    "        m = self.block9(m)\n",
    "        m = self.block10(m)\n",
    "        m = self.block11(m)\n",
    "        m = self.block12(m)\n",
    "        r = self.f_lin(self.drop(m))\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scratch_Triple_Big_Model = Model(12).to(device)\n",
    "optimizer_scratch_triple_big = torch.optim.Adam(Scratch_Triple_Big_Model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 10.4064, Val Loss: 9.4877\n",
      "Epoch 2, Train Loss: 9.4502, Val Loss: 8.5155\n",
      "Epoch 3, Train Loss: 8.7658, Val Loss: 7.7916\n",
      "Epoch 4, Train Loss: 8.1542, Val Loss: 7.1874\n",
      "Epoch 5, Train Loss: 7.5930, Val Loss: 6.5563\n",
      "Epoch 6, Train Loss: 7.0691, Val Loss: 6.0666\n",
      "Epoch 7, Train Loss: 6.6272, Val Loss: 5.5814\n",
      "Epoch 8, Train Loss: 6.2291, Val Loss: 5.2536\n",
      "Epoch 9, Train Loss: 5.8361, Val Loss: 4.8816\n",
      "Epoch 10, Train Loss: 5.5200, Val Loss: 4.5601\n",
      "Epoch 11, Train Loss: 5.2320, Val Loss: 4.2855\n",
      "Epoch 12, Train Loss: 4.9931, Val Loss: 4.0645\n",
      "Epoch 13, Train Loss: 4.7750, Val Loss: 3.9382\n",
      "Epoch 14, Train Loss: 4.5500, Val Loss: 3.7063\n",
      "Epoch 15, Train Loss: 4.3790, Val Loss: 3.5285\n",
      "Epoch 16, Train Loss: 4.2122, Val Loss: 3.4177\n",
      "Epoch 17, Train Loss: 4.1093, Val Loss: 3.2118\n",
      "Epoch 18, Train Loss: 3.9330, Val Loss: 3.1325\n",
      "Epoch 19, Train Loss: 3.7925, Val Loss: 2.9629\n",
      "Epoch 20, Train Loss: 3.6634, Val Loss: 2.8554\n",
      "Epoch 21, Train Loss: 3.5863, Val Loss: 2.7533\n",
      "Epoch 22, Train Loss: 3.4195, Val Loss: 2.6692\n",
      "Epoch 23, Train Loss: 3.3486, Val Loss: 2.5642\n",
      "Epoch 24, Train Loss: 3.2291, Val Loss: 2.4372\n",
      "Epoch 25, Train Loss: 3.1220, Val Loss: 2.3830\n",
      "Epoch 26, Train Loss: 3.0319, Val Loss: 2.2961\n",
      "Epoch 27, Train Loss: 2.9568, Val Loss: 2.2084\n",
      "Epoch 28, Train Loss: 2.8749, Val Loss: 2.1029\n",
      "Epoch 29, Train Loss: 2.8006, Val Loss: 2.0178\n",
      "Epoch 30, Train Loss: 2.6787, Val Loss: 1.9265\n",
      "Epoch 31, Train Loss: 2.5966, Val Loss: 1.8842\n",
      "Epoch 32, Train Loss: 2.5381, Val Loss: 1.8849\n",
      "Epoch 33, Train Loss: 2.4663, Val Loss: 1.8117\n",
      "Epoch 34, Train Loss: 2.3917, Val Loss: 1.7730\n",
      "Epoch 35, Train Loss: 2.2759, Val Loss: 1.6700\n",
      "Epoch 36, Train Loss: 2.2239, Val Loss: 1.6023\n",
      "Epoch 37, Train Loss: 2.1763, Val Loss: 1.5967\n",
      "Epoch 38, Train Loss: 2.1222, Val Loss: 1.5009\n",
      "Epoch 39, Train Loss: 2.0742, Val Loss: 1.4591\n",
      "Epoch 40, Train Loss: 1.9986, Val Loss: 1.4261\n",
      "Epoch 41, Train Loss: 1.9516, Val Loss: 1.3788\n",
      "Epoch 42, Train Loss: 1.8959, Val Loss: 1.3639\n",
      "Epoch 43, Train Loss: 1.8267, Val Loss: 1.3478\n",
      "Epoch 44, Train Loss: 1.7729, Val Loss: 1.2292\n",
      "Epoch 45, Train Loss: 1.7409, Val Loss: 1.2210\n",
      "Epoch 46, Train Loss: 1.6687, Val Loss: 1.1897\n",
      "Epoch 47, Train Loss: 1.6541, Val Loss: 1.1669\n",
      "Epoch 48, Train Loss: 1.5927, Val Loss: 1.0863\n",
      "Epoch 49, Train Loss: 1.5577, Val Loss: 1.0691\n",
      "Epoch 50, Train Loss: 1.5398, Val Loss: 1.0268\n",
      "Epoch 51, Train Loss: 1.4824, Val Loss: 1.0746\n",
      "Epoch 52, Train Loss: 1.4511, Val Loss: 0.9942\n",
      "Epoch 53, Train Loss: 1.3842, Val Loss: 0.9571\n",
      "Epoch 54, Train Loss: 1.3287, Val Loss: 0.9544\n",
      "Epoch 55, Train Loss: 1.3252, Val Loss: 0.9123\n",
      "Epoch 56, Train Loss: 1.2931, Val Loss: 0.8954\n",
      "Epoch 57, Train Loss: 1.2306, Val Loss: 0.8973\n",
      "Epoch 58, Train Loss: 1.2369, Val Loss: 0.8492\n",
      "Epoch 59, Train Loss: 1.1868, Val Loss: 0.8336\n",
      "Epoch 60, Train Loss: 1.1630, Val Loss: 0.8354\n",
      "Epoch 61, Train Loss: 1.1253, Val Loss: 0.8377\n",
      "Epoch 62, Train Loss: 1.1067, Val Loss: 0.7818\n",
      "Epoch 63, Train Loss: 1.0791, Val Loss: 0.7396\n",
      "Epoch 64, Train Loss: 1.0701, Val Loss: 0.7298\n",
      "Epoch 65, Train Loss: 1.0174, Val Loss: 0.6737\n",
      "Epoch 66, Train Loss: 0.9884, Val Loss: 0.6918\n",
      "Epoch 67, Train Loss: 0.9846, Val Loss: 0.6394\n",
      "Epoch 68, Train Loss: 0.9587, Val Loss: 0.6422\n",
      "Epoch 69, Train Loss: 0.9172, Val Loss: 0.6325\n",
      "Epoch 70, Train Loss: 0.9105, Val Loss: 0.6529\n"
     ]
    }
   ],
   "source": [
    "scratch_triple_big_curve = train(Scratch_Triple_Big_Model, optimizer_scratch_triple_big, 70, train_dataloader, val_dataloader)\n",
    "torch.save(Scratch_Triple_Big_Model.state_dict(), 'models/Scratch_Triple_Big_Model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot triple model curves\n",
    "df3 = pd.DataFrame({'Big Model': scratch_triple_big_curve, 'Stacked Model': triple_stack_curve, 'Stacked Model (Average)': triple_stack_curve_avg, 'Stacked Model (Same)': triple_stack_curve_same, 'Interweaved Model': triple_interweaved_curve, 'Interweaved Model (Average)': triple_interweaved_avg, 'Interweaved Model (Same)': triple_interweaved_same_curve, 'Single Stacked Model': triple_single_stack_curve, 'Single Interweaved Model': triple_single_interweave_curve})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big Model Parameters:\n",
      "embedding.weight torch.Size([50258, 128])\n",
      "blocks.0.multihead.head1.k.weight torch.Size([128, 128])\n",
      "blocks.0.multihead.head1.q.weight torch.Size([128, 128])\n",
      "blocks.0.multihead.head1.v.weight torch.Size([128, 128])\n",
      "blocks.0.multihead.head2.k.weight torch.Size([128, 128])\n",
      "blocks.0.multihead.head2.q.weight torch.Size([128, 128])\n",
      "blocks.0.multihead.head2.v.weight torch.Size([128, 128])\n",
      "blocks.0.multihead.mh_lin.weight torch.Size([128, 256])\n",
      "blocks.0.l_norm_1.weight torch.Size([128])\n",
      "blocks.0.l_norm_1.bias torch.Size([128])\n",
      "blocks.0.l_norm_2.weight torch.Size([128])\n",
      "blocks.0.l_norm_2.bias torch.Size([128])\n",
      "blocks.0.ffn.weight torch.Size([128, 128])\n",
      "blocks.0.ffn.bias torch.Size([128])\n",
      "blocks.1.multihead.head1.k.weight torch.Size([128, 128])\n",
      "blocks.1.multihead.head1.q.weight torch.Size([128, 128])\n",
      "blocks.1.multihead.head1.v.weight torch.Size([128, 128])\n",
      "blocks.1.multihead.head2.k.weight torch.Size([128, 128])\n",
      "blocks.1.multihead.head2.q.weight torch.Size([128, 128])\n",
      "blocks.1.multihead.head2.v.weight torch.Size([128, 128])\n",
      "blocks.1.multihead.mh_lin.weight torch.Size([128, 256])\n",
      "blocks.1.l_norm_1.weight torch.Size([128])\n",
      "blocks.1.l_norm_1.bias torch.Size([128])\n",
      "blocks.1.l_norm_2.weight torch.Size([128])\n",
      "blocks.1.l_norm_2.bias torch.Size([128])\n",
      "blocks.1.ffn.weight torch.Size([128, 128])\n",
      "blocks.1.ffn.bias torch.Size([128])\n",
      "blocks.2.multihead.head1.k.weight torch.Size([128, 128])\n",
      "blocks.2.multihead.head1.q.weight torch.Size([128, 128])\n",
      "blocks.2.multihead.head1.v.weight torch.Size([128, 128])\n",
      "blocks.2.multihead.head2.k.weight torch.Size([128, 128])\n",
      "blocks.2.multihead.head2.q.weight torch.Size([128, 128])\n",
      "blocks.2.multihead.head2.v.weight torch.Size([128, 128])\n",
      "blocks.2.multihead.mh_lin.weight torch.Size([128, 256])\n",
      "blocks.2.l_norm_1.weight torch.Size([128])\n",
      "blocks.2.l_norm_1.bias torch.Size([128])\n",
      "blocks.2.l_norm_2.weight torch.Size([128])\n",
      "blocks.2.l_norm_2.bias torch.Size([128])\n",
      "blocks.2.ffn.weight torch.Size([128, 128])\n",
      "blocks.2.ffn.bias torch.Size([128])\n",
      "blocks.3.multihead.head1.k.weight torch.Size([128, 128])\n",
      "blocks.3.multihead.head1.q.weight torch.Size([128, 128])\n",
      "blocks.3.multihead.head1.v.weight torch.Size([128, 128])\n",
      "blocks.3.multihead.head2.k.weight torch.Size([128, 128])\n",
      "blocks.3.multihead.head2.q.weight torch.Size([128, 128])\n",
      "blocks.3.multihead.head2.v.weight torch.Size([128, 128])\n",
      "blocks.3.multihead.mh_lin.weight torch.Size([128, 256])\n",
      "blocks.3.l_norm_1.weight torch.Size([128])\n",
      "blocks.3.l_norm_1.bias torch.Size([128])\n",
      "blocks.3.l_norm_2.weight torch.Size([128])\n",
      "blocks.3.l_norm_2.bias torch.Size([128])\n",
      "blocks.3.ffn.weight torch.Size([128, 128])\n",
      "blocks.3.ffn.bias torch.Size([128])\n",
      "blocks.4.multihead.head1.k.weight torch.Size([128, 128])\n",
      "blocks.4.multihead.head1.q.weight torch.Size([128, 128])\n",
      "blocks.4.multihead.head1.v.weight torch.Size([128, 128])\n",
      "blocks.4.multihead.head2.k.weight torch.Size([128, 128])\n",
      "blocks.4.multihead.head2.q.weight torch.Size([128, 128])\n",
      "blocks.4.multihead.head2.v.weight torch.Size([128, 128])\n",
      "blocks.4.multihead.mh_lin.weight torch.Size([128, 256])\n",
      "blocks.4.l_norm_1.weight torch.Size([128])\n",
      "blocks.4.l_norm_1.bias torch.Size([128])\n",
      "blocks.4.l_norm_2.weight torch.Size([128])\n",
      "blocks.4.l_norm_2.bias torch.Size([128])\n",
      "blocks.4.ffn.weight torch.Size([128, 128])\n",
      "blocks.4.ffn.bias torch.Size([128])\n",
      "blocks.5.multihead.head1.k.weight torch.Size([128, 128])\n",
      "blocks.5.multihead.head1.q.weight torch.Size([128, 128])\n",
      "blocks.5.multihead.head1.v.weight torch.Size([128, 128])\n",
      "blocks.5.multihead.head2.k.weight torch.Size([128, 128])\n",
      "blocks.5.multihead.head2.q.weight torch.Size([128, 128])\n",
      "blocks.5.multihead.head2.v.weight torch.Size([128, 128])\n",
      "blocks.5.multihead.mh_lin.weight torch.Size([128, 256])\n",
      "blocks.5.l_norm_1.weight torch.Size([128])\n",
      "blocks.5.l_norm_1.bias torch.Size([128])\n",
      "blocks.5.l_norm_2.weight torch.Size([128])\n",
      "blocks.5.l_norm_2.bias torch.Size([128])\n",
      "blocks.5.ffn.weight torch.Size([128, 128])\n",
      "blocks.5.ffn.bias torch.Size([128])\n",
      "blocks.6.multihead.head1.k.weight torch.Size([128, 128])\n",
      "blocks.6.multihead.head1.q.weight torch.Size([128, 128])\n",
      "blocks.6.multihead.head1.v.weight torch.Size([128, 128])\n",
      "blocks.6.multihead.head2.k.weight torch.Size([128, 128])\n",
      "blocks.6.multihead.head2.q.weight torch.Size([128, 128])\n",
      "blocks.6.multihead.head2.v.weight torch.Size([128, 128])\n",
      "blocks.6.multihead.mh_lin.weight torch.Size([128, 256])\n",
      "blocks.6.l_norm_1.weight torch.Size([128])\n",
      "blocks.6.l_norm_1.bias torch.Size([128])\n",
      "blocks.6.l_norm_2.weight torch.Size([128])\n",
      "blocks.6.l_norm_2.bias torch.Size([128])\n",
      "blocks.6.ffn.weight torch.Size([128, 128])\n",
      "blocks.6.ffn.bias torch.Size([128])\n",
      "blocks.7.multihead.head1.k.weight torch.Size([128, 128])\n",
      "blocks.7.multihead.head1.q.weight torch.Size([128, 128])\n",
      "blocks.7.multihead.head1.v.weight torch.Size([128, 128])\n",
      "blocks.7.multihead.head2.k.weight torch.Size([128, 128])\n",
      "blocks.7.multihead.head2.q.weight torch.Size([128, 128])\n",
      "blocks.7.multihead.head2.v.weight torch.Size([128, 128])\n",
      "blocks.7.multihead.mh_lin.weight torch.Size([128, 256])\n",
      "blocks.7.l_norm_1.weight torch.Size([128])\n",
      "blocks.7.l_norm_1.bias torch.Size([128])\n",
      "blocks.7.l_norm_2.weight torch.Size([128])\n",
      "blocks.7.l_norm_2.bias torch.Size([128])\n",
      "blocks.7.ffn.weight torch.Size([128, 128])\n",
      "blocks.7.ffn.bias torch.Size([128])\n",
      "blocks.8.multihead.head1.k.weight torch.Size([128, 128])\n",
      "blocks.8.multihead.head1.q.weight torch.Size([128, 128])\n",
      "blocks.8.multihead.head1.v.weight torch.Size([128, 128])\n",
      "blocks.8.multihead.head2.k.weight torch.Size([128, 128])\n",
      "blocks.8.multihead.head2.q.weight torch.Size([128, 128])\n",
      "blocks.8.multihead.head2.v.weight torch.Size([128, 128])\n",
      "blocks.8.multihead.mh_lin.weight torch.Size([128, 256])\n",
      "blocks.8.l_norm_1.weight torch.Size([128])\n",
      "blocks.8.l_norm_1.bias torch.Size([128])\n",
      "blocks.8.l_norm_2.weight torch.Size([128])\n",
      "blocks.8.l_norm_2.bias torch.Size([128])\n",
      "blocks.8.ffn.weight torch.Size([128, 128])\n",
      "blocks.8.ffn.bias torch.Size([128])\n",
      "blocks.9.multihead.head1.k.weight torch.Size([128, 128])\n",
      "blocks.9.multihead.head1.q.weight torch.Size([128, 128])\n",
      "blocks.9.multihead.head1.v.weight torch.Size([128, 128])\n",
      "blocks.9.multihead.head2.k.weight torch.Size([128, 128])\n",
      "blocks.9.multihead.head2.q.weight torch.Size([128, 128])\n",
      "blocks.9.multihead.head2.v.weight torch.Size([128, 128])\n",
      "blocks.9.multihead.mh_lin.weight torch.Size([128, 256])\n",
      "blocks.9.l_norm_1.weight torch.Size([128])\n",
      "blocks.9.l_norm_1.bias torch.Size([128])\n",
      "blocks.9.l_norm_2.weight torch.Size([128])\n",
      "blocks.9.l_norm_2.bias torch.Size([128])\n",
      "blocks.9.ffn.weight torch.Size([128, 128])\n",
      "blocks.9.ffn.bias torch.Size([128])\n",
      "blocks.10.multihead.head1.k.weight torch.Size([128, 128])\n",
      "blocks.10.multihead.head1.q.weight torch.Size([128, 128])\n",
      "blocks.10.multihead.head1.v.weight torch.Size([128, 128])\n",
      "blocks.10.multihead.head2.k.weight torch.Size([128, 128])\n",
      "blocks.10.multihead.head2.q.weight torch.Size([128, 128])\n",
      "blocks.10.multihead.head2.v.weight torch.Size([128, 128])\n",
      "blocks.10.multihead.mh_lin.weight torch.Size([128, 256])\n",
      "blocks.10.l_norm_1.weight torch.Size([128])\n",
      "blocks.10.l_norm_1.bias torch.Size([128])\n",
      "blocks.10.l_norm_2.weight torch.Size([128])\n",
      "blocks.10.l_norm_2.bias torch.Size([128])\n",
      "blocks.10.ffn.weight torch.Size([128, 128])\n",
      "blocks.10.ffn.bias torch.Size([128])\n",
      "blocks.11.multihead.head1.k.weight torch.Size([128, 128])\n",
      "blocks.11.multihead.head1.q.weight torch.Size([128, 128])\n",
      "blocks.11.multihead.head1.v.weight torch.Size([128, 128])\n",
      "blocks.11.multihead.head2.k.weight torch.Size([128, 128])\n",
      "blocks.11.multihead.head2.q.weight torch.Size([128, 128])\n",
      "blocks.11.multihead.head2.v.weight torch.Size([128, 128])\n",
      "blocks.11.multihead.mh_lin.weight torch.Size([128, 256])\n",
      "blocks.11.l_norm_1.weight torch.Size([128])\n",
      "blocks.11.l_norm_1.bias torch.Size([128])\n",
      "blocks.11.l_norm_2.weight torch.Size([128])\n",
      "blocks.11.l_norm_2.bias torch.Size([128])\n",
      "blocks.11.ffn.weight torch.Size([128, 128])\n",
      "blocks.11.ffn.bias torch.Size([128])\n",
      "f_lin.weight torch.Size([50258, 128])\n",
      "f_lin.bias torch.Size([50258])\n"
     ]
    }
   ],
   "source": [
    "# Print Big Model parameters\n",
    "print(\"Big Model Parameters:\")\n",
    "for name, param in Scratch_Triple_Big_Model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAADhH0lEQVR4nOzdeVwVVf/A8c9lveyLG2AIKoKQiJLiloILgaLhUlmSS6iZT6ZWKvrkmmJWltqmZgpkmPnkxpOlmYkhLqgJLhAg4o47osgO5/cHP+fxCupF0St43q8Xr5czc+bMd+aC871nzpyjEkIIJEmSJEmSqoGergOQJEmSJKn2kImFJEmSJEnVRiYWkiRJkiRVG5lYSJIkSZJUbWRiIUmSJElStZGJhSRJkiRJ1UYmFpIkSZIkVRuZWEiSJEmSVG0MHvcBy8rKOHfuHBYWFqhUqsd9eEmSJEmSHoAQghs3buDg4ICe3t3bJR57YnHu3DkcHR0f92ElSZIkSaoGp0+f5plnnrnr9seeWFhYWADlgVlaWj7uw0uSJEmS9ACuX7+Oo6Ojch+/m8eeWNx6/GFpaSkTC0mSJEmqYe7XjUF23pQkSZIkqdrIxEKSJEmSpGojEwtJkiRJkqrNY+9jIUlSzSCEoKSkhNLSUl2HIknSY6Cvr4+BgcFDDwUhEwtJkiooKioiKyuLvLw8XYciSdJjZGpqir29PUZGRg9ch0wsJEnSUFZWRmZmJvr6+jg4OGBkZCQHs5OkWk4IQVFREZcuXSIzM5NmzZrdcxCse5GJhSRJGoqKiigrK8PR0RFTU1NdhyNJ0mNiYmKCoaEhJ0+epKioCLVa/UD1yM6bkiRV6kG/rUiSVHNVx9+9/J9DkiRJkqRqIxMLSZKeKidOnEClUpGYmKjrUO5r5syZtGrVSuvyNencpNpLJhaSJNUaw4YNQ6VSKT916tQhMDCQQ4cOKWUcHR3JysqiRYsWD3UsZ2dnVCoVq1evrrDt2WefRaVSERkZ+VDHkKSaSCYWkiTVKoGBgWRlZZGVlcW2bdswMDCgd+/eynZ9fX3s7OwwMHj4vuuOjo5ERERorNuzZw/nz5/HzMzsoeuXpJqoViQWJaVlfBN7jLE/HiS/SA7mI0lPM2NjY+zs7LCzs6NVq1ZMnjyZ06dPc+nSJaDyxwUxMTE0a9YMtVpN165diYqKQqVSce3atXseKyQkhB07dnD69Gll3YoVKwgJCamQuJw6dYrg4GDMzc2xtLTklVde4cKFCxpl5s2bR4MGDbCwsGD48OEUFBRUOOZ3332Hu7s7arWa5s2b880331TxCknSo1UrEgt9PRXfxWUSk3SOtAs3dB2OJNU6Qgjyikp08iOEeOC4c3Nz+eGHH3BxcaFOnTqVlsnMzOSll16ib9++JCUlMWrUKD744AOt6m/QoAEBAQFERUUBkJeXx08//URoaKhGubKyMoKDg7l69So7duxg69atHD9+nIEDBypl1qxZw8yZM5k7dy779+/H3t6+QtIQHR3N9OnTCQ8PJyUlhblz5zJt2jTl+JL0JKgV41ioVCrc7S2IP3aFlKzreDla6zokSapV8otL8Zi+RSfHTv4wAFMj7f+r+uWXXzA3Nwfg5s2b2Nvb88svv9z1NbqlS5fi5ubGp59+CoCbmxtHjhwhPDxcq+OFhoby/vvv88EHH/Dzzz/TtGnTCh0ut23bxuHDh8nMzMTR0RGA77//nmeffZZ9+/bRtm1bFi5cyPDhwxk+fDgAc+bM4Y8//tBotZgxYwafffYZ/fv3B6Bx48YkJyezdOlShg4dqvU1kqRHqVa0WAC421kCkJJ1XceRSJKkS127diUxMZHExEQSEhIICAigZ8+enDx5stLyqamptG3bVmOdj4+P1scLCgoiNzeXv/76ixUrVlRorQBISUnB0dFRSSoAPDw8sLa2JiUlRSnTrl07jf06dOig/PvmzZtkZGQwfPhwzM3NlZ85c+aQkZGhdbyS9KjVihYLAHf7W4mFfBQiSdXNxFCf5A8DdHbsqjAzM8PFxUVZ/u6777CysmLZsmXMmTOnusPDwMCAwYMHM2PGDPbu3cv69eur/RhQ/lgHYNmyZRUSEH39ql0jSXqUal9icf46Qgg5t4EkVSOVSlWlxxFPEpVKhZ6eHvn5+ZVud3Nz49dff9VYt2/fviodIzQ0lPnz5zNw4EBsbGwqbHd3d+f06dOcPn1aabVITk7m2rVreHh4KGX27t3LkCFDlP327Nmj/LtBgwY4ODhw/PhxQkJCqhSfJD1ONfN/ikq41DfHUF/FjYISzmTn42gr5ziQpKdRYWEh58+fByA7O5uvvvqK3Nxc+vTpU2n5UaNG8fnnnxMWFsbw4cNJTExUxp/Q9guKu7s7ly9fvuvcKj169MDT05OQkBAWLlxISUkJ//rXv/D19aVNmzYAjBs3jmHDhtGmTRs6depEdHQ0R48epUmTJko9s2bNYuzYsVhZWREYGEhhYSH79+8nOzub9957T9tLJEmPVK3pY2FkoEfTeuUdtmQ/C0l6em3evBl7e3vs7e1p164d+/bt4z//+Q9+fn6Vlm/cuDE///wz69ato2XLlixevFh5K8TY2Fjr49apUwcTE5NKt6lUKjZu3IiNjQ1dunShR48eNGnShJ9++kkpM3DgQKZNm8akSZN47rnnOHnyJKNHj9aoZ8SIEXz33XdERETg6emJr68vkZGRNG7cWOs4JelRU4mHeZfrAVy/fh0rKytycnKwtLSs1rrfW5PIur/P8m4PV8b1aFatdUvS06KgoIDMzEwaN278wLMb1nTh4eEsWbJEY3wKSXoa3OvvX9v7d61psQDwsJdvhkiSVHXffPMN+/bt4/jx46xcuZJPP/1Uvr4pSQ+o1vSxAM0OnJIkSdpKT09nzpw5XL16lUaNGvH+++8zZcoUXYclSTVSrUwsTl7JI7ewBHPjWnV6kiQ9IgsWLGDBggW6DkOSaoVa9SjE1syIBpblna1SZauFJEmSJD12tSqxgP+1WiTLgbIkSZIk6bGrtYmF7MApSZIkSY+fTCwkSZIkSao2tS6x8LC3ACD1/A3Kyh7rEB2SJEmS9NSrdYmFcx0zjA30yCsq5eTVPF2HI0mSJElPlVqXWBjo6+FmV95qIR+HSJL0OMTGxqJSqbh27Vq11XnixAlUKhWJiYnVVueDiIyMxNraukr7qFQqNmzY8EjikZ58tS6xAHC3k/0sJOlpdOnSJUaPHk2jRo0wNjbGzs6OgIAA4uPjlTK16abn5+eHSqVi3rx5FbYFBQWhUqmYOXPm4w9MeqrVzsTCXrZYSNLTaMCAARw8eJCoqCjS0tKIiYnBz8+PK1eu6Dq0R8bR0VGZjfWWs2fPsm3bNuzt7XUTlPRUq6WJxa0WCzmWhSQ9La5du0ZcXBwff/wxXbt2xcnJCR8fH6ZMmcKLL74IgLOzMwD9+vVDpVIpyxkZGQQHB9OgQQPMzc1p27Ytf/zxh0b9hYWFhIWF4ejoiLGxMS4uLixfvrzSWPLy8ujZsyedOnVSHo989913uLu7o1arad68Od98843GPgkJCbRu3Rq1Wk2bNm04ePCgVufdu3dvLl++rNEqExUVxQsvvED9+vU1ymZnZzNkyBBsbGwwNTWlZ8+epKena5SJjIykUaNGmJqa0q9fv0qTso0bN+Lt7Y1araZJkybMmjWLkpISreKVar9amVg0///E4uy1fHLyinUcjSTVAkJA0U3d/Gg5AbO5uTnm5uZs2LCBwsLCSsvs27cPgIiICLKyspTl3NxcevXqxbZt2zh48CCBgYH06dOHU6dOKfsOGTKEH3/8kS+++IKUlBSWLl2Kubl5hWNcu3YNf39/ysrK2Lp1K9bW1kRHRzN9+nTCw8NJSUlh7ty5TJs2jaioKOX4vXv3xsPDgwMHDjBz5kwmTJig1XkbGRkREhJCRESEsi4yMpLQ0NAKZYcNG8b+/fuJiYlh9+7dCCHo1asXxcXl/0/u3buX4cOHM2bMGBITE+natStz5szRqCMuLo4hQ4Ywbtw4kpOTWbp0KZGRkYSHh2sVr1T71crJNKxMDGlobcLZa/mknL9O+yZ1dB2SJNVsxXkw10E3x/73OTAyu28xAwMDIiMjGTlyJEuWLMHb2xtfX19effVVWrZsCUC9evUAsLa2xs7OTtnXy8sLLy8vZXn27NmsX7+emJgYxowZQ1paGmvWrGHr1q306NEDgCZNmlSI4fz58wwcOJBmzZqxatUqjIyMAJgxYwafffYZ/fv3B6Bx48bKTXno0KGsWrWKsrIyli9fjlqt5tlnn+XMmTOMHj1aq0sUGhpK586dWbRoEQcOHCAnJ4fevXtr9K9IT08nJiaG+Ph4OnbsCEB0dDSOjo5s2LCBl19+mUWLFhEYGMikSZMAcHV1ZdeuXWzevFmpZ9asWUyePFmZ/bVJkybMnj2bSZMmMWPGDK3ilWq3WtliAXKgLEl6Gg0YMIBz584RExNDYGAgsbGxeHt7V+iDcKfc3FwmTJiAu7s71tbWmJubk5KSorRYJCYmoq+vj6+v7z3r8ff3x8XFhZ9++klJKm7evElGRgbDhw9XWlXMzc2ZM2cOGRkZAKSkpNCyZUvUarVSV4cOHbQ+by8vL5o1a8bPP//MihUrGDx4MAYGmt8bU1JSMDAwoF27dsq6OnXq4ObmRkpKilLm9u2VxZGUlMSHH36ocS4jR44kKyuLvDz5ir9US1ssoHygrD9SLsjEQpKqg6FpecuBro5dBWq1Gn9/f/z9/Zk2bRojRoxgxowZDBs27K77TJgwga1btzJ//nxcXFwwMTHhpZdeoqioCAATExOtjh0UFMTatWtJTk7G09MTKE9aAJYtW1bhpq2vr1+lc7uX0NBQvv76a5KTk0lISKi2eu+Um5vLrFmzlNaX292eGElPr1qbWMgOnJJUjVQqrR5HPIk8PDw0Xi81NDSktLRUo0x8fDzDhg2jX79+QPnN88SJE8p2T09PysrK2LFjh/IopDLz5s3D3Nyc7t27Exsbi4eHBw0aNMDBwYHjx48TEhJS6X7u7u6sXLmSgoIC5ea8Z8+eKp3noEGDmDBhAl5eXnh4eFR6jJKSEvbu3as8Crly5QqpqalKeXd3d/bu3aux351xeHt7k5qaiouLS5Xik54etT6xSL1wg5LSMgz0a+1TH0mSKL9Jvvzyy4SGhtKyZUssLCzYv38/n3zyCcHBwUo5Z2dntm3bRqdOnTA2NsbGxoZmzZqxbt06+vTpg0qlYtq0aZSVlWnsM3ToUEJDQ/niiy/w8vLi5MmTXLx4kVdeeUUjjvnz51NaWkq3bt2IjY2lefPmzJo1i7Fjx2JlZUVgYCCFhYXs37+f7Oxs3nvvPQYNGsQHH3zAyJEjmTJlCidOnGD+/PlVOn8bGxuysrIwNDSsdHuzZs0IDg5m5MiRLF26FAsLCyZPnkzDhg2V6zN27Fg6derE/PnzCQ4OZsuWLRr9KwCmT59O7969adSoES+99BJ6enokJSVx5MiRCh09padTrb3bNrI1xcxIn6KSMjIv39R1OJIkPWLm5ua0a9eOBQsW0KVLF1q0aMG0adMYOXIkX331lVLus88+Y+vWrTg6OtK6dWsAPv/8c2xsbOjYsSN9+vQhICAAb29vjfoXL17MSy+9xL/+9S+aN2/OyJEjuXmz8v9bFixYwCuvvEK3bt1IS0tjxIgRfPfdd0RERODp6Ymvry+RkZE0btxYif2///0vhw8fpnXr1nzwwQd8/PHHVb4G1tbWmJndvWUpIiKC5557jt69e9OhQweEEPz6669KMtK+fXuWLVvGokWL8PLy4vfff2fq1KkadQQEBPDLL7/w+++/07ZtW9q3b8+CBQtwcnKqcrxS7aQSQst3uarJ9evXsbKyIicnB0tLy0d6rP7fxPP3qWsserUVwa0aPtJjSVJtUVBQQGZmJo0bN5bPzCXpKXOvv39t79+1tsUCZD8LSZIkSXrcnpLEQr4ZIkmSJEmPg0wsJEmSJEmqNrU6sWhuZ4FKBRdvFHIlt/IhfiVJkiRJqj61OrEwMzbAybZ8cB3Zz0KSJEmSHr1anVjA/x6H/HNePg6RJEmSpEftqUksks/JxEKSJEmSHrVan1h4NrQC4O9T2TqORJIkSZJqv1qfWDznbIOeCk5cyePC9QJdhyNJkiRJtVqtTyws1YY861DearHn+BUdRyNJUm0UGxuLSqXi2rVr1VbniRMnUKlUJCYmVludDyIyMhJra+sq7aNSqTQmfqvMlStXqF+/vsZkb7VdUVERzs7O7N+/X9ehPFK1PrEAaNfYFoC9mVd1HIkkSY/SpUuXGD16NI0aNcLY2Bg7OzsCAgKIj49Xymhz06sp/Pz8UKlUzJs3r8K2oKAgVCoVM2fOfPyBaSE8PJzg4GCcnZ0rbAsICEBfX599+/Y9/sAeISMjIyZMmEBYWJiuQ3mkno7EokkdAPbKFgtJqtUGDBjAwYMHiYqKIi0tjZiYGPz8/Lhypfb+7Ts6OhIZGamx7uzZs2zbtg17e3vdBHUfeXl5LF++nOHDh1fYdurUKXbt2sWYMWNYsWLFI4+lqKjokR/jdiEhIezcuZOjR48+1uM+Tk9FYuHjbItKBRmXbnLxhuxnIUm10bVr14iLi+Pjjz+ma9euODk54ePjw5QpU3jxxRcBlG/H/fr1Q6VSKcsZGRkEBwfToEEDzM3Nadu2LX/88YdG/YWFhYSFheHo6IixsTEuLi4sX7680ljy8vLo2bMnnTp1Uh6PfPfdd7i7u6NWq2nevDnffPONxj4JCQm0bt0atVpNmzZtOHjwoFbn3bt3by5fvqzRKhMVFcULL7xA/fr1NcpmZ2czZMgQbGxsMDU1pWfPnqSnp2uUiYyMpFGjRpiamtKvX79Kk7KNGzfi7e2NWq2mSZMmzJo1i5KSEq3iBfj1118xNjamffv2FbZFRETQu3dvRo8ezY8//kh+fj4AaWlpqFQq/vnnH43yCxYsoGnTpsrykSNH6NmzJ+bm5jRo0IDBgwdz+fJlZbufnx9jxoxh/Pjx1K1bl4CAAKB8hltPT0/MzMxwdHTkX//6F7m5uRrHWrZsGY6Ojsq1+fzzzys8JrrftbGxsaFTp06sXr1a6+tV0zwViYWVqSHN7cpfO02Qj0MkqcqEEOQV5+nkR9sJmM3NzTE3N2fDhg0UFlY+0u6tpvWIiAiysrKU5dzcXHr16sW2bds4ePAggYGB9OnTh1OnTin7DhkyhB9//JEvvviClJQUli5dirm5eYVjXLt2DX9/f8rKyti6dSvW1tZER0czffp0wsPDSUlJYe7cuUybNo2oqCjl+L1798bDw4MDBw4wc+ZMJkyYoNV5GxkZERISQkREhLIuMjKS0NDQCmWHDRvG/v37iYmJYffu3Qgh6NWrF8XFxQDs3buX4cOHM2bMGBITE+natStz5szRqCMuLo4hQ4Ywbtw4kpOTWbp0KZGRkYSHh2sV7606nnvuuQrrhRBERETw+uuv07x5c1xcXPj5558BcHV1pU2bNkRHR2vsEx0dzaBBg4Dya9+tWzdat27N/v372bx5MxcuXOCVV17R2CcqKgojIyPi4+NZsmQJAHp6enzxxRccPXqUqKgo/vzzTyZNmqTsEx8fz1tvvcW4ceNITEzE39+/wjlre218fHyIi4vT+nrVOOIxy8nJEYDIycl5rMedsfGIcAr7RUxdf/ixHleSapr8/HyRnJws8vPzlXU3i26KFpEtdPJzs+im1rH//PPPwsbGRqjVatGxY0cxZcoUkZSUpFEGEOvXr79vXc8++6z48ssvhRBCpKamCkBs3bq10rLbt28XgEhJSREtW7YUAwYMEIWFhcr2pk2bilWrVmnsM3v2bNGhQwchhBBLly4VderU0bjmixcvFoA4ePDgXWP09fUV48aNE4mJicLCwkLk5uaKHTt2iPr164vi4mLh5eUlZsyYIYQQIi0tTQAiPj5e2f/y5cvCxMRErFmzRgghxGuvvSZ69eqlcYyBAwcKKysrZbl79+5i7ty5GmVWrlwp7O3tleX7XePg4GARGhpaYf3vv/8u6tWrJ4qLi4UQQixYsED4+voq2xcsWCCaNm2qLN/6XFJSUoQQ5df0hRde0Kjz9OnTAhCpqanKNWvduvVdY7vlP//5j6hTp46yPHDgQBEUFKRRJiQkpMrXRgghFi1aJJydne8bgy5U9vd/i7b376eixQKgfZNbHThr77NWSXraDRgwgHPnzhETE0NgYCCxsbF4e3tX6INwp9zcXCZMmIC7uzvW1taYm5uTkpKitFgkJiair6+Pr6/vPevx9/fHxcWFn376CSMjIwBu3rxJRkYGw4cPV1pVzM3NmTNnDhkZGQCkpKTQsmVL1Gq1UleHDh20Pm8vLy+aNWvGzz//zIoVKxg8eDAGBgYaZVJSUjAwMKBdu3bKujp16uDm5kZKSopS5vbtlcWRlJTEhx9+qHEuI0eOJCsri7y8PK3izc/P1zjXW1asWMHAgQOV2F977TXi4+OV6/Tqq69y4sQJ9uzZA5S3Vnh7e9O8eXMltu3bt2vEdmvbrTqASltL/vjjD7p3707Dhg2xsLBg8ODBXLlyRTmn1NRUfHx8NPa5c1nba2NiYqL1taqJDO5fpHbwaVzegTPtQi5XbxZha2ak44gkqeYwMTBh76C9Ojt2VajVavz9/fH392fatGmMGDGCGTNmMGzYsLvuM2HCBLZu3cr8+fNxcXHBxMSEl156SenYZ2KiXQxBQUGsXbuW5ORkPD09AZTn9MuWLatw09bX16/Sud1LaGgoX3/9NcnJySQkJFRbvXfKzc1l1qxZ9O/fv8K2ypKFytStW5fsbM1BC69evcr69espLi5m8eLFyvrS0lJWrFhBeHg4dnZ2dOvWjVWrVtG+fXtWrVrF6NGjNWLr06cPH3/8cYVj3t6R1czMTGPbiRMnlH4d4eHh2NrasnPnToYPH05RURGmpqZanZe21+bq1avUq1dPqzproqcmsbA1M8K1gTlpF3JJyLxCYIsns7e0JD2JVCoVpoba/ef6pPHw8NB4vdTQ0JDS0lKNMvHx8QwbNox+/foB5TeI28dX8PT0pKysjB07dtCjR4+7HmvevHmYm5vTvXt3YmNj8fDwoEGDBjg4OHD8+HFCQkIq3c/d3Z2VK1dSUFCg3IBufSvX1qBBg5gwYQJeXl54eHhUeoySkhL27t1Lx44dgfKxJFJTU5Xy7u7u7N2rmUDeGYe3tzepqam4uLhUKb7btW7dmh9++EFjXXR0NM8880yFV4F///13PvvsMz788EP09fUJCQlh0qRJvPbaaxw/fpxXX31VI7a1a9fi7OxcocXmXg4cOEBZWRmfffYZenrlDflr1qzRKOPm5lbh9dc7l7W9NkeOHKF169Zax1fjPKrnNHejqz4WQggxbcNh4RT2i5ix8chjP7Yk1RT3esb6JLt8+bLo2rWrWLlypUhKShLHjx8Xa9asEQ0aNNB4nt+sWTMxevRokZWVJa5evSqEEKJfv36iVatW4uDBgyIxMVH06dNHWFhYiHHjxin7DRs2TDg6Oor169eL48ePi+3bt4uffvpJCPG/PhbZ2dlCCCHGjx8vGjRooDz7X7ZsmTAxMRGLFi0Sqamp4tChQ2LFihXis88+E0IIcePGDVG3bl3x+uuvi6NHj4pNmzYJFxcXrftY3JKdnS1yc3OV5dv7WAhR3rfBw8NDxMXFicTERBEYGChcXFxEUVGREEKI3bt3Cz09PfHpp5+KtLQ08eWXXwpra2uNfgSbN28WBgYGYubMmeLIkSMiOTlZ/Pjjj+KDDz5QynCfPhaHDh0SBgYGyvW/FWtYWFiFsteuXRNGRkbil19+EUIIcf36dWFiYiK8vLxE9+7dNcqePXtW1KtXT7z00ksiISFBHDt2TGzevFkMGzZMlJSUVHrNhBAiMTFRAGLhwoUiIyNDfP/996Jhw4Yan+nOnTuFnp6e+Oyzz0RaWppYsmSJqFOnjrC2tq7StRFCCCcnJ/H999/f9froUnX0sXiqEotfks4Jp7BfRODCvx77sSWppqipiUVBQYGYPHmy8Pb2FlZWVsLU1FS4ubmJqVOniry8PKVcTEyMcHFxEQYGBsLJyUkIIURmZqbo2rWrMDExEY6OjuKrr76qcAPKz88X7777rrC3txdGRkbCxcVFrFixQghRMbEQQoh33nlH2NvbK50Go6OjRatWrYSRkZGwsbERXbp0EevWrVPK7969W3h5eQkjIyPRqlUrsXbt2ionFne6M7G4evWqGDx4sLCyshImJiYiICBApKWlaeyzfPly8cwzzwgTExPRp08fMX/+fI3EQojyG2jHjh2FiYmJsLS0FD4+PuLbb79Vtt8vsRBCCB8fH7FkyRIhhBD79+8XgEhISKi0bM+ePUW/fv2U5VdeeUUAyvW/XVpamujXr5+wtrYWJiYmonnz5mL8+PGirKxMCHH3a/b5558Le3t75bp8//33FT7Tb7/9VjRs2FCYmJiIvn37ijlz5gg7O7sqXZtdu3YJa2trjd/JJ0l1JBYqIbR8l6uaXL9+HSsrK3JycrC0tHych+bSjULahv+BSgUHp/ljbSr7WUjSnQoKCsjMzKRx48ZaPzOXpKratGkTEydO5MiRI8rjh5pm5MiR/PPPP1V6dXTgwIF4eXnx73//+xFG9uDu9fev7f27Zn6aD6iehTFN65khhBzPQpIkSZeCgoJ48803OXv2rK5D0dr8+fNJSkri2LFjfPnll0RFRTF06FCt9y8qKsLT05N33333EUape1VKLEpLS5k2bRqNGzfGxMSEpk2bMnv2bK0HsHkSKMN7y8RCkiRJp8aPH4+jo6Ouw9BaQkIC/v7+eHp6smTJEr744gtGjBih9f5GRkZMnTpV67eMaqoqvRXy8ccfs3jxYqKionj22WfZv38/b7zxBlZWVowdO/ZRxVit2jW2ZdXeU3I8C0mSJKlK7nxTRKpclRKLXbt2ERwcTFBQEFA+7v6PP/74SN+Zrm7t/7/FIvncda4XFGOpNtRxRJIkSZJUe1TpUUjHjh3Ztm0baWlpQPkoYzt37qRnz5533aewsJDr169r/OhSA0s1znVMKROw/4R8HCJJkiRJ1alKicXkyZN59dVXad68OYaGhrRu3Zrx48ffddAXgI8++ggrKyvl50l4ntau8a1p1GViIUmSJEnVqUqJxZo1a4iOjmbVqlX8/fffREVFMX/+fGWGvspMmTKFnJwc5ef06dMPHfTDavf/84bskR04JUmSJKlaVamPxcSJE5VWCygf5vbkyZN89NFHd33lxtjYGGNj44ePtBrdejPkyNkccgtLMDd+akY2lyRJkqRHqkotFnl5eRUGMtHX16esrKxag3rUGlqb4GhrQmmZkP0sJEmSJKkaVSmx6NOnD+Hh4WzatIkTJ06wfv16Pv/8c2XinppE6WchH4dIkiRJUrWpUmLx5Zdf8tJLL/Gvf/0Ld3d3JkyYwKhRo5g9e/ajiu+Rade4vJ/F3uNyPAtJkh5ObGwsKpWKa9euVVudJ06cQKVSkZiYWG11PojIyEisra2rtI9KpaowS+mdrly5Qv369TVmkX2StW/fnrVr1+o6jBqhSomFhYUFCxcu5OTJk+Tn55ORkcGcOXMwMqp5c27cGs/i0Jkc8opKdByNJEnV4dKlS4wePZpGjRphbGyMnZ0dAQEBxMfHK2W0uenVFH5+fqhUKubNm1dhW1BQECqVipkzZz7+wLQQHh5OcHAwzs7Oyrr169fTvn17rKyssLCw4Nlnn2X8+PE6i/F2U6dOZfLkyTXu0b8uPFVzhdzuGRsTHKzUlJQJOW+IJNUSAwYM4ODBg0RFRZGWlkZMTAx+fn5cuVJ7WyYdHR2JjIzUWHf27Fm2bduGvb29boK6j7y8PJYvX87w4cOVddu2bWPgwIEMGDCAhIQEDhw4QHh4OMXFxTqM9H969uzJjRs3+O2333QdyhPvqU0sVCoVvm71AIhNvaTjaCTpySaEoCwvTyc/2s5FdO3aNeLi4vj444/p2rUrTk5O+Pj4MGXKFF588UUA5dtxv379UKlUynJGRgbBwcE0aNAAc3Nz2rZtyx9//KFRf2FhIWFhYTg6OmJsbIyLiwvLly+vNJa8vDx69uxJp06dlMcj3333He7u7qjVapo3b84333yjsU9CQgKtW7dGrVbTpk0bDh48qNV59+7dm8uXL2u0ykRFRfHCCy9Qv359jbLZ2dkMGTIEGxsbTE1N6dmzJ+np6RplIiMjadSoEaampvTr16/SpGzjxo14e3ujVqtp0qQJs2bNoqRE+5bfX3/9FWNjY9q3b6+s++9//0unTp2YOHEibm5uuLq60rdvX77++muljDafk7OzM3PmzGHIkCGYm5vj5ORETEwMly5dIjg4GHNzc1q2bMn+/fs19tu5cyedO3fGxMQER0dHxo4dy82bN5Xt+vr69OrVi9WrV2t9nk+rp/o9y65u9fkx4TTb/rnAjD4eqFQqXYckSU8kkZ9PqvdzOjm2298HUJma3recubk55ubmbNiwgfbt21f6mvu+ffuoX78+ERERBAYGoq+vD0Bubi69evUiPDwcY2Njvv/+e/r06UNqaiqNGjUCYMiQIezevZsvvvgCLy8vMjMzuXz5coVjXLt2jaCgIMzNzdm6dSumpqZER0czffp0vvrqK1q3bs3BgwcZOXIkZmZmDB06lNzcXHr37o2/vz8//PADmZmZjBs3TqvrY2RkREhICBEREXTq1AkoTw4++eSTCo9Bhg0bRnp6OjExMVhaWhIWFkavXr1ITk7G0NCQvXv3Mnz4cD766CP69u3L5s2bmTFjhkYdcXFxDBkyhC+++ILOnTuTkZHBm2++CVCh7N3ExcXx3HOav092dnasWrWKI0eO0KJFi0r30+ZzAliwYAFz585l2rRpLFiwgMGDB9OxY0dCQ0P59NNPCQsLY8iQIRw9ehSVSkVGRgaBgYHMmTOHFStWcOnSJcaMGcOYMWOIiIhQ6vXx8an0sZN0B/GY5eTkCEDk5OQ87kNXkFtQLJp98KtwCvtFpF+4rutwJOmJkJ+fL5KTk0V+fr6yrvTmTZHs1lwnP6U3b2od+88//yxsbGyEWq0WHTt2FFOmTBFJSUkaZQCxfv36+9b17LPPii+//FIIIURqaqoAxNatWystu337dgGIlJQU0bJlSzFgwABRWFiobG/atKlYtWqVxj6zZ88WHTp0EEIIsXTpUlGnTh2Na7548WIBiIMHD941Rl9fXzFu3DiRmJgoLCwsRG5urtixY4eoX7++KC4uFl5eXmLGjBlCCCHS0tIEIOLj45X9L1++LExMTMSaNWuEEEK89tprolevXhrHGDhwoLCyslKWu3fvLubOnatRZuXKlcLe3l5Zvt81Dg4OFqGhoRrrcnNzRa9evQQgnJycxMCBA8Xy5ctFQUHBXesRQvNzEkIIJycn8frrryvLWVlZAhDTpk1T1u3evVsAIisrSwghxPDhw8Wbb76pUW9cXJzQ09PT+Ew2btwo9PT0RGlp6T1jqskq+/u/Rdv791PdYmFmbECHJnXYkXaJbSkXcalvoeuQJOmJpDIxwe3vAzo7trYGDBhAUFAQcXFx7Nmzh99++41PPvmE7777jmHDht11v9zcXGbOnMmmTZvIysqipKSE/Px8Tp06BUBiYiL6+vr4+vre8/j+/v74+Pjw008/Ka0hN2/eJCMjg+HDhzNy5EilbElJCVZWVgCkpKTQsmVL1Gq1sr1Dhw5an7eXlxfNmjXj559/Zvv27QwePBgDA83/3lNSUjAwMKBdu3bKujp16uDm5kZKSopS5s7hAzp06MDmzZuV5aSkJOLj4wkPD1fWlZaWUlBQQF5eHqZatC7l5+drnCuAmZkZmzZtIiMjg+3bt7Nnzx7ef/99Fi1axO7duzE1Nb3v53RLy5YtlX83aNAAKB/Q8c51Fy9exM7OjqSkJA4dOkR0dLRSRghBWVkZmZmZuLu7A2BiYkJZWRmFhYW1furzh/FUJxYA3ZrXL08s/rnIKN+mug5Hkp5IKpVKq8cRTwK1Wo2/vz/+/v5MmzaNESNGMGPGjHsmFhMmTGDr1q3Mnz8fFxcXTExMeOmllygqKgLQ+iYSFBTE2rVrSU5OVm5kubm5ACxbtkzjpg4oyUd1CA0N5euvvyY5OfmRzjidm5vLrFmz6N+/f4VtdyYLd1O3bl2ys7Mr3da0aVOaNm3KiBEj+OCDD3B1deWnn37ijTfeuO/ndIuh4f9mrb71iLuydbfe8MjNzWXUqFGMHTu2Qjy3P2K5evUqZmZmMqm4D5lYNK/PjJijHDiZTU5eMVamchp1SapNPDw8NF4vNTQ0pLS0VKNMfHw8w4YNU76t5+bmaoyv4OnpSVlZGTt27KBHjx53Pda8efMwNzene/fuxMbG4uHhQYMGDXBwcOD48eN3nbDR3d2dlStXUlBQoNyc9+zZU6XzHDRoEBMmTMDLywsPD49Kj1FSUsLevXvp2LEjUD6WRGpqqlLe3d2dvXv3aux3Zxze3t6kpqbi4uJSpfhu17p1a3744Yf7lnN2dsbU1FTpRHm/z+lBeXt7k5ycfN9zOnLkCK1bt37o49V2T+1bIbc42prSrL45pWWCHeny7RBJqqmuXLlCt27d+OGHHzh06BCZmZn85z//4ZNPPiE4OFgp5+zszLZt2zh//rzyrblZs2asW7eOxMREkpKSGDRokMZ4Bc7OzgwdOpTQ0FA2bNhAZmYmsbGxrFmzpkIc8+fPJyQkhG7duvHPP/8AMGvWLD766CO++OIL0tLSOHz4MBEREXz++edAeVKgUqkYOXIkycnJ/Prrr8yfP79K529jY0NWVhbbtm2rdHuzZs0IDg5m5MiR7Ny5k6SkJF5//XUaNmyoXJ+xY8eyefNm5s+fT3p6Ol999ZXGYxCA6dOn8/333zNr1iyOHj1KSkoKq1evZurUqVrHGhAQwNGjRzVaLWbOnMmkSZOIjY0lMzOTgwcPEhoaSnFxMf7+/so53OtzelBhYWHs2rWLMWPGkJiYSHp6Ohs3bmTMmDEa5eLi4njhhRce+ni13iPq/3FXT1LnzVvm/posnMJ+EeN+/FvXoUiSzt2r89aTrKCgQEyePFl4e3sLKysrYWpqKtzc3MTUqVNFXl6eUi4mJka4uLgIAwMD4eTkJIQQIjMzU3Tt2lWYmJgIR0dH8dVXXykdI2/Jz88X7777rrC3txdGRkbCxcVFrFixQgjxv86b2dnZSvl33nlH2Nvbi9TUVCGEENHR0aJVq1bCyMhI2NjYiC5duoh169Yp5Xfv3i28vLyEkZGRaNWqlVi7dq3WnTfv5vbOm0IIcfXqVTF48GBhZWUlTExMREBAgEhLS9PYZ/ny5eKZZ54RJiYmok+fPmL+/PkanTeFEGLz5s2iY8eOwsTERFhaWgofHx/x7bffKtvRooOsj4+PWLJkibL8559/igEDBghHR0dhZGQkGjRoIAIDA0VcXJxSRpvPycnJSSxYsEDjWHfGk5mZWeHaJiQkCH9/f2Fubi7MzMxEy5YtRXh4uLL9zJkzwtDQUJw+ffqe51XTVUfnTZUQWr4kXk2uX7+OlZUVOTk5WFpaPs5D31VC5lVeWboba1NDDkz1R19PvnYqPb0KCgrIzMykcePGWj8zl6Sq2rRpExMnTuTIkSMVJrd8EoWFhZGdnc23336r61AeqXv9/Wt7/37q+1gAeDeyxsrEkGt5xRw8lU0bZ1tdhyRJklSrBQUFkZ6eztmzZ3F0dNR1OPdVv3593nvvPV2HUSM8+WniY2Cgr4eva/konNv+uajjaCRJkp4O48ePrxFJBcD777+vvKYq3ZtMLP5ft+blQ99ul4mFJEmSJD0wmVj8P1/Xeuip4J/zNziTnafrcCRJkiSpRpKJxf+zMTPiOScbQLZaSJIkSdKDkonFbbr+/+OQP2ViIUmSJEkPRCYWt+nevLxjTnzGFfKKtJ8CWJIkSZKkcjKxuI1rA3MaWptQVFLGrmNXdB2OJEmSJNU4MrG4jUqlUt4O+TNVPg6RJOnppVKpNOZY0YXY2FhUKhXXrl3Teh9nZ2cWLlz4yGKS7k8mFnfo5v6/104f86CkkiQ9pGHDhtG3b98q7fMk3EBromHDhqFSqXjrrbcqbHv77bdRqVT3nFFWqr1kYnGHDk3qoDbUIyungJSsG7oOR5KkGqK4uFjXITx2jo6OrF69mvz8fGVdQUEBq1at0phuXHq6yMTiDmpDfZ53qQvAn/9c0HE0kiQ9DD8/P8aOHcukSZOwtbXFzs6OmTNnKtudnZ0B6NevHyqVSlkG2LhxI97e3qjVapo0acKsWbMoKflfp26VSsXixYt58cUXMTMzIzw8nDZt2mjMStq3b18MDQ3Jzc0F4MyZM6hUKo4dOwZAYWEhEyZMoGHDhpiZmdGuXTtiY2OV/a9cucJrr71Gw4YNMTU1xdPTkx9//FHZ/u233+Lg4FBhhs/g4GBCQ0O1Ppf09HS6dOmCWq3Gw8ODrVu3anV9vb29cXR0ZN26dcq6devW0ahRowrTixcWFjJ27Fjq16+PWq3m+eefZ9++fRplfv31V1xdXTExMaFr166VTom+c+dOOnfujImJCY6OjowdO1aZVl16MsjEohLd/v/tEDm8tySVE0JQXFiqk5+HfSQZFRWFmZkZe/fu5ZNPPuHDDz9Ubpy3bmwRERFkZWUpy3FxcQwZMoRx48aRnJzM0qVLiYyMJDw8XKPumTNn0q9fPw4fPkxoaCi+vr5KYiCEIC4uDmtra3bu3AnAjh07aNiwIS4uLgCMGTOG3bt3s3r1ag4dOsTLL79MYGAg6enpQPm3/+eee45NmzZx5MgR3nzzTQYPHkxCQgIAL7/8MleuXGH79u1KTFevXmXz5s2EhIRodS5lZWX0798fIyMj9u7dy5IlSwgLC9P6+oaGhhIREaEsr1ixgjfeeKNCuUmTJrF27VqioqL4+++/cXFxISAggKtXrwJw+vRp+vfvT58+fUhMTGTEiBFMnjxZo46MjAwCAwMZMGAAhw4d4qeffmLnzp0VpjeXdEvOblqJc9fy6TjvT/RU8Pc0f6xNjXQdkiQ9NpXNblhcWMq343boJJ43F/liaKyvVdlhw4Zx7do1pc+En58fpaWlxMXFKWV8fHzo1q0b8+bNA8pbHtavX6/RN6NHjx50796dKVOmKOt++OEHJk2axLlz55T9xo8fz4IFC5Qy//3vfxk8eDBXrlzhyJEjBAYGMnDgQNRqNfPmzWPkyJHk5eURHR3NqVOnaNKkCadOncLBwUHj2D4+PsydO7fSc+zduzfNmzdXWkb69u1LnTp1WL58OVDeijFr1ixOnz6Nnp7efc/l999/JygoiJMnTypxbN68mZ49e1a4LpVd62XLluHo6EhqaioAzZs35/Tp04wYMQJra2siIyO5efMmNjY2REZGMmjQIKD80ZGzszPjx49n4sSJ/Pvf/2bjxo0cPXpUOcbkyZP5+OOPyc7OxtramhEjRqCvr8/SpUuVMjt37sTX15ebN2+iVquVOsePH19p3NK9ydlNHxEHaxNcG5iTdiGXuPTL9PFyuP9OkiQ9kVq2bKmxbG9vz8WL926NTEpKIj4+XqOForS0lIKCAvLy8jA1NQWgTZs2Gvt17tyZGzducPDgQXbt2oWvry9+fn5KErNjxw4mTpwIwOHDhyktLcXV1VWjjsLCQurUqaMcc+7cuaxZs4azZ89SVFREYWGhcnyAkJAQRo4cyTfffIOxsTHR0dG8+uqrylTk9zuXlJQUHB0dNZKbDh063PP63K5evXoEBQURGRmJEIKgoCDq1q2rUSYjI4Pi4mI6deqkrDM0NMTHx4eUlBQAUlJSaNeuncZ+d8aRlJTEoUOHiI6OVtYJISgrKyMzMxN3d3et45YeHZlY3IWvaz3SLuQSm3pJJhbSU8/ASI83F/nq7NgPw9DQUGNZpVJV6JNwp9zcXGbNmkX//v0rbLv9W5yZmZnGNmtra7y8vIiNjWX37t34+/vTpUsXBg4cSFpaGunp6fj6+irH0NfX58CBA+jra7bImJubA/Dpp5+yaNEiFi5ciKenJ2ZmZowfP56ioiKlbJ8+fRBCsGnTJtq2bUtcXJxGK4q25/IwQkNDlccRX3/9dbXUWZnc3FxGjRrF2LFjK2yTnUWfHDKxuAs/t/osi8tkR9olysoEenoqXYckSTqjUqm0fhxR0xgaGlJaWqqxztvbm9TUVKUvRFX4+vqyfft2EhISCA8Px9bWFnd3d8LDw7G3t1daKFq3bk1paSkXL16kc+fOldYVHx9PcHAwr7/+OlDeHyItLQ0PDw+ljFqtpn///kRHR3Ps2DHc3Nzw9vbW+lzc3d05ffo0WVlZ2NvbA7Bnz54qnXNgYCBFRUWoVCoCAgIqbG/atClGRkbEx8fj5OQElD8K2bdvn/LIwt3dnZiYGI397ozD29ub5OTkB/pcpMdHdt68izbONpga6XM5t5CU89d1HY4kSY+Is7Mz27Zt4/z582RnZwMwffp0vv/+e2bNmsXRo0dJSUlh9erVTJ069b71+fn5sWXLFgwMDGjevLmyLjo6WmmtAHB1dSUkJIQhQ4awbt06MjMzSUhI4KOPPmLTpk0ANGvWjK1bt7Jr1y5SUlIYNWoUFy5UfFstJCSETZs2sWLFCqXT5i33O5cePXrg6urK0KFDSUpKIi4ujg8++KBK11BfX5+UlBSSk5MrtL5AecvO6NGjmThxIps3byY5OVnpbzJ8+HAA3nrrLdLT05k4cSKpqamsWrWKyMhIjXrCwsLYtWsXY8aMITExkfT0dDZu3Cg7bz5hZGJxF8YG+nRsWv6cMzb1ko6jkSTpUfnss8/YunUrjo6OyiuSAQEB/PLLL/z++++0bduW9u3bs2DBAuXb9r107tyZsrIyjSTiVidSPz8/jbIREREMGTKE999/Hzc3N/r27cu+ffuUZv2pU6fi7e1NQEAAfn5+2NnZVdqZslu3btja2pKamqp0jrzlfueip6fH+vXryc/Px8fHhxEjRlR4+0UblpaW9+zQN2/ePAYMGMDgwYPx9vbm2LFjbNmyBRub8lmlGzVqxNq1a9mwYQNeXl4sWbKkQgfWli1bsmPHDtLS0ujcuTOtW7dm+vTpGv1DJN2Tb4Xcw8rdJ5i28Sg+jW1ZM0r7zkySVJPdq1e4JEm1W3W8FSJbLO7B17V8eO+/T2ZzveDpG1VPkiRJkqpKJhb30KiOKU3qmlFSJth17LKuw5EkSZKkJ55MLO6ji2s9AHakyX4WkiRJknQ/MrG4Dz+38sQiNvWSnO1UkiRJku5DJhb30b5JHYwNymc7Tb+Yq+twJEmSJOmJJhOL+1Ab6tOuSflrpzvka6eSJEmSdE8ysdCC3//3s4hNk7OdSpIkSdK9yMRCC77/389iX2Y2NwtLdByNJEmSJD25ZGKhhSZ1zXC0NaGotIw9x6/oOhxJkiRJemLJxEILKpUKX9f/vR0iSZJU26lUKjZs2KDTGGJjY1GpVFy7dk3rfZydnVm4cOE9yxQVFeHi4sKuXbseLsAapn379qxdu/aRH0cmFlq6NQpnbNpF+dqpJD2hhg0bVulcGvfyJNxAa6Jhw4ahUql46623Kmx7++23UalUDBs27PEHpoUlS5bQuHFjOnbsWGHbqFGj0NfX5z//+Y8OInu0pk6dyuTJkykrK3ukx5GJhZY6Nq2Dob6K01fzybx8U9fhSJL0hCkufvqG/Xd0dGT16tXk5+cr6woKCli1apUykdqTRgjBV199pcyqeru8vDxWr17NpEmTWLFixSOPpaio6JEf43Y9e/bkxo0b/Pbbb4/0ODKx0JKZsQFtnW0BOQqnJNUUfn5+jB07lkmTJmFra4udnR0zZ85Utjs7OwPQr18/VCqVsgywceNGvL29UavVNGnShFmzZlFS8r/O2yqVisWLF/Piiy9iZmZGeHg4bdq0Yf78+UqZvn37YmhoSG5u+Rg4Z86cQaVScezYMQAKCwuZMGECDRs2xMzMjHbt2hEbG6vsf+XKFV577TUaNmyIqakpnp6e/Pjjj8r2b7/9FgcHhwrfQIODgwkNDdX6XNLT0+nSpQtqtRoPDw+2bt2q1fX19vbG0dGRdevWKevWrVtHo0aNlJlibyksLGTs2LHUr18ftVrN888/z759+zTK/Prrr7i6umJiYkLXrl05ceJEhWPu3LmTzp07Y2JigqOjI2PHjuXmTe2/7B04cICMjAyCgoIqbPvPf/6Dh4cHkydP5q+//uL06dNA+eRbJiYmFW7I69evx8LCgry8PABOnz7NK6+8grW1Nba2tgQHB2ucw60WtfDwcBwcHHBzcwNg5cqVtGnTBgsLC+zs7Bg0aBAXL2q+hRgTE0OzZs1Qq9V07dqVqKioCo+J7ndt9PX16dWrF6tXr9b6ej0ImVhUwa1ROGViIT1thBAUFxTo5OdhHz1GRUVhZmbG3r17+eSTT/jwww+VG+etG1tERARZWVnKclxcHEOGDGHcuHEkJyezdOlSIiMjK0wnPnPmTPr168fhw4cJDQ3F19dXSQyEEMTFxWFtbc3OnTsB2LFjBw0bNsTFxQWAMWPGsHv3blavXs2hQ4d4+eWXCQwMJD09HSj/9v/cc8+xadMmjhw5wptvvsngwYNJSEgA4OWXX+bKlSts375dienq1ats3ryZkJAQrc6lrKyM/v37Y2RkxN69e1myZAlhYWFaX9/Q0FAiIiKU5RUrVvDGG29UKDdp0iTWrl1LVFQUf//9Ny4uLgQEBHD16lWg/Kbcv39/+vTpQ2JiIiNGjGDy5MkadWRkZBAYGMiAAQM4dOgQP/30Ezt37mTMmDFaxxsXF4erqysWFhYVti1fvpzXX38dKysrevbsSWRkJFA+JXzv3r1ZtWqVRvno6Gj69u2LqakpxcXFBAQEYGFhQVxcHPHx8ZibmxMYGKjRMrFt2zZSU1PZunUrv/zyC1De2jV79mySkpLYsGEDJ06c0HiMlJmZyUsvvUTfvn1JSkpi1KhRfPDBBw90bXx8fIiLi9P6ej0Q8Zjl5OQIQOTk5DzuQz+0f7KuC6ewX4TrB7+K/KISXYcjSY9Efn6+SE5OFvn5+cq6ovx8Mf+VIJ38FN0Wx/0MHTpUBAcHK8u+vr7i+eef1yjTtm1bERYWpiwDYv369RplunfvLubOnauxbuXKlcLe3l5jv/Hjx2uUiYmJEVZWVqKkpEQkJiYKOzs7MW7cOOV4I0aMEIMGDRJCCHHy5Emhr68vzp49W+HYU6ZMues5BgUFiffff19ZDg4OFqGhocry0qVLhYODgygtLdXqXLZs2SIMDAw04vjtt98qvS63u3WtL168KIyNjcWJEyfEiRMnhFqtFpcuXRLBwcFi6NChQgghcnNzhaGhoYiOjlb2LyoqEg4ODuKTTz4RQggxZcoU4eHhoXGMsLAwAYjs7GwhhBDDhw8Xb775pkaZuLg4oaenp/y+Ojk5iQULFtw17nHjxolu3bpVWJ+WliYMDQ3FpUuXhBBCrF+/XjRu3FiUlZUpy+bm5uLmzZtCiPJ7mVqtFr/99ptyTd3c3JTyQghRWFgoTExMxJYtW5Rr1qBBA1FYWHjX+IQQYt++fQIQN27cUK5DixYtNMp88MEHVb42QgixceNGoaenp/x+3Kmyv/9btL1/yxaLKnBtYI6dpZrCkjJ2ZcjZTiWpJmjZsqXGsr29fYVm5jslJSXx4YcfYm5urvyMHDmSrKwspdkboE2bNhr7de7cmRs3bnDw4EF27NiBr68vfn5+SivGjh078PPzA+Dw4cOUlpbi6uqqcZwdO3aQkZEBQGlpKbNnz8bT0xNbW1vMzc3ZsmULp06dUo4ZEhLC2rVrKSwsBMq/Rb/66qvo6elpdS4pKSk4Ojri4OCg1NmhQwetr2+9evUICgoiMjKSiIgIgoKCqFu3rkaZjIwMiouL6dSpk7LO0NAQHx8fUlJSAEhJSaFdu3Ya+90ZR1JSEpGRkRrnEhAQQFlZGZmZmVrFm5+fj1qtrrB+xYoVBAQEKLH36tWLnJwc/vzzT2XZ0NCQmJgYANauXYulpSU9evRQYjt27BgWFhZKbLa2thQUFCifJ4CnpydGRkYaxz5w4AB9+vShUaNGWFhY4OvrC6B8zqmpqbRt21ZjHx8fnwe6NiYmJpSVlSm/L4+CwSOruRZSqVQEPNuAqN0n2Zh4jm7NG+g6JEl6LAyMjRkb9bPOjv0wDA0NNZZVKtV9e8Xn5uYya9Ys+vfvX2Hb7TclMzMzjW3W1tZ4eXkRGxvL7t278ff3p0uXLgwcOJC0tDTS09OVm0Zubi76+vocOHAAfX19jXrMzc0B+PTTT1m0aBELFy7E09MTMzMzxo8fr9G03qdPH4QQbNq0ibZt2xIXF8eCBQuqfC4PIzQ0VGly//rrr6ulzsrk5uYyatQoxo4dW2Gbtp1F69aty+HDhzXWlZaWEhUVxfnz5zEwMNBYv2LFCrp3746RkREvvfQSq1at4tVXX2XVqlUMHDhQKZ+bm8tzzz1HdHR0hWPWq1dP+fedvzM3b94kICCAgIAAoqOjqVevHqdOnSIgIKBKnTu1vTZXr17FzMwMExMTreuuKplYVFE/72eI2n2SLUfPk1tYgrmxvIRS7adSqTCsppvQk8bQ0JDS0lKNdd7e3qSmpip9IarC19eX7du3k5CQQHh4OLa2tri7uxMeHo69vT2urq4AtG7dmtLSUi5evEjnzp0rrSs+Pp7g4GBef/11oLw/RFpaGh4eHkoZtVpN//79iY6O5tixY7i5ueHt7a31ubi7u3P69GmysrKwt7cHYM+ePVU651v9CFQqFQEBARW2N23aFCMjI+Lj43FycgLK+xXs27eP8ePHK3Hcag245c44vL29SU5OfqDP5ZbWrVuzePFihBCoVCqgvNPorZam25O8I0eO8MYbb3Dt2jWsra0JCQnB39+fo0eP8ueffzJnzhyN2H766Sfq16+PpaWl1vH8888/XLlyhXnz5uHo6AjA/v37Ncq4ubnx66+/aqy7s+OrttfmyJEjFTrWVjf5KKSKvJ6xokldMwqKy/jtcJauw5Ek6SE5Ozuzbds2zp8/T3Z2NgDTp0/n+++/Z9asWRw9epSUlBRWr17N1KlT71ufn58fW7ZswcDAgObNmyvroqOjldYKAFdXV0JCQhgyZAjr1q0jMzOThIQEPvroIzZt2gRAs2bN2Lp1K7t27SIlJYVRo0Zx4cKFCscMCQlh06ZNrFixQum0ecv9zqVHjx64uroydOhQkpKSiIuLq9Ax8H709fVJSUkhOTm5QusLlH9LHz16NBMnTmTz5s0kJyczcuRI8vLylNc+33rrLdLT05k4cSKpqamsWrVK6Tx5S1hYGLt27WLMmDEkJiaSnp7Oxo0bq9R5s2vXruTm5nL06FFl3fLlywkKCsLLy4sWLVooP7fe8LjVCtGlSxfs7OwICQmhcePGGo9uQkJCqFu3LsHBwcTFxZGZmUlsbCxjx47lzJkzd42nUaNGGBkZ8eWXX3L8+HFiYmKYPXu2RplRo0bxzz//EBYWRlpaGmvWrFGuza3kSNtrExcXxwsvvKD19XoQMrGoIpVKRX/vhgCsP3hWx9FIkvSwPvvsM7Zu3Yqjo6PyTS4gIIBffvmF33//nbZt29K+fXsWLFigfNu+l86dO1NWVqaRRPj5+VFaWqr0r7glIiKCIUOG8P777+Pm5kbfvn3Zt2+f0nQ9depUvL29CQgIwM/PDzs7u0oHAOvWrRu2trakpqYyaNAgjW33Oxc9PT3Wr19Pfn4+Pj4+jBgxosLbL9qwtLS85zf1efPmMWDAAAYPHoy3tzfHjh1jy5Yt2NjYAOU32LVr17Jhwwa8vLxYsmQJc+fO1aijZcuW7Nixg7S0NDp37kzr1q2ZPn26Rv+Q+6lTpw79+vVTkoULFy6wadMmBgwYUKGsnp4e/fr1Y/ny5UD5//+vvfYaSUlJFRI4U1NT/vrrLxo1akT//v1xd3dn+PDhFBQU3PO61KtXj8jISOVV13nz5mm8sgzQuHFjfv75Z9atW0fLli1ZvHixkvwZ//+jQm2uzdmzZ9m1a1elb+1UJ5UQj3cYyevXr2NlZUVOTk6VmoueJKev5tH5k+2oVBAf1g0H60f3rEqSHreCggIyMzNp3LhxtT2Dl6QnyaFDh/D39ycjI0Ppz1LThIeHs2TJEmWsDW2EhYWRnZ3Nt99+e9cy9/r71/b+LVssHoCjrSk+jW0RAjYkylYLSZKkmqRly5Z8/PHHWr9J8iT45ptv2LdvH8ePH2flypV8+umnDB06tEp11K9fv8JjlkdB9jx8QP1bNyQh8yrr/z7LaN+mynMuSZIk6cn3pM5jcjfp6enMmTOHq1ev0qhRI95//32mTJlSpTref//9RxSdJtli8YB6tbTHyECP9Iu5HD13XdfhSJIkSbXYggULOHfuHAUFBaSlpTFt2jSNV2OfJDKxeECWakP8PcrHsVj79917/EqSJEnS00QmFg+hf+vyt0P+m3SOktJHOw2tJEmSJNUEMrF4CF1c61HHzIjLuUXEpcshviVJkiRJJhYPwVBfjz5e5e8Iy8chkiRJkiQTi4d2a7CsrckXuF5QrONoJEmSJEm3ZGLxkDwbWuFS35zCkjI2Hz6v63AkSZIkSadkYvGQVCoV/f6/E6d8HCJJkiQ97WRiUQ36/n9isTfzKmey83QcjSRJ0sNTqVRs2LBBpzHExsaiUqm4du2a1vs4OzuzcOHCe5YpKirCxcWFXbt2PVyAj8mrr77KZ599puswtFblxOLs2bO8/vrr1KlTBxMTEzw9PStM8fq0aWhtQocmdQDYmHhOx9FI0tNr2LBhlU7SdS9Pwg20Jho2bBgqlYq33nqrwra3334blUr1xI5uuWTJEho3bkzHjh2VdTt27FAmczM1NaVZs2YMHTqUoqIiHUZaburUqYSHh5OTk6PrULRSpcQiOzubTp06YWhoyG+//UZycjKfffaZMjvd06yf9/8ehzzmed0kSXoCFBc/fZ23HR0dWb16Nfn5+cq6goICVq1apczQ+qQRQvDVV18p07UDJCcnExgYSJs2bfjrr784fPgwX375JUZGRpSWluow2nItWrSgadOm/PDDD7oORStVSiw+/vhjHB0diYiIwMfHh8aNG/PCCy/QtGnTRxVfjdGzhR1qQz2OX7pJ0pmakVVKkraEEJQVlerk52ESdT8/P8aOHcukSZOwtbXFzs6OmTNnKtudnZ0B6NevHyqVSlkG2LhxI97e3qjVapo0acKsWbMoKSlRtqtUKhYvXsyLL76ImZkZ4eHhtGnTRmPK6759+2JoaEhubi4AZ86cQaVScezYMQAKCwuZMGECDRs2xMzMjHbt2hEbG6vsf+XKFV577TUaNmyIqakpnp6e/Pjjj8r2b7/9FgcHB8rKNAfoCw4OJjQ0VOtzSU9Pp0uXLqjVajw8PNi6datW19fb2xtHR0fWrVunrFu3bh2NGjVSpqC/pbCwkLFjx1K/fn3UajXPP/88+/bt0yjz66+/4urqiomJCV27duXEiRMVjrlz5046d+6MiYkJjo6OjB07lps3b2oVL8CBAwfIyMggKChIWff7779jZ2fHJ598otzEAwMDWbZsGSYm5bNX3++zgPLft3feeYfx48djY2NDgwYNWLZsGTdv3uSNN97AwsICFxcXfvvtN439jhw5Qs+ePTE3N6dBgwYMHjyYy5c1x0bq06cPq1ev1vo8dalKA43HxMQQEBDAyy+/zI4dO2jYsCH/+te/GDly5F33KSwspLCwUFm+fr12zqthoTYk4Fk7NiaeY93fZ2jlaK3rkCSp2ojiMs5N183zaIcPO6Iy0n/g/aOionjvvffYu3cvu3fvZtiwYXTq1Al/f3/27dtH/fr1iYiIIDAwEH398uPExcUxZMgQvvjiCzp37kxGRgZvvvkmADNmzFDqnjlzJvPmzWPhwoUYGBhw48YNYmNjmTBhAkII4uLisLa2ZufOnQQGBir/b7q4uAAwZswYkpOTWb16NQ4ODqxfv57AwEAOHz5Ms2bNKCgo4LnnniMsLAxLS0s2bdrE4MGDadq0KT4+Prz88su88847bN++ne7duwNw9epVNm/ezK+//qrVuZSVldG/f38aNGjA3r17ycnJYfz48Vpf39DQUCIiIggJCQFgxYoVvPHGGxoJEsCkSZNYu3YtUVFRODk58cknnxAQEMCxY8ewtbXl9OnT9O/fn7fffps333yT/fv3V5g0KyMjg8DAQObMmcOKFSu4dOkSY8aMYcyYMURERGgVb1xcHK6urlhYWCjr7OzsyMrK4q+//qJLly6V7ne/z+KWqKgoJk2aREJCAj/99BOjR49m/fr19OvXj3//+98sWLCAwYMHc+rUKUxNTbl27RrdunVjxIgRLFiwgPz8fMLCwnjllVf4888/lXp9fHwIDw+nsLAQY2Njrc5VZ0QVGBsbC2NjYzFlyhTx999/i6VLlwq1Wi0iIyPvus+MGTMEUOEnJyenKoeuEWJTLwqnsF+E16wtorC4VNfhSNIDyc/PF8nJySI/P19ZV1pYIk6H/aWTn9LCEq1jHzp0qAgODlaWfX19xfPPP69Rpm3btiIsLExZBsT69es1ynTv3l3MnTtXY93KlSuFvb29xn7jx4/XKBMTEyOsrKxESUmJSExMFHZ2dmLcuHHK8UaMGCEGDRokhBDi5MmTQl9fX5w9e7bCsadMmXLXcwwKChLvv/++shwcHCxCQ0OV5aVLlwoHBwdRWlqq1bls2bJFGBgYaMTx22+/VXpdbnfrWl+8eFEYGxuLEydOiBMnTgi1Wi0uXbokgoODxdChQ4UQQuTm5gpDQ0MRHR2t7F9UVCQcHBzEJ598IoQQYsqUKcLDw0PjGGFhYQIQ2dnZQgghhg8fLt58802NMnFxcUJPT0/5fXVychILFiy4a9zjxo0T3bp101hXUlIihg0bJgBhZ2cn+vbtK7788sv73qfu/Czu/H0rKSkRZmZmYvDgwcq6rKwsAYjdu3cLIYSYPXu2eOGFFzTqPX36tABEamqqsi4pKUkA4sSJE/eM6WFV9vd/S05Ojlb37yq1WJSVldGmTRvmzp0LQOvWrTly5AhLliy567zwU6ZM4b333lOWr1+/jqOjY5WSn5rieZe61Lcw5uKNQranXiTgWTtdhyRJ1UJlqIfDhx3vX/ARHfthtGzZUmPZ3t6eixcv3nOfpKQk4uPjCQ8PV9aVlpZSUFBAXl4epqamALRp00Zjv86dO3Pjxg0OHjzIrl278PX1xc/Pj3nz5gHlHQQnTpwIwOHDhyktLcXV1VWjjsLCQurUqaMcc+7cuaxZs4azZ89SVFREYWGhcnyAkJAQRo4cyTfffIOxsTHR0dG8+uqr6OnpaXUuKSkpODo64uDgoGzv0KHDPa/P7erVq0dQUBCRkZEIIQgKCqJu3boaZTIyMiguLqZTp07KOkNDQ3x8fEhJSQEgJSWFdu3aaex3ZxxJSUkcOnSI6OhoZZ0QgrKyMjIzM3F3d79vvPn5+ajVao11+vr6REREMGfOHP7880/27t3L3Llz+fjjj0lISMDe3l6rzwI0f9/09fWpU6cOnp6eyroGDconr7z1O5iUlMT27dsxNzevEGtGRoby+3HrkUxe3pP/5mGVEgt7e3s8PDw01rm7u7N27dq77mNsbPzkN9tUE3298jEtlv51nHV/n5GJhVRrqFSqh3ocoUuGhoYayyqVqkKfhDvl5uYya9Ys+vfvX2Hb7TclMzMzjW3W1tZ4eXkRGxvL7t278ff3p0uXLgwcOJC0tDTS09Px9fVVjqGvr8+BAweURzC33LrJfPrppyxatIiFCxfi6emJmZkZ48eP13hToU+fPggh2LRpE23btiUuLo4FCxZU+VweRmhoKGPGjAHg66+/rpY6K5Obm8uoUaMYO3ZshW3adhatW7cuhw8frnRbw4YNGTx4MIMHD2b27Nm4urqyZMkSZs2apdVnAZX/vt2+TqVSASi/g7m5ufTp04ePP/64Qjz29vbKv69evQqUJ3JPuiolFp06dSI1NVVjXVpaGk5OTtUaVE3W3/sZlv51nD//uUj2zSJszIx0HZIkSfdgaGhYoee/t7c3qampSl+IqvD19WX79u0kJCQQHh6Ora0t7u7uhIeHY29vr3wDbd26NaWlpVy8eJHOnTtXWld8fDzBwcG8/vrrQPnNKC0tTeMLnlqtpn///kRHR3Ps2DHc3Nzw9vbW+lzc3d05ffo0WVlZyo1sz549VTrnwMBAioqKUKlUBAQEVNjetGlTjIyMiI+PV+4XxcXF7Nu3T+nP4e7uTkxMjMZ+d8bh7e1NcnLyA30ut7Ru3ZrFixcjhFBu8pWxsbHB3t5e6RiqzWfxILy9vVm7di3Ozs4YGNz9lnzkyBGeeeaZCq1BT6IqtTG+++677Nmzh7lz53Ls2DFWrVrFt99+y9tvv/2o4qtx3OwseNbBkuJSwX8PyTEtJOlJ5+zszLZt2zh//jzZ2dkATJ8+ne+//55Zs2Zx9OhRUlJSWL16NVOnTr1vfX5+fmzZsgUDAwOaN2+urIuOjlZaKwBcXV0JCQlhyJAhrFu3jszMTBISEvjoo4/YtGkTAM2aNWPr1q3s2rWLlJQURo0axYULFyocMyQkhE2bNrFixQqlE+Ut9zuXHj164OrqytChQ0lKSiIuLo4PPvigStdQX1+flJQUkpOTK7S+QHnLzujRo5k4cSKbN28mOTmZkSNHkpeXp7z2+dZbb5Gens7EiRNJTU1l1apVREZGatQTFhbGrl27GDNmDImJiaSnp7Nx40altUQbXbt2JTc3l6NHjyrrli5dyujRo/n999/JyMjg6NGjhIWFcfToUfr06QNo/1lU1dtvv83Vq1d57bXX2LdvHxkZGWzZsoU33nhDI+GNi4vjhRdeeOjjPQ5VSizatm3L+vXr+fHHH2nRogWzZ89m4cKFFX6Rn3b9vZ8BYO3fZ3UciSRJ9/PZZ5+xdetWHB0dlVckAwIC+OWXX/j9999p27Yt7du3Z8GCBVq1znbu3JmysjKNJMLPz4/S0lL8/Pw0ykZERDBkyBDef/993Nzc6Nu3L/v27VOa9adOnYq3tzcBAQH4+flhZ2dX6QBgtwZ2Sk1NZdCgQRrb7ncuenp6rF+/nvz8fHx8fBgxYoRGfwxtWVpaYmlpedft8+bNY8CAAQwePBhvb2+OHTvGli1blHGQGjVqxNq1a9mwYQNeXl4sWbJE6c93S8uWLdmxYwdpaWl07tyZ1q1bM336dI3+IfdTp04d+vXrp9FPw8fHh9zcXN566y2effZZfH192bNnDxs2bFA+R20/i6pycHAgPj6e0tJSXnjhBTw9PRk/fjzW1tZKP5mCggI2bNhwzzcwnyQqIR7vaE7Xr1/HysqKnJyce/4S1mSXbhTS/qNtlJYJ/njPF5f6FTvlSNKTqqCggMzMTBo3blxtz+Al6Uly6NAh/P39ycjIqLTT5JNm8eLFrF+/nt9///2RH+tef//a3r/lXCGPQD0LY/xcyzvYrD8oJyaTJEl6krRs2ZKPP/6YzMxMXYeiFUNDQ7788ktdh6E1mVg8Irceh6z/+yxlZXKIb0mSpCfJsGHDNF4DfZKNGDECNzc3XYehNZlYPCLd3etjoTbgXE4Be45f0XU4kiRJkvRYyMTiEVEb6tO7ZXmHItmJU5IkSXpayMTiERrw/zOe/nYki7yikvuUliRJkqSaTyYWj9BzTjY41TElr6iULUfP6zocSZIkSXrkZGLxCKlUKvq3Lu/EuU4+DpEkSZKeAjKxeMT6tS5/HLLz2GWycvJ1HI0kSZIkPVoysXjEGtUxxcfZFiEgIv6ErsORpKeeSqViw4YN1VrnzJkzadWqVbXW+bjExsaiUqm4du1atdV54sQJVCoViYmJ1Vbng4iMjMTa2rpK+zyK34+njUwsHoPRfk0BiNx1gjPZT/6Ut5JUU126dInRo0fTqFEjjI2NsbOzIyAggPj4eKVMVlYWPXv21GGUldMm9tp00/Pz80OlUilTyt8uKCgIlUrFzJkzH39g0kOTicVj4OdWj/ZNbCkqKePz39N0HY4k1VoDBgzg4MGDREVFkZaWRkxMDH5+fly58r+xZOzs7DA2NtZhlJXTJvbaxtHRscJEY2fPnmXbtm0aU4ZLNYtMLB4DlUrFlJ7uAKxPPEvyues6jkiSap9r164RFxfHxx9/TNeuXXFycsLHx4cpU6bw4osvKuVu/9Z/q8l+3bp1dO3aFVNTU7y8vNi9e7dG3cuWLcPR0RFTU1P69evH559/ft8m9u+++w53d3fUajXNmzfnm2++eajYnZ2dAejXrx8qlUpZzsjIIDg4mAYNGmBubk7btm35448/NOovLCwkLCwMR0dHjI2NcXFxYfny5ZXGkpeXR8+ePenUqZPyeOR+55KQkEDr1q1Rq9W0adOGgwcP3vPa3NK7d28uX76s0SoTFRXFCy+8QP369TXKZmdnM2TIEGxsbDA1NaVnz56kp6drlImMjKRRo0bK51RZUrZx40a8vb1Rq9U0adKEWbNmUVIihwOoTjKxeEy8HK3p3dIeIWDe5n90HY4kVYkQgqKiIp38aDtPorm5Oebm5mzYsIHCwsIqnd8HH3zAhAkTSExMxNXVlddee0252cTHx/PWW28xbtw4EhMT8ff3v+/sn9HR0UyfPp3w8HBSUlKYO3cu06ZNIyoq6oFj37dvH1A+I2pWVpaynJubS69evdi2bRsHDx4kMDCQPn36cOrUKWXfIUOG8OOPP/LFF1+QkpLC0qVLK51869q1a/j7+1NWVsbWrVuxtra+77nk5ubSu3dvPDw8OHDgADNnzmTChAn3ueLljIyMCAkJISIiQlkXGRlJaGhohbLDhg1j//79xMTEsHv3boQQ9OrVi+LiYgD27t3L8OHDlSnVu3btypw5czTqiIuLY8iQIYwbN47k5GSWLl1KZGTkA83mKt2dga4DqE7nb57HzsxO12Hc1cQAN7YcPc9faZfYmX6Z55vV1XVIkqSV4uLiClNYPy7//ve/MTIyum85AwMDIiMjGTlyJEuWLMHb2xtfX19effVVWrZsec99J0yYQFBQEACzZs3i2Wef5dixYzRv3pwvv/ySnj17KjdLV1dXdu3axS+//HLX+mbMmMFnn31G//79AWjcuLFyIxs6dOgDxV6vXvnEhtbW1tjZ/e//OS8vL7y8vJTl2bNns379emJiYhgzZgxpaWmsWbOGrVu30qNHDwCaNGlSIYbz588zcOBAmjVrxqpVq5Rrfr9zWbVqFWVlZSxfvhy1Ws2zzz7LmTNnGD169D2v+S2hoaF07tyZRYsWceDAAXJycujdu7dG/4r09HRiYmKIj4+nY8eOQHny5ujoyIYNG3j55ZdZtGgRgYGBTJo0Cfjf57R582alnlmzZjF58mTlM2jSpAmzZ89m0qRJzJgxQ6t4pfurFS0WecV59F7fmxd+foHsgmxdh3NXTnXMCGnnBMBHv6XIyckkqZoNGDCAc+fOERMTQ2BgILGxsXh7e1d4jn+n2xOPW8/2L168CEBqaio+Pj4a5e9cvt3NmzfJyMhg+PDhSkuEubk5c+bMISMjo9pjz83NZcKECbi7u2NtbY25uTkpKSlKi0ViYiL6+vr4+vresx5/f39cXFz46aeflKRCm3NJSUmhZcuWGlNsd+jQ4Z7Hup2XlxfNmjXj559/ZsWKFQwePBgDA83vvCkpKRgYGNCuXTtlXZ06dXBzcyMlJUUpc/v2yuJISkriww8/1DiXkSNHkpWVRV6e7FhfXWpFi4WpoSnG+sYIBDvP7qRP0z66Dumu3unmwtoDZzh67joxSefo+//jXEjSk8zQ0JB///vfOjt2VajVavz9/fH392fatGmMGDGCGTNmMGzYMK2OoVKpACgrK3ugeHNzc4Hyfhl33uj09fWrPfYJEyawdetW5s+fj4uLCyYmJrz00ksUFRUBYGJiolXcQUFBrF27luTkZGXWz4c5l6oIDQ3l66+/Jjk5mYSEhGqr9065ubnMmjVLaX253e2JkfRwakWLBYDvM74YlBrx15m/dB3KPdUxN+at/3/99NMtqRSWlOo4Ikm6P5VKhZGRkU5+bt3oH5SHhwc3b9584P3d3NyU/gy33Ll8uwYNGuDg4MDx48dxcXHR+GncuHGVjn1n7IaGhpSWav6fER8fz7Bhw+jXrx+enp7Y2dlx4sQJZbunpydlZWXs2LHjnseaN28eQ4cOpXv37iQnJ2t9Lu7u7hw6dIiCggKlrj179lTpPAcNGsThw4dp0aIFHh4eFba7u7tTUlLC3r17lXVXrlwhNTVVKe/u7q6xvbI4vL29SU1NrXAuLi4u6OnVmtuhztWKFouSolKsNnnxxrmPWGcwj5KyEgz0ntxTC+3UmO93n+DstXxW7j7JiM4Vn3dKklQ1V65c4eWXXyY0NJSWLVtiYWHB/v37+eSTTwgODn7get955x26dOnC559/Tp8+ffjzzz/57bff7pnwzJo1i7Fjx2JlZUVgYCCFhYXs37+f7Oxs3nvvvQeO3dnZmW3bttGpUyeMjY2xsbGhWbNmrFu3jj59+qBSqZg2bZpGa4uzszNDhw4lNDSUL774Ai8vL06ePMnFixd55ZVXNOKYP38+paWldOvWjdjYWJo3b37fcxk0aBAffPABI0eOZMqUKZw4cYL58+dX6Rrb2NiQlZV119apZs2aERwczMiRI1m6dCkWFhZMnjyZhg0bKtdn7NixdOrUifnz5xMcHMyWLVs0+lcATJ8+nd69e9OoUSNeeukl9PT0SEpK4siRIxU6ekoPQTxmOTk5AhA5OTnVWu8P03eLr0ZtE70/f03sy9pXrXU/CqsTTgqnsF+E16wt4lpeka7DkSRFfn6+SE5OFvn5+boOpUoKCgrE5MmThbe3t7CyshKmpqbCzc1NTJ06VeTl5SnlALF+/XohhBCZmZkCEAcPHlS2Z2dnC0Bs375dWfftt9+Khg0bChMTE9G3b18xZ84cYWdnp2yfMWOG8PLy0ognOjpatGrVShgZGQkbGxvRpUsXsW7duoeKPSYmRri4uAgDAwPh5OSknEPXrl2FiYmJcHR0FF999ZXw9fUV48aNU/bLz88X7777rrC3txdGRkbCxcVFrFixQgghxPbt2wUgsrOzlfLvvPOOsLe3F6mpqVqdy+7du4WXl5cwMjISrVq1EmvXrq1wXe90Z4x38vLyEjNmzFCWr169KgYPHiysrKyEiYmJCAgIEGlpaRr7LF++XDzzzDPCxMRE9OnTR8yfP19YWVlplNm8ebPo2LGjMDExEZaWlsLHx0d8++23yvbbfz+eRvf6+9f2/q0SQst3uarJ9evXsbKyIicnB0tLy2qr968fUzm84yyH7Xbg2tuK99pU/FbwJCkpLaPnojjSL+bylm9TJvdsruuQJAmAgoICMjMzady4sXzufBcjR47kn3/+IS4uTtehSFK1utffv7b371rzUMnBzZqy0hwa5rg+8f0sAAz09ZRkIiI+k8u5VXvvXpKkx2f+/PkkJSVx7NgxvvzyS6Kioip9bVSSpFqSWBTl57EjchpF11dgc9Oac5cucjb3yZ+mvFvz+ng9Y0VhSRnLd2bqOhxJku4iISEBf39/PD09WbJkCV988QUjRozQdViS9ESqFYmFkYkpRiYmgKC0+BgO111qRKuFSqXi7a4uAKzcfZKc/GIdRyRJUmXWrFnDxYsXyc/P5+jRo7z11lu6DkmSnli1IrGg8AbNrK8BUFaUTsMcV3acuferVU+KHu4NcGtgQW5hCd/vOqHrcCRJkiTpodSOxMLQjGZliQCUlZyiYbYT+7L2kVf85I+kpqen4l9dy8e1WBGfyc1CORmOJEmSVHPVjsRCT486bXpjY5QPlGGeex2jfDMSzj+6EdyqU5CnPU51TMnOK+bHhFP330GSJEmSnlC1I7EAaDEAN4tLAJQWp9Mwp1mNeRxioK/HaN/yVotv/zpOQbEcjVOSJEmqmWpPYuHQmmbPlL9zW1Z8gobZTYg7E6f1lMu61t/7Geyt1Fy8Ucjav8/oOhxJkiRJeiC1J7FQqajXrg9mBqVACQ0vm3Dh5gXSstN0HZlWjAz0eLNL+dDeS3ZkUFL6YBMgSZIkSZIu1Z7EAlB5vkRzywsAGOadxaqgXo15HALwattG1DEz4vTVfGKSzuk6HEmqlVQqFRs2bKjWOmfOnEmrVq2qtU7pwT2Kz7iqYmNjUalUXLt2Tet9nJ2dWbhw4SOL6XGpVYkF9d1xczIDoKz4OM9kN6sR41ncYmKkT+jz5TMGfhObQVlZzXiMI0lPikuXLjF69GgaNWqEsbExdnZ2BAQEEB8fr5TJysqiZ8+eOoyycsOGDaNv375V2udJuIHWRMOGDUOlUlU6Hsnbb7+NSqW651T10r3VrsQCsOvQB2N9PaCYJhdsOXTpENkF2boOS2uDOzhhoTbg2MVcthw9r+twJKlGGTBgAAcPHiQqKoq0tDRiYmLw8/PjypUrShk7OzuMjY11GOWTp7j46Rucz9HRkdWrV5Ofn6+sKygoYNWqVTRq1EiHkdV8tS6xULUYgJN5LgB1sosQAnae3anjqLRnqTZkWEdnAL6OPVZjOp9Kkq5du3aNuLg4Pv74Y7p27YqTkxM+Pj5MmTKFF198USl3+7f8EydOoFKpWLduHV27dsXU1BQvLy92796tUfeyZctwdHTE1NSUfv368fnnn2NtbX3PeL777jvc3d1Rq9U0b96cb775pkrn4+fnx9ixY5k0aRK2trbY2dkxc+ZMZbuzszMA/fr1Q6VSKcsAGzduxNvbG7VaTZMmTZg1axYlJf8bI0elUrF48WJefPFFzMzMCA8Pp02bNhrTnfft2xdDQ0Nyc8v/Pz1z5gwqlYpjx44BUFhYyIQJE2jYsCFmZma0a9eO2NhYZf8rV67w2muv0bBhQ0xNTfH09OTHH39Utn/77bc4ODhoTPEOEBwcTGhoqNbnkp6eTpcuXVCr1Xh4eLB161atrq+3tzeOjo6sW7dOWbdu3ToaNWpE69atNcoWFhYyduxY6tevj1qt5vnnn2ffvn0aZX799VdcXV0xMTGha9eunDhxosIxd+7cSefOnTExMcHR0ZGxY8dy8+ZNreKtSWpdYkGdprRs8v+nVXiCurn2NepxCMAbnRpjYqjPkbPXiU29pOtwJAkhBKWleTr50Ta5Njc3x9zcnA0bNlBYWLVJ/T744AMmTJhAYmIirq6uvPbaa8rNKz4+nrfeeotx48aRmJiIv78/4eHh96wvOjqa6dOnEx4eTkpKCnPnzmXatGlERUVVKa6oqCjMzMzYu3cvn3zyCR9++KFy47x1Y4uIiCArK0tZjouLY8iQIYwbN47k5GSWLl1KZGRkhZhnzpxJv379OHz4MKGhofj6+iqJgRCCuLg4rK2t2bmz/IvZjh07aNiwIS4u5dMQjBkzht27d7N69WoOHTrEyy+/TGBgIOnp6UD5t//nnnuOTZs2ceTIEd58800GDx5MQkL5+EIvv/wyV65cYfv27UpMV69eZfPmzYSEhGh1LmVlZfTv3x8jIyP27t3LkiVLCAsL0/r6hoaGEhERoSyvWLGCN954o0K5SZMmsXbtWqKiovj7779xcXEhICCAq1evAnD69Gn69+9Pnz59SExMZMSIEUyePFmjjoyMDAIDAxkwYACHDh3ip59+YufOnYwZM0breGuMap/M/T60nc/9YZTGLRILXu0r5r8SJEZNGS46rOogikuLH9nxHoU5vxwVTmG/iIAFO0RJaZmuw5GeIvn5+SI5OVnk5+cr60pKboo/tjXRyU9JyU2tY//555+FjY2NUKvVomPHjmLKlCkiKSlJowwg1q9fL4QQIjMzUwDiu+++U7YfPXpUACIlJUUIIcTAgQNFUFCQRh0hISHCyspKWZ4xY4bw8vJSlps2bSpWrVqlsc/s2bNFhw4d7hr70KFDRXBwsLLs6+srnn/+eY0ybdu2FWFhYZWeyy3du3cXc+fO1Vi3cuVKYW9vr7Hf+PHjNcrExMQIKysrUVJSIhITE4WdnZ0YN26ccrwRI0aIQYMGCSGEOHnypNDX1xdnz56tcOwpU6bc9RyDgoLE+++/rywHBweL0NBQZXnp0qXCwcFBlJaWanUuW7ZsEQYGBhpx/Pbbb5Vel9vdutYXL14UxsbG4sSJE+LEiRNCrVaLS5cuieDgYDF06FAhhBC5ubnC0NBQREdHK/sXFRUJBwcH8cknnwghhJgyZYrw8PDQOEZYWJgARHZ2thBCiOHDh4s333xTo0xcXJzQ09NT/tacnJzEggUL7hr341DZ3/8t2t6/a1+LBaDn2Z8GpuWn5njJiBtFN0i8mKjboKroX34uWKoN+Of8DTmuhSRpacCAAZw7d46YmBgCAwOJjY3F29ubyMjIe+7XsmVL5d/29vYAXLx4EYDU1FR8fHw0yt+5fLubN2+SkZHB8OHDlVYUc3Nz5syZQ0ZGRpXO5/a4bsV2K667SUpK4sMPP9Q49siRI8nKyiIv73/THLRp00Zjv86dO3Pjxg0OHjzIjh078PX1xc/PT2nF2LFjB35+fgAcPnyY0tJSXF1dNY6zY8cO5RxLS0uZPXs2np6e2NraYm5uzpYtWzh16n+jC4eEhLB27VqlhSk6OppXX30VPT09rc4lJSUFR0dHHBwclDo7dOig9fWtV68eQUFBREZGEhERQVBQEHXr1tUok5GRQXFxMZ06dVLWGRoa4uPjQ0pKCgApKSm0a9dOY78740hKSiIyMlLjXAICAigrKyMzs3bNbm2g6wAeCatn8HAy4tzRQtQ3LqFXpsdfZ/6ijV2b++/7hLAxM+Kdbs0I/zWF+VtS6d3SHlOj2vlxSU8+PT0T/HwP6+zYVaFWq/H398ff359p06YxYsQIZsyYcc9e/oaGhsq/VSoVQIVn/9q61Sdh2bJlFW42+vr6Varr9rhuxXa/uHJzc5k1axb9+/evsE2tViv/NjMz09hmbW2Nl5cXsbGx7N69G39/f7p06cLAgQNJS0sjPT0dX19f5Rj6+vocOHCgwjmZm5sD8Omnn7Jo0SIWLlyIp6cnZmZmjB8/nqKiIqVsnz59EEKwadMm2rZtS1xcHAsWLKjyuTyM0NBQ5XHE119/XS11ViY3N5dRo0YxduzYCttqW2fRWnun8ujux7bk3xEin2bnHPnT+k/efe5d5T+NmmBIRye+33OC01fzWfZXJuN6NNN1SNJTSqVSoa9vquswHoiHh8dDvZLp5uZWoaPencu3a9CgAQ4ODhw/flzpK/CoGBoaUlqqOQWAt7c3qampSl+IqvD19WX79u0kJCQQHh6Ora0t7u7uhIeHY29vj6urKwCtW7emtLSUixcv0rlz50rrio+PJzg4mNdffx0oT9TS0tLw8PBQyqjVavr37090dDTHjh3Dzc0Nb29vrc/F3d2d06dPk5WVpbQ07dmzp0rnHBgYSFFRESqVioCAgArbmzZtipGREfHx8Tg5OQHlb9Hs27eP8ePHK3HExMRo7HdnHN7e3iQnJz/Q51LT1MpHIQCGXv2wNLYEoNk5G05eP0lqdqqOo6oaYwN9wgKbA7D0rwwuXi/QcUSS9OS6cuUK3bp144cffuDQoUNkZmbyn//8h08++YTg4OAHrvedd97h119/5fPPPyc9PZ2lS5fy22+/3fNLyqxZs/joo4/44osvSEtL4/Dhw0RERPD5558/cByVcXZ2Ztu2bZw/f57s7PLX6qdPn87333/PrFmzOHr0KCkpKaxevZqpU6fetz4/Pz+2bNmCgYEBzZs3V9ZFR0crrRUArq6uhISEMGTIENatW0dmZiYJCQl89NFHbNq0CYBmzZqxdetWdu3aRUpKCqNGjeLChQsVjhkSEsKmTZtYsWJFhUTsfufSo0cPXF1dGTp0KElJScTFxfHBBx9U6Rrq6+uTkpJCcnJypS1KZmZmjB49mokTJ7J582aSk5MZOXIkeXl5DB8+HIC33nqL9PR0Jk6cSGpqKqtWrarw+C0sLIxdu3YxZswYEhMTSU9PZ+PGjbWy82atTSwwr4+zgwUAda4VgIDfT/yu46CqLsjTnlaO1uQVlbLgj5oxPLkk6YK5uTnt2rVjwYIFdOnShRYtWjBt2jRGjhzJV1999cD1durUiSVLlvD555/j5eXF5s2beffdd+/ZFD9ixAi+++47IiIi8PT0xNfXl8jISBo3bvzAcVTms88+Y+vWrTg6OiqvSAYEBPDLL7/w+++/07ZtW9q3b8+CBQuUb9v30rlzZ8rKyjSSCD8/P0pLS5X+FbdEREQwZMgQ3n//fdzc3Ojbty/79u1TmvWnTp2Kt7c3AQEB+Pn5YWdnV+kAYN26dcPW1pbU1FQGDRqkse1+56Knp8f69evJz8/Hx8eHESNG3PeNncpYWlpiaWl51+3z5s1jwIABDB48GG9vb44dO8aWLVuwsbEByh9lrF27lg0bNuDl5cWSJUuYO3euRh0tW7Zkx44dpKWl0blzZ1q3bs306dM1+ofUFiohHu9ACdevX8fKyoqcnJx7fpDV4eKWlaxcsQ4oZlP7C5g6N+C/ff9box6HAOw/cZWXluxGTwW/jeuCm52FrkOSarGCggIyMzNp3LhxtT3Hrm1GjhzJP//8Q1xcnK5DkaRqda+/f23v37W3xQKo93wvDI2eAaDFGYca+TgEoI2zLT1b2FEmYO6vKboOR5KeOvPnzycpKYljx47x5ZdfEhUVxdChQ3UdliQ9kWp1YqEyq4N9HVsAHC/oo6qhj0MAwgKbY6CnYkfaJf5Kk4NmSdLjlJCQgL+/P56enixZsoQvvviCESNG6DosSXoi1erEAqC5jxeojNErLsLhkgm/n/y9Rg6T7VzXjMEdyp8rzv01hVI5QZkkPTZr1qzh4sWL5Ofnc/To0Uonr5IkqVytTyzcgnthbNwUgOeOO9XYxyEAY7s1w0IOmiVJkiQ9wWp9YmFkpsbrufLxH2yvFqEuNKyxj0PKB80qfwd6/pZUrhc8fTMSSpIkSU+2Wp9YALQfOQR9fRuglI6pnjX2cQjA0I7OONcx5eKNQj769R9dhyPVYg868qQkSTVXdfzd19qRN29naGJMs+au/HN0L43O6xOXfZ7U7FSa2zbXdWhVZmygz7wBLXn12z38mHCKPl72dGxa9/47SpKWjIyM0NPT49y5c9SrVw8jI6Ma94q2JElVI4SgqKiIS5cuoaenh5GR0QPXVavHsdA47uVLLHs7FBCkujrTop8rY70rjtleU3yw/jDRe0/RyNaULeO7YGJUtTkIJOleioqKKkxaJUlS7Wdqaoq9vX2liYW29++nosUCwLJuPewcGnH+3EncTluzPW0777R+p8Z+E5vcszl//nORU1fz+HxrKh8Eedx/J0nSkpGREY0aNaKkpKTCXBSSJNVO+vr6GBgYPPR98alJLAB8Br5GzIJ5UJBOndTmNfZxCICF2pDwfi0IjdzP8p2ZBLV0oJWjta7DkmoRlUqFoaFhhRk2JUmS7uWp6Lx5S5M27TAyVoO4ybOnnfg96Vddh/RQujVvQN9WDpQJmPRzEkUlsrOdJEmSpFtPVWKhb2BIi27+5QsFqZz5q6DGvh1yy/Q+z1LHzIi0C7l8E3tM1+FIkiRJT7mnKrEAaNH1BQDKijNoeO5ZEo/u1nFED8fWzIiZLz4LwNfbj5F6/oaOI5IkSZKeZk9dYlHPqTH1nZsAZVB4jJ1rE3Ud0kPr3dKeHu4NKC4VTPo5SQ73LUmSJOnMU5dYADzrV/44pLToKKosV84dOq7jiB6OSqUivF8LLNQGJJ3J4Zvt8pGIJEmSpBtPZWLh/rwvegYGiNKLiJIr/HdZMjkXa/b7+g0s1UzvXf7K6ed/pPHnPxd0HJEkSZL0NHoqEwsTC0tcnmsHQK7YQ0mxKb98vpPCvJo998bLbRwJadcIIWDcj4lkXMrVdUiSJEnSU+apTCwAnvXrUf6PwjRyDS9z7ZoBm7/5m9LSmv3K5ow+z9LW2YYbhSWM/H6/nKhMkiRJeqye2sTC2csbMxtbjIpVHLJdQbFeAWeO3eSvVak1+hVUIwM9vgl5DnsrNccv3eTd1YmUyc6ckiRJ0mPy1CYWevr6ePn3BMD9giFbm0UiKCM5PouDW0/pOLqHU8/CmKWDn8PYQI9t/1zk861pug5JkiRJeko8tYkFQKuA3hgaqzHJFlB6knjndQDsXp9BxsGLOo7u4bR8xpp5AzwB+Gr7MX49nKXjiCRJkqSnwUMlFvPmzUOlUjF+/PhqCufxMjG3wLN7AAA9LrhxxD6O9PqxIOCPFclcyLyu2wAfUr/WzzDi+cYAvL8miZSsmn0+kiRJ0pPvgROLffv2sXTpUlq2bFmd8Tx2zwUFo6evT9mpq3gWOvBnkw0UmSdRUlzGf79M5Nyxa7oO8aFM7tmc513qkl9cyvDIfRw4ma3rkCRJkqRa7IESi9zcXEJCQli2bBk2NjbVHdNjZVm3Ps07dgEg4HJLhErwg/sPWKrTKcwrIWZRIpmHLus4ygdnoK/Hl6+1pkldM87lFPDK0t0s2JpGSQ1/+0WSJEl6Mj1QYvH2228TFBREjx497lu2sLCQ69eva/w8adq+OACAy4f+oa/tCxQZFBHT/EsaWWdQWlzGb0sO88/umttHwcbMiA1jOtG3lQOlZYJF29J5eeluTl65qevQJEmSpFqmyonF6tWr+fvvv/noo4+0Kv/RRx9hZWWl/Dg6OlY5yEetbiNnmni3BSFod8YBa0ML/jFRcbH+HJo3SEOUCbZFpXDw95r7toil2pCFr7Zm0autsFAbcPDUNXotimPN/tM1+vVaSZIk6clSpcTi9OnTjBs3jujoaNRqtVb7TJkyhZycHOXn9OnTDxToo9a2T3mrxbH4nYxzHwPAchtz2uhNplXj8tc1d607xq61x2r0jTi4VUN+G9cZn8a23CwqZdLPh/hX9N/k5MuBtCRJkqSHV6XE4sCBA1y8eBFvb28MDAwwMDBgx44dfPHFFxgYGFBaWlphH2NjYywtLTV+nkQN3Z/FvpkbpcXF1E8poqlVU27o6fGjlSWd8sPo0KJ8Yq+DW0/x5/cpNXqEzmdsTPlxZHsmBrhhoKfityPnGbVyv+x3IUmSJD20KiUW3bt35/DhwyQmJio/bdq0ISQkhMTERPT19R9VnI+cSqWibfBLABza+hsj3N4AYGXdBuSqVHhfnki3DqdQ6an4Z/d5YhYmkne9SJchPxR9PRVvd3Xh59EdMTPSZ8/xq3wmB9KSJEmSHlKVEgsLCwtatGih8WNmZkadOnVo0aLFo4rxsXF5rh02Ds9QmHeT+unFOFs6c720gNXP9QfAPXMcvXpcwFCtz7n0a/zno31cPPnkdUatilaO1nz8Uvkrw4tjM9iaLGdFlSRJkh7cUz3y5p1Uenq07VOeRBz8NYaRHsMB+D73GHkd3wHA+fBoXhqQg3UDU3KzC1k3/29S99TcN0YAerd0YFhHZwDeW5PIqSs1ewp5SZIkSXceOrGIjY1l4cKF1RDKk8G9c1fMbWzJzb6K01kTHC0cyS7MZo1DU/AeCqIM2+2hvNT1AE4tbCktLuOPyBR2/iedshrcR+HfvdzxbmTNjYISRkcfoKC4Yn8ZSZIkSbof2WJxBwNDQ7x7BQOwZ000oY1fByDiaCT5gXOhxQAoK8Y49t8EGb1HG19zAJK2nSbmiyTyc2tmvwsjAz2+DvHG1syIo+euMzPmqK5DkiRJkmogmVhUotULQdg6PENu9lXKfjlCQ9OGXC24ys/H1sOA5fDil2BsiercAdql9yLw+QwMjfU5m5rN+vl/U3CzZr66aW9lwqJXW6FSwep9p/nP/ifz1WBJkiTpySUTi0oYqtW8+P6/MTRWc/rIIQZc8gZgxZEVFJQWgvcQeHsvuAZCWTFNj01ggPNCzC31yD6fx6+LD1FSQx8ldG5Wj3d7uAIwdcMROXGZJEmSVCUysbiLOs80wn9UeYfN6zsO4ZnjwOX8y6xLL59aHUsHeG019P8OTGypk/MnvY3GYmRYQtaxHP5YkYwoq5kDaY3p6oKfWz0KS8p464cDXLpRqOuQJEmSpBpCJhb34N7Jl1YBvQFoc8AM8zwDlh9ZTlHp//ejUKmg5cvwdgI82486BifpZT4TPVUJGQcvsfPn9Bo5SqeenooFr7TiGRsTTl7J47Vle7h4o0DXYUmSJEk1gEws7sNvyHDsXdwQBUX4J9pz5cZFNhzboFnIvB68HAkDltPQ4hQ9LBcBcOjPMyRtq5n9FGzMjIge0Q4HKzXHLuby2rcyuZAkSZLuTyYW96FvYEjvdyejtrDE6poePsm2fHf4O4pLK+mg6fkSvPUXzZrepKNFJADxPx8jfe+Zxxt0NXGqY8bqNzvgYKUm49LN8uTiukwuJEmSpLuTiYUWLOvWI2jsRFCpcDttgVnqdebsnVP5Yw7bJhD6O626PYOn6S8A/BGZzNn9NfP1zUZ1TFn9ZgcaWpuQcekmry6TyYUkSZJ0dzKx0JJzy9Z0fGkQAO2P2BJ74L98uv/TypMLAyNUPefy/MgXaGJ6gDJhwK/Lj3Pyjz8fc9TVozy5aE9DaxOOX7rJq9/u4YJMLiRJkqRKyMSiCtr3H0jjVs9hUKZH1wP1+CkxmiVJS+5aXs89EP9/v4q9+RmKhBm//Ay/L9xWIycvc7S9Lbm4XP5Y5Ny1fF2HJUmSJD1hZGJRBSo9PXq+MwGr+g2wyDekS1Jdvkn8hpXJK++6j0FdR/rMegmvZw6jopT0f1SsmhZHcvy5GvfGyJ3JRcCCv1i5+wSlNfS1WkmSJKn6ycSiikzMLXjx/Q8wMDTimUsmtEq34pN9n/xvfItKGJqZ8vzk0bz03H+pa5BBYaGK7Sv/YeOCg1y7ULMm/HK0NeWnUe3xesaKG4UlTNt4lP6Ld3H0XI6uQ5MkSZKeACrxmL82X79+HSsrK3JycrC0tHych65WyX/9yW9ffw7AH89d5GyDAj7x/YRA58C771RaTNl/RpB0oIyE3NcoEWr0DfRo29sZ7xecUOmpHlP0D6+0TBC99ySfbE4lt7AEfT0Vb3R05l1/V8yMDXQdniRJklTNtL1/yxaLB+TRpZsyeFb3w/aY39Rnyl9T+OvMX3ffSd8QvZeX07qdIa/VGUcj44OUlpSxZ8NxNn1zqEbNMaKvp2JIB2e2ve9LUEt7SssE3+3MpMfnO9iafEHX4UmSJEk6IhOLh+A3ZDgOru6oikp58XATKC4l7K8wzty4x7gV+gbQ71ssW/vR2/pDulp9g76+4OSRK6wJ38fFkzVrbo4Glmq+HuRN5BttcbQ1ISungJHf72ftgZo5dockSZL0cGRi8RD0DQzp8+5kzKxtMLxaSFC6C7lFuYTFhVFcdo/WB30D6LcEVavX8DDZygDr97A0K+DG1QLWfnqAo3Fna1zHTj+3+vw+3peQdo0AmLzuELsyLus4KkmSJOlxk4nFQzK3rUPvdyejp6+PTWYR3qfqcejSIRYnLr73jnr6EPw1tBtNPcMTvGI2HOc6JykrEcRGp/JnVArFRTVrhlQTI31mB7egd0t7iksFo1YeIP3CDV2HJUmSJD1GMrGoBs80fxbfwSMAaJlsilOWKd8d/o595/fde0c9feg5D4K/xtiwhF4G79LB7jdUKvhnz3nWfnygxr01oqenYv7LXrRxsuFGQQnDIvbJOUYkSZKeIjKxqCatA3vj5d8LBPgdqk+Dy8ZMjpvMtYJrWuz8OrzxGyqLBnjzLcH152FiKrhyNpc1H+3j2IGLjzz+6qQ21OfbIW1oXNeMs9fyGRG1n7yiEl2HJUmSJD0GMrGoJiqVim6ho2jWriOqUkGPvxtQkpXNjF0ztOsv8UwbGLUDnmlLQ9VeBpqPxKH+TYoLStmy7AhxP6VRWlL26E+kmtiaGRExrC02poYcOpPDuNWJciAtSZKkp4BMLKqRnp4+vcZMwNHDE4MS8N/XgH3//MWa1DXaVWBhB8M2QevBmOldIVg1BO8G8QAc2n6G9Z/9zY2rNeexgnNdM5YNaYORgR5bky8wZ1OyrkOSJEmSHjGZWFQzAyMjgidOpZ5zE0yK9PHfV58v4z8jPTtdywqM4cUvodd89IxM6KCaT5B1OMb6BVzIvM5P4QmcOFxz3rZo42zL5694ARARf4LpG4/IOUYkSZJqMTny5iNy81o2q6ZN4PrFC1yxLCQtwIyVfX/ExMBE+0pyL0Lc57B/OdcLrdiSM5GLxc0AaNPLGZ8+jVGpasZonUt2ZDDvt3+A8sG1gjztGf58Y7wcrXUbmCRJkqQVbe/fMrF4hLLPn2PV1PcpuHGDrDoFXO7lwCL/L7EytqpaRddOwY6PKT24hvjrQzicFwSAZ6e6dA7xrDFDgW//5yLf/nWc3cevKOvaOtsw/Pkm+Hs0QL+GnIckSdLTSCYWT4gLx4/x48xJlBYWcbJBHif9zFn8whIczB2qXtmlNIidS/K+G2y/PhrQw6NFKX7/6lFjkguAo+dyWL4zk/8mnaO4tPzXr3FdM5YOfg7XBhY6jk6SJEmqjEwsniCnjiSx9qPplJWUcqxhLqnt9PjG/xua2zZ/sApP7yM1chnbzr6MQB83hxN0e78femY21Rv4I3bhegHf7z5B9N5TXMsrxsbUkO9D2+H5TBVbdCRJkqRHTk5C9gRp1MKL3uMno9LTw+WsOY3/LmHYb8PYdW7Xg1Xo2Ba3yYvw90lBRSmp55z5Y8YySv/ZWr2BP2INLNVMDGjO9vf98HrGiuy8YgYt28P+E1d1HZokSZL0gGRi8Zg0a9uBwNHjAfA4YUmzZAPe/uNt/pvx3wer0NCEZqFjCehvih4lpOe24ffFCZSufwfyr1Vb3I+DjZkRP4xoh09jW24UljB4eQI702vOmy+SJEnS/8jE4jHy6NKNbm+MAqDVMWtcj5vw753/5rvD3z3wpGNNX+hEzzefRU+vjOOFHdgc+wwli9rB399DWc0ZUMtCbUjUGz50ca1HfnEpoZH75PTrkiRJNZBMLB6z1oF96PTK6wD4pNjictqMRX8v4tP9n1ImHiwRcPZuSNCY1ugb/F979x0nR3En/P/TPT05b85Bq1XOEZFEBhuDA8Y4nXE4nzH4zj7fz499yb57nrOxfXc+h/M54nCOgG3AmCiSQIBQRFna1WpznJ2cZ7q7fn+MtEJIAoFXWklb79erXzPb3dtdUzs7/Z3qqm9BT34lDwz+Den7/hl+fAX0v858JWcRp83Cjz60nGvnV1MwTG775Vb+uGNoqoslSZIkvQEysJgCq991C8vf9k4ALt5dSduAm1/s+QX/tOGfXnu69dfQNK+cG/56KXanhZHiXH4X+Q9CvXG46yq475OQPDe+/ds1C999/zLeubQewxR8+rfb+dGzh2Q6cEmSpHPEeTUqxCwUUG22ST3m6SKE4PEffIfdTz8OQCiQZ1t7jPYlK/mPy/7zjSXSeoXYaIaH/mcnsdEMmqpzhfebtDufB5sXLv40LP2LUurws5xpCv75gd386qU+AObX+fh/71jAsqZza+SLJEnS+WJaDTfVk0kO/eKzRPTNrPyrjVgcjkk57ulmmgYv3PMrtj70AHohD8BIMEdqdSVf+8AP8NneXP3kszqP/3gPfXtKiaiW1zzHavFfKIoAxQIzr4Ql74dZbwHr2VtXQgh++VIf//7ofhK50uyot6xo5P9cN5tyj32KSydJkjS9TKvAIhvv48WNVyCsgqbQDbTf8s1JOe6Zko5F2XT/vby87mFMvXQBjdUovO9j/8C8RWve1DFNU7Dxvi62ryt9429pyXN18L/Rhl5AFzZ0YUe3V6DPvAFl7vUEFq5AUc/OO2PjqTxfe2Q/924dAMDvtPK5a2fzvlVNMlunJEnSGTKtAguAPX/4NCOBP6GmFC5Y8wTOypZJO/aZkgyP89hvf0j3c8+jitIFs/36q7nhL/7mTc8JcuClEZ7+xf7SlOsKcJK/dkNwkGs+cw3O6uo3WfrTb2tvhH+6fw/7hhMALGrw86Ub5rG8uWyKSyZJknT+m1aBhZEqMPS1F+hZ+c8UPEMEx+aw7L0PTcqxp8L+7pf50Q/+kbruUjAhVjbyyb/5D9w295s63mh3gkd/uItUNH/MeosGGnmKugUTDY8W4bpbAlRfctWf/RpOF90w+eXGXv7z8Q6S+VLrzo2L6/j8W+ZQH3hz/VIkSZKk1zetAguA2J8OMbbnKfpX3gnA4sbvUtF+3aQd/0wLZ8N860efw//8GAB9bQbvuu0LrG1a+6aOZxommUQBzWpBs6lYNHVifpHwjm08clcn8UIlKkUuXdzBvI/9JYrt7L1Qh5J5/uOxA9yztR8hwK6pfOLSGdx2WRsumzbVxZMkSTrvTLvAwkgXGfn6ZoZm/pBE/XPYEz4uvHETqmqdtHNMhd//7jt03/soCgqd9Sncb1vGFy74eypdlZN6nnwiyZPfeJDukdKIkbnBLVx6+1vQGhdO6nkm2+7BOP/3T3vZ1F1KA17ts/P56+bwjiX1qLL/hSRJ0qSZdoEFQOLJPiLP7Kb7ws9j2jM0u29l5uovTuo5psLLzzzGk9//bxCC7to021bk+Nzq/8M72985qecRQrD9V0+wcYOCQKXSeojr3lbAd8VHz/rRI4/uHuErj+yjP5IFYG6tj9sva+OtC2tlB09JkqRJMC0DCzNvMPLvm4n6nmJkwV0oRZULLn0al7NhUs8zFTo3vcCD3/wawjDoq8qwfmmI/++Cz/PBeR+c9HP1b+/m8Z/sJVd0YleSXFHzG2Zcdzksu/WsDjByRYOfPt/Dfz/VSbpgANBS7uITa9t417J67JpliksoSZJ07pqWgQVA6oUhon88SP+yfyNb0UWARSy7/A9velTF2aR7+xYe+M+vYBQLDJfnWL8kxOfW/iPvmf2eST9XMpzl0W+uZyxUSjg23/kYF9U+hPXSO2D5h8F69va/iKYL/PzFHn72Qg+xTCmTaZXXzl9e0sr7Vzfjscs+GJIkSW/UtAssDMMgHA5TWVbByDe2kskfoueCfwSLYOG8/6aq5i2Tdq6p1L9nJ/d9/f9RzGXJ2HWeXTLO7Tf846TfFgEwdJNND3Sybd0AoBC09HNN4BtUBDJw0adLAYbdM+nnnSyZgs5vNvXz4+cOMRzPAaUcGJ++sp2/WNOM1XJ25u2QJEk6G02rwCKdTnP33XcTCoW47bbbsB7KE7n7AKEZvyUy81Gshp8LL38WTTt7L4JvRHigjwf/607CA/2YCHa2x7n51s9xY/vbT8v5+vdHeOIne8gkiqiKzoWen7HI9RCKzQ3z3wlLPwBNa+AsbRUq6Cb3vzzI99d3cSiUBmBWtYd/uWE+F86smOLSSZIknRumVWCh6zp33XUXw8PDNDU1ceuHbmX8v3eQH4vSvfJv0f1p6qpvYe78r0zK+c4GxXyOJ3/yA/Y8sw6A4fIcaz/xSW5YfNNpOV82WeCpX+ynZ+c4AM2efdSxiYwZLC1qLRmtgXTBhd1t44a/Xkyw5s3l3ThdDFNwz5Z+/v2xA0TSBQDeurCGf3jrXBqCrikunSRJ0tltWgUWAJFIhO9///sUCgUuvfRS1tQtIfyzPaTKtjO44lsAzJ1zJ3V1k98fYSrtXv8kj/7oWyhFk6zNYMGHbuYdV3/0tJxLCMGuZwZ54fcHS5k8X4M3YOGmv78At//sm9MjninyX0908L8v9mAKcFhVPrl2Jp9YOwOHVXbwlCRJOpFpF1gA7Nq1i9///vcAfOhDH8LzeJJCT4LRsm8RW7EdBY1ly35FILBiUs871cYH+rjrK59FC+cQCJIL/PivWkp71RxmBGYwwz/jTc+WeiLhwRTbHu8FwO2z43ILXImduIYexxbazGOxvyNh1FIZTPOOf7wCm+fsark4Yv9Igi89sIeXDufAaCl38c33LmVJY2BqCyZJknQWmpaBBcADDzzA9u3b8Xg8fPT6D5D5+UGE0Omr/Wtyi7JYtTJWrXoAh6Nu0s89lXK5NP/+tU/g2hsDIOXQeXFBmMGqHAoKdZ46llQt4ab2m1hRveL0jZIZ7yR23538YfsNZE0/TZ4DvPWTC7C0XXJ6zvdnEkLw0K5h/u1P+xhJ5LCoCn97VTufvGymzH8hSZL0CtM2sCgUCvzoRz8iFArR1tbGdcZS8vuj6OkOelfeid4g8HjmsmL5PVgs59d9dSEET6y/hz2/+QNGrNRJcaC+wPNzRsnaj966aPG18O5Z7+bGthsJOoKnoyCMPvUA9//Oji7szHE8xRVroyjX/j9wnZ0ThsUzRf7h/l08tHMYgFUtZXzjlsWy74UkSdJh0zawABgdHeVHP/oRuq5zxZq1tD1vRxQMsskNDFzzY0wvVFW+hQULvnNe5Ld4tWIuxwu/+zVb/3Q/QpjYXC6abrycl6tHebj7ETJ6BgCrauXq5qu5edbNLK9ePul10bOll4fv6kQIlRXue1hd/Ti0XQE2N1jdYHOB1VX6uaIdZlw+pSNLhBD8YdsgX3xgN+mCgdeh8eV3LuTGxedX65YkSdKbMa0DC4CtW7fy4IMPoqoqH7jmZux/ioBukizcz9B194MGM1o/Q2vrX5+2Mky10UMHefyH32GsuwuA+jnzWfXe97JF7eR3Hb9jX2TfxL43td/EF9d8EVWZ3NwOezcM8fQv9wOw1vc9FrgeP/nOa78Al//9pJ7/zegNp/nM3S+zvS8GwDuX1nP7ZW3MrPKcl4GoJEnSqZj2gYUQgt/97nfs2bOHQCDArVfcQvq3XWAKwvYfMr72BQAWLvwfqiqvPW3lmGqmYbDtkT/y/D2/RM+Xpk2fufICLrrlLxh1pri3417uO3gfpjB57+z38g+r/2HSL56bHjzE5od6UBTBhctHmdM8jIM4FDJQzEAmDB2Plna+/huw8mOTev43QzdMvv3UQf77qU7Mw/8hzeUurppbzZVzq1jZUiYTbEmSNK1M+8ACIJfL8YMf/IBoNEpjYyNvm38lxfv7ARiq/grJxR1YLC6WLPkZAf/y01qWqZYYD/Hi737NnmeeRAgTRVGZd+kVXPie97M+/hL/uOEfEQg+PP/DfHb5Zyc1uBBC8PQv97Pv+VL/BYum0rq4gjlrammcG0S1qPDUl+HZr4Oiwnv+F+beMGnn/3Ns7Y3wnacO8sLBMAXjaD8Vn0Pj8jlVvG1RHVfOqZIzqUqSdN6TgcVhg4OD/PznP6dQKOB2u3nbgivwrs8gFIO+WV8g1xzCYnGzZPFPzrthqCcSHujn+bt/QeemUouNRdNYcu31DLcpfKXzW6DAJxZ9gk8t/dSkntc0THY/O8jeDUOEB9MT611+G7NX1TBrVTXl2/4RZfvPwWKHD90PzRdOahn+HOm8znOdIZ7YN8ZT+8cmEmwBzK72cscVM7lezqQqSdJ5TAYWrxAOh7n77rsZGxtDURQumrGc2Xt8CEuB/tn/RK4hhMXiYvGiuwgGV52RMk214c4DPPebn9O/Z+fEOkvAzR7fCAOVWd515Uf5+PLbJv28QgjG+1Psf3GYjs2j5FLFiW12t0atvYuawrPUevqp+stvoTXMn/Qy/LkMU7C9L8qju0e4e3M/ybwOwIwKN7dfPpN3LKlDk7dJJEk6z8jA4lUKhQIPPfQQO3bsAKA1UM/FI21YVZOBOV8k2zCKqjpZsvjHBIMXnLFyTSUhBL07t7PlT/cxsHcXhq5PbDNUgaOlhksueydzL74Mh3vy51kxdJPe3WH2vzhM/94IevHYbJ4qOpWNHhrmV9OyqIKqFt9Zd8shni3y8xd6uGtDN/FsKUhqLHNy+2UzuWlZAzZNBhiSJJ0fZGBxAkIItm3bxsMPP4xhGPisLq5IzadMcTAw50tkG4ZRVQeLF/2QsrKLzmjZploxl6Nvz066t29h56anEfHsxDbNZmPeJVew+Jq3UtUy47Sc3zBMxvtTjHTFGe4YY3hPPxn92PeH02uleX45LYsqaJxXhs1x9kx/nsrr/OLFXn783CHCh2+T1PkdfPLymbxnRQN2TaYKlyTp3CYDi9cwPDzMPffcQzQaxaKoXJVfSD0+Bub8X7INA6iqnUULf0B5+dmZLfJ0M02Tbz7xFTY9/wgzB9wEU7aJbVpjObOuuJxLLn8XHudpHNUT6yf5g/czFCmjN7+MvsJyCubRZFWqKmiY6eSi9y2mrPbsSRl+ZKr2H6zvYixZGoVT43PwibUzeN+qJjkXiSRJ5ywZWLyObDbLH/7wBzo7O7EoKlfnF1EnfAzO+Tcyjb2oqo3Zs/6V6uq3Y7GcfRNpnW5CCH6656f8sfMB0t1DzO7z0DziQhWlWxFZm0FmTQ3veOcnWVmz8vTkdxjbB3f/BYQ7MYSF4cI8evIr6MmvIG6UklZZFJ2Lr7Yx/x2Xoqhnz22HXNHgni39fO+ZLobjOQAqPHZuWzuD969uwmU7e1pbJEmSToUMLE6BYRjce++97N+/H4ti4ZqJ4OIrpBsPAaBpAWpr30V93Xtxu9umtLxTJVVIsSe8hx3dm+h/fhOO3WEcudJF/EBjkvELg7x/wQe5fsb1kzrZ2YRsDCJdED4EkS7EeBfRoRgbutbQn18CQIu/k8vf14Jr8TVTmr3z1fK6we+2DvA/T3cxGCvdXqrw2Pn7t8zhnUvrz7o+I5IkSScjA4tTpOs69957LwcOHCgFF7mF1OFnvOpnpFd0ki+OTuwbCKyivu59VFVdi6pOv1aMI/Rikcfv/TF7H3gIBQj58zy9LITmd3PTrJt43+z3UeupPe3lEJFedv7qQV7Y046JFaca5crmB2m+/kaY93ZQz57bDkXD5L5tg/z30wfpi5RSqi9rCvB/376ABfX+KS6dJEnS65OBxRug6zr33HMPHR0daKqFa7ILqBMV6NE98JYo8dZuxsNPA6VRC1ZrOfPnf4PysountuBTrOflrfzp218nn06TtwueWjLKaHkeTdG4adZN3Lb4NiqcFae9HOMHell31w4iidLIlYWuP3Fh3VNoi2+EhTdD9YKzphWjoJv85Pluvv1kJ5mCgaLAe1c28blrZ1Pmtr3+ASRJkqaIDCzeIF3Xufvuu+ns7CwFF5kF1FGBmY0h4uvxf/YGIt6dDA3dTT4/jKJYaG//Zxob/mKqiz6lYqMj/PE/v0yotxtUhZGVbh4t2wMKODUnH5z7QT6y4CN4bd7TWg69YPDivXvY+dw4ACpFyrR+yrVeygMZKua0U77mSlzN7ae1HKdqJJ7jzkf28cDLQwD4nVb+7ppZvH9Vk8yBIUnSWem0BBZ33nknf/jDH9i/fz9Op5MLL7yQr33ta8yePXvSCzYVisUid999NwcPHsSqWbk6P5u6YjVCmBQOPo5znpWKz3ySztF/Z2TkPgDq6z/IrPZ/QlWtU1z6qVPM51j3w/9m34ZnAKhcOp+nW/rYkt8NgN/u5+MLP85757wX+2nuCNu7O8zTv9xHOlY44XaPNc6FCw7QvtADFbOgYiYEmqfstsmm7ghf+uMe9g0ngNJ8JLetbeNdy+rlEFVJks4qpyWwuO6663jve9/LypUr0XWdf/iHf2D37t3s3bsXt/vUhvydzYEFlIKL3/zmNxw6VOq8Oc/XzLKxJhzYMKI95A/cTflfvY/khUm6Dv0HICgLXsSCBd/Bap2+98qFEGx/9E+s/8WPMQ0DRVHxL5zJkzWd7NJ6AKhx1/Dh+R/m7W1vx2Ob/IRbE2UxBclIjvGBFOHeMOEDXYSHcsRyfqDUGjDf+RgX+36CphRKKcTL22DWdbD282B1nLaynYhumPxmUx/fWNdBNFNKslXts/PxS0pDVN12OYJEkqSpd0ZuhYRCIaqqqli/fj2XXnrppBZsKhWLRR5++GG2b98OgMNmZ2WulVmFOtDz5Hb+FltVAds/vpUDY1/BMDK4XK0sXvQjXK7WKS791BruPMAL9/6Knh3bJtY5WmvZUNfLbt/wxC2SG2bcwC1zbmFWcNYZK1sxPMLWP2xh61YnoFDuGOFa39cJqt1Hd6peCO++CypPvRVushzJgfGjZw8xkigNUQ26rHzkolZuXdOC3zV9W8UkSZp6ZySwOHjwIO3t7ezatYsFCxZMasHOBn19fTz00EOMjpZGhlRbg1yYaqdceDEiXeQPPoT79ovorv09+fwQmuZjzpwvU1V5LYoyvZuxQ73dbHnwD+x/4VlMwwBAq/LT50sQNmMUrCYFzaS+vJmLZqzl0nnXUNN8Zobz9u+NsO6ne8gmi2g2lbVvL2dO5T549O8hMw6aE667E5Z/eEo6feZ1g/u2DfK99V30hksjSDx2jY9e1MLHLpmB3ykDDEmSzrzTHliYpsmNN95ILBZjw4YNJ90vn8+Tz+ePKVhjY+M5EVhAKdfFpk2bePrppykUCigozDUbWFhsxCuc6MM7MP37Cb+vn0S21KfAbq+lvu4W6ureg91ePcWvYGolxkNse+SP7HryUQrZ7Gvum2120/i2y7hgwRW0B9tRldPXiTEdz7PuJ3sZPBAFYM4FNVzytgC2h2+HQ0+Xdpp7A9zwbXCVnbZyvBbdMHl49wj/8/RB9o8kgdJ07X916Qw+fFErHnmLRJKkM+i0Bxaf/OQneeSRR9iwYQMNDQ0n3e9f/uVf+Nd//dfj1p8rgcURiUSCxx57jD179kysazDKmG3U02SUoY9tIntTNxH/ForF0sVKUSxUVFxNQ/37CQbXoJzGC+XZLpdO0bFxA6lImHwmQz6TJpGIMBjuJRoP4Y0rqELBVAQHGlMcWmCyoHEpy6uXc23LtdS4aya9TKYp2PZoD5se7EYIQAGXz4ZHi+FJ78KjhnC7ingvejfVK1fhLXecngyjp1DOR/eM8F/rOugcSwGlWyS3rW3jQ2tacNqmd+uYJElnxmkNLD71qU/xwAMP8Oyzz9La+tp9Cs71FotX6+rq4vnnn5/o3AngFDbajVpmF6pw53swro4TrXmZeGLrxD4uVystzXdQU3PjtL9N8mpFs8hLu59m6z33oHeOAFDQTHbNiLO3NYnFauUv5v0Ff7nwL09Lp8+hzihP/GwfyXDuNfdzazFqvYPUlkWoq8pSVqOhBpuhbhnULADt9I54MUzBn3YO8c0nOukeTwOlLJ5vW1TLypYyVrYGqfKe2Y6nkiRNH6clsBBC8Nd//dfcd999PPPMM7S3v/GcAOdSH4vXEolE2LZtGy+//DKpVGpifbNRwQX6LNzpOGb7CJkL+xlLPYZhlPZxu9tpm/FZKiqunpJvv2e7/j07eeYXdzHW3QVAwa2waUaIrvo0QVcZdyy5g3e1vwtNndzbAMIUZFNF0rE8yUiOdCxPajxJau9G4qE8oWIrJsf2bbApaRptO5jnWkejYy9KzdxSkFG/rPRYPf+09NHQDZP7tg/y7ac66Y8ce3upudxVCjJagsyp8aEqCoLSv7gQcLhhhlnVXtnSIUnSG3JaAovbb7+dX//61zzwwAPH5K7w+/04nac2R8T5ElgcYRgGHR0dbNu2jc7OTgAsQmWJ3sJCowmLKRDaAJm1HYw6H0LX4wD4vItoa/v/pt307KdCmCb7n1/Pc7/5X5LhEAA5F2xvDXOwIUVLWRt/t+LvuLj+4jMTnOXiFKNjjB0MMXwoyXC/wfColWLx6IXZaxllnnMdc51P4rbESiuDrbDkA7D4vRBonPRiFXSTJ/aNsvFQmM09UfaPJDjV/2avQ+OmZQ18YHUT7dWnN3mZJEnnh9MSWJzsQ/ynP/0pH/7whye1YOeiUCjEww8/THd3afii17BxoTGXRrOU1rqo95NY9DiRhs2YHB5OGFxDa8vfEAicphlCz2HFQp4djz3E5gf/QCYeAyDrMNnVEqOjKcXKxgu4fcntLKlacsbLZhom4wMpDmwc4cDGEfJZHQBVMWkJdjFXvQ+vGEBVTBRM1MYVqAvejjL7KpxBP+ppyK4ZzxbZ1hdlc3eEzT0R+iIZFBQUpdRKceT9lSsahNNHE4itbi3jgxc0c+38Gmza9O0HJEnSa5MpvaeIEII9e/bw2GOPkUyWevI3Gy5WZ+fg04IAFIxewo2/JjmvE6GW5h/xeOZQX/8BaqrfjqadWrKx6aJYyLP76XVsfuD3R1swbAb7mpMMl+dobV/Ax5Z9ggtqL5iS4EwvGBzcNsaeZ4cYORR/3f09zizXf/oCKlrKz0Dpjmeagg0Hx/nlxl6e2DeKefgToMJj56Zl9Vwwo5wljQGCcu4SSZJeQQYWUyyfz7N+/Xo2btyIaZpomsY8WyXzwnV4lAAABUsfoaqfkp7Tg7CW/gwWi4fa2nfSUP9B3O6ZU/gKzj6GXmTvs0+z6f57iY0OT6wXCGKeIkaVi8ULL+aiZddR2zYbzXrm8z2EB1Ps2TBEz45x9KKBaQhMw0Doeuk5pb4hdkuWt92xkJp5TWe8jK80HM/ym039/HZTH2PJ/DHbWspdLG0KsqQxwNKmAPNqfXIeE0maxmRgcZYYGxvjoYceore3FwBVVZlb3cb84TJ8udIIB12EiFh/TGJBB0bV0T9HILCaqqrrKC9bi8vVPCXlPxuZhsGBF57lwMYNDHV1kI1Gj9tH9TlZcNPbuezqW7BazpKEUqZJbtdjPPSTPkby7WhKnus/0kDDqoVTXTKKhskTe0dZt2+Ul/tjHAqlj9untcLNP79tLpfPrpK37SRpGpKBxVlECEFXVxcbNmygp6dnYv3s+jYWxGoJhkvDFPVYD4n0T0gu7Se3SByZ1gIAp7OZ8vK1lJevJRi4AItFDis8Ih2L0rlvG09v+SMDB/dRFrHgKJQ6Vo4Hi6QuqWHe/AtYWr2URRWLcFldU1rewsA+HvnGMwxkZmOhyLU3OWi9eu2UlunV4pkiLw/E2N4X5eX+GFt7oyRzpX4ka2dV8s9vm8vMKtnpU5KmExlYnKX6+/t57rnn6OjomFjXEKylJRqkJVeBAyvFoc2ke39NZlGC/AIotJmgHv0zqaqdyspraZ/5hWmf2fPVkoUkf9h7LwfWPYl/ewzNKH2zPlifYtvsGHknzK+Yz+qa1aysWcmSqiU4tVMb0TSZ9Ogoj3/tD3THZqNgcNWVSWbd/K4zXo5TlcwV+e+nDvKT57spGgKLqvChNc185spZcg4TSZomZGBxlhsdHWXDhg3s3r2bI38CFYV6o4wZRjXNlGMZ3Uh2ywMYSoL8bIFxVTW5mVkKIgKU+mPMbPsc9fXvn9ZZPU8mEQ7xyP/+DwMbNwOgWwR7WuOMlOVJunTSDh1Ns7KochGralZxYd2FLKpcdFpTib+Smcvw1Fd/xYGRNsBk7bJDLLhmPsZ4D4XxQYrhYYrREIVYBIe1QHDZhbDwPaVkXFOkezzNlx/axxP7SvPnBF1WPnv1LG5e0YjDKvNiSNL5TAYW54h4PM7u3bvZtWsXIyMjE+stQqXJrKDVqKJBt0H/HozxDvRwB6z0EH93kbSl1G/D51vK3DlfxuM58zNyngtGujp5+uc/YujA3mPWm4og5dRJOXWSLp2wv0C22cUls6/iquarWFa1DIt6ei+WQjd49r9+y+6uWgBUiscl4jqiytrJfOfjzGwMY1v6dlh4M/hPnk7/dHquM8T/fXDvRIpxr13jbYvruHlFA0sbA7IPhiSdh2RgcQ4KhUITQUYkEplYbxEq9WYZLUYVTWYFtmyK4ujLJOduIHrJICY5FEWjqenjtLZ8Sva/OAEhBB0bN7D32aeIjQwTD41iFIsn3Hfcn6evKkui0cqKhWu5puVaVteuPm0tGUIIXvrhg2zd7uKVHWs0i4nVBla7RiphYJqli7VVydLueI75rnVUttWgLH4PzHs7OAOnpXwnoxsmv97Uxw+fPcRA9GgG0JlVHt69vIF3La2nyiffi5J0vpCBxTlMCMHw8DC7d+9m3959RGNHRz0oQqHWDDLDrGKGXokRe4HwhY+RbiwNv3Q6m2huvo3qqrfJfBivQZgmqViE+OgI8bFRosND9OzazmhXB7ziPyLl0BmoyuIsC3LJzMtZ0rwSp8+P0+PD6fPh9HhR1MkJODKJAqZhYnVoWG3qMUm0MokC+zcOs/e5AeKho8NCK7RDzHBspN6xn+p5rViW3Azt14D1zF3QTVOwsTvM77YM8PDuYXLFUm4Wi6rQVObCrqmHFwt2a+m532njAxc0sawpeMbKKUnSn0cGFucJIQRjY2Ps27ePffv2MTo6OrHNKiy0GTXMNeqx2J4kvOpxdGcGAIvFTXXV9dTVvxefd5Fsmj5F6ViUQ9s207nlRXp3bscs6q+5f6C6lis+ehutS5afkfIJIRjqjLF3wxBd28Yw9KP/vhp5amz7qHcdpGFOBZWrL8ZSOaN0u+QMBRrJXJGHdg5zz5Z+tvXFXnf/a+dX87lr5zCzavInl5MkaXLJwOI8FQ6H2bt3L9u3bz/mdkml6WMO5QSDL5Bq2UzRfzQDpCNXQUVxDTW1N+BbthZFm9wJvM5XxXyOvt076dq9ld192xkIdWPNg6Og4tJtaIWj/zqz11zCZbd+HE+w7IyVL5cucnDrGAP7owztD5HNHPuvrCk5qq2d1Fr3UeMdoaY6j728EvyN4K4ARwAc/sPL4eeeKnBMzv9lfyTDSCJHvmiS1w3y+uHHosnW3ii/3zaAKUBV4D0rGvnMVbOo8ctbJ5J0tpKBxXnONE16enrYsmUL+/fvxzRLzc82odFiVNDsS2Ot206qegvCcrQvgSXpwJEN4g0upGz2VfirluF0NstRJacgnA3z090/5bcHfkveyKPpCmt7m2joAEWAsFlwX7mQ+otWUeOpYXHVYuyW0zuV+hFCCCLDaQb3Rxh8uYuh7hy54qvPbVKm9VNj3U+LfStN9m1YFOPYXVQN5r0D1twO9ae3FaZzNMnXHzvAur2lVji7pvKRi1r5q0tnUCbTiUvSWUcGFtNIMpnk5ZdfZuvmLcQSr2ipQKNV8VJfOQB1W8j7+k74+6ppxe9dRkXd1YezfLbKWyevIZQJ8eNdP+bejnspmkXK4jYu3F1GRbx0IR8L5HlxQRi9wsFbW9/KO2a+g/nl889onQpTEBlJM9IVZ6RjjOGuOPGIecw+DmuW9rL9zPa9RJWyDyUfh/wr5jppWoO56naigcuIjGTxljuobvFN+uvY0hPha4/uZ3NPqS+RosDsai8rWoKsbCljRUsZ9YEzn2tEkqRjycBiGjJNk97e3lKnz337yGQyE9vcThftZdXUZeM4LRHy3j7y3n7yngGEpXDMcRzWWsorL6O8/FL8/uVYrUHZonEC4WyYg7GDjGfHCaVDhF/cgflsF2rRxFQEY8E8w+U5RspzeJvquXH2O7ih7QYqnBVTUt5MosDIoTiDB6J0bh0jmzj6dw9Uu5i9uoa6shDRTU8R6okyXmxmvNiCwdGWD1+Fg/aV1cxaWUNZ3eR1DhZC8OS+Mb6xroO9w4njttf5HSxvKWNhvY+5taWlwnNmWoMkSSqRgcU0ZxgGPT097Nmzh3379pHNHh0O6PN4afc30zzuJRC3U/AMkSnfTbp8J5ngAbAc+81WUTSs1jJstgpstnJstnLstmpqat4uc2e8SjIyztM/+yGdL71wzHpdNRkL5hmtKFA2o5WZ7YtZ0ryShRULCTrO/MgI0zDp3x/lwMYRul8OoRfNk+5rVbIEtX4ieiO6ONpyUF5tYdYFTbSvrsVbNnl9I8aSObb2RNncE2VLb4Q9QwkM8/iPqSqvfSLIWNLo54IZ5QRc8haKJJ0uMrCQJhiGwaFDh9i9ezf79+8nnz86XDHoCzDDU096NE60mCRhieEIDhIMDhEMDuFyJU96XEWx0tryKZqbP4GqyrTOrxQdHqRv90769+ykd/cOcsnjv4XnbAYxdxEjaMdXV0tTyxzmNC5mXsMivIFyLGeok20hq9O1PcSBl0aIjaQpq3NT2eSlotFLZY2Gf/iPKFvvojjUQU9+JR25S+jLL5uYqRUETc06C9+ymKZF1ajq5N4qSed1dvTH2NYXZe9wgn3DSXrCaV79yaUosLDez0UzK7h4ZgXLm4MyG6gkTSIZWEgnVCwWOXjwILt37+bAgQPo+omHU/pMJ37hImSJYNpSWG05mu025s9qxl4pSCo7iETXA+D1LmDe3H/H45l1Jl/KOUMIQXigj/49O9n/8kZGu7swYqnX/z27ht3rxheooLZ5Jhe88xZ8lVVnoMQnkY3CwFYY2ETu0E66ujQ6kqsYKh5NMe5151lwWQtzL2vD6X2DrQepMeh9AcpmQO2i19w1ndc5MJpk71CCvcMJNndHJrKAHmHXVJY3B2kMuijz2Ch32yj32Chz2yeel7vt2DR5m0+SToUMLKTXlc/n6ejooLu7G4/HQ2VlJRUVFZSXl8NYnszOEJFtXWzKdbPfMgQKaMLCUr2F+WYj+fk7Ga69C0NJoShWZsz4W5oaP4aqyuGsr6eYyxEZGmCgt4POgy8z0n+IzNg4pAvYCgoKx3/rN1WILvSgXtBKVaCWKlcV1a5qVteunpoZW00DQvuJvfQYe54fY19iDXlRmvFUVQxmLnQx98p51M30H5Psa0IhXQokDj0DXU/D2J7DGxS45O/gsr8Hy6m/l0YTOZ4/OM6Gg+M8f3Cc0UT+9X8J8DutVHrtVHhsVHjsVPsc3LSsgXl18vNJkl5JBhbSpDHTOfb/5G6e7j9IyFVqWvabLlrMKrzWAtrcP2JU7APAmWqkJXsL1W/9IBavnFb7jRJC0Bvv4eW+zezt38GhkQ5C4wO097qpDZf6MWRtBttmxzjYkEIoUOeu498u/jdW1qycuoLrefQd99H52IvsGppPSJ85sclhSdHi209r4CBN5UNodisUMjC4FcxXpVUva4NIV+l588Xw7rvAW/OGiyOEoCuUYktPlFAyTzhdIJwuEEnnCaeOPC+csO8GgKYq3H75TD51+UzZoiFJh8nAQpp0eiLBC9/7Pi9EI+Qcr+ysJ6iu7mJG2xY0rXShEMkA3kQdtXOvo2rBO3E46qam0OeBnJ6jP9HPgc3Pc+iBxymGS/01CuU2ts1Pst8zgoLCh+Z9iL9e9tdnLHfGCQkB/ZsYXXcPe3bbOZRbPdGKAaWkXU227TTZt2NRipjOKszyuRhlszADbZgWJ47YLoJ7v0EZHTi8TnjXj6Dt8kkvqmkKYtki46k848k8oVSeUDLPxkORidlb59R4+Y+bF7Og3j/p55ekc40MLKTTJtnXx6Z77iGSzZEwDRJCkBSgOdLMnPkS5eWDx/2Opgcpr70En38hbncbTmsrashDoS9NoTeBkSpgrXRhrXFjrXFhrXWjem0yn8arGHqR7Y/+iY2//y35TLq0rsrFjsAgfdVZKhqauPPSrzKnbM4UlxRIhzFjQwx3pzm0L8uhDpNU8o39PZ1qlDJtgLKmSspWXELL4io8QXupv8foXhjdA2N7SxOwLf0LKG/7s4sthOChXcN88YE9RNIFLKrCHZe18akr2mXrhTStycBCOqNM0ySZTBKNRjm0bwcD3esQnh78vjE83giKcoK3mWHDlq7BnqrDkWzGM7YCW7ZyYrPq0rDWuLE1+3DMKcPW6EWZ5BEH56pMIs6Lv/s1O9Y9gjCPDhVNuIoM1ORYcfFb+Ni1f4tNK7VemMIkVUyRLCRJFVLUeerw2s7srSohBOP9KQ69HGK0O46iKKgWBdWiomqHn6sKmUSR6HCaZCR33DEUTBpd+5ljfYRWxyY05dgcLLRdCSv/EmZdC3/mlPfjqTxffGA3D+8aAUqtF/90/TzqAg7cdg2XzYLLpmGR70lpmpCBhTTlIiPj7HjkBTr695LzHcLrD+F2xXC64jidSVT1+NwJluwMyscvwXtoGVr+2OZn1aVhnxXEOacMe3sQi1sOcU3HonRt3UTXlo307NyO+YpRPgWbIFEGIX+OYW+KkD9H1l6qc1VRmROczQrHAtry1QQSGonBIYq5LG3LV9N6wQUMiXE6Ih10RDvoTnSzoHwBH13w0TPWUbSQ04mNZoi89DTRzc8wnJvJcHHuxHabmmFmVQ9z5+apLm5EObiOialp/Y2w/MOw7FbwVJ7w+KfqoZ3DfPGB3YTThRNut2sqbrtGU5mLOTVeZtd4mV1deiyXSbyk84gMLKSzhjAF41v72Ld+E9HMOIncOPFCDN2bxVKWw+mKEwiMEAiMTrRsCFNBxOsJZtbgzzRjpIuYSgFTyyEseUxLDkU1Kbddjq99GbZaN9YaN6pz+o5IKWQzdL+8jfXP/I7w7g5s+vHN9hmHQTxgouYNyhI2rMaJm/ZNRTBUkeNQXZq+6gy6Vvq7VLuq+dzKz3FN8zVn9jbV+EHY/TtiRi0Hhlo5sFclGT16ofeU2WlotdKgbqZ+5Md4iodKG1QNAs0QaIJAI/iPPDaWJl3Tc6WlmDv63DSg5aLSrLCHhVN5vvzwPp4/OE6mYJApGCft+PlKFR47s6o9NAZdNJY5aSxz0RB00hh0Uem1y1t90jlFBhbSWU8Ph0nu3MnYrl0M9fQyKGIUZ0YI1A/i84VP7SBCwT2+kMDA5bjHF6H5nFhr3DjmlOFeUY0yTRMkjSVH2bZrPcWhCLmBMeK9A8SHh3l1VilhUUj5BMPuFFFf6ULdOuSmKnb0m7bQVBxzG9hr9JDNJtEMlVp7FXO9s7ELDavDyYXvfh/lDU1n7PUJUzDYGePAi8Mc3B5Czx87mVrAr1Nv3UF98RncljA2JVNa1Aw2JYuqnDzTKACKCrPfWrqtMuOyUvatV55fCPK6SbZgkC7oJLI6h8ZTHBhJsn8kyYGRJH2RzImPfZhdU7l0ViV/dekMVjQHZZAhnfVkYCGdk/RkipGXt9N5YANRdTNaeR+KamIYGoapYeoaDkPDYzqwainyla/oKJrzIYZWkB9agj1fTpnDT8NFs6i4uAXVNj0DjFcqZDOMdncR6u3G6fVR2dxKWV0DqsXCeHacLSNbGM2M0upvpa4QYGzrbvY//wzR4aHXPbZmt3P1xz/FvEsmf/TG6ynmDYYPxhg4EGXwQJRQX/K4rJyvpil5nJYkHmsCjy2B25bG48jgcebwqOMEIk9gVw8HBuXtsPJjsPh9pU6ipyid1+kYTdIVSjMQzdAfyTIQzTAQzTIcz/LKBo/FjQE+fkkr182vQTtRzg9JOgvIwEI6L+i6Tk9PD/v372f//v2kUsdmV3Q4EtTUdlJT3YXVVkqIJIRCLFZDNFJHNFqHSFdS7i+jZmYj1bXVzJ49+5j3nhACI5XC4nCgWGW/jVcSQjDS1UHHxucp5nJYHQ4y5Ngw+iIdqS50i8nskQCVodItKPeKdpa+9z20lrdR6ayckm/h+UyRoc4YgwdijHTHyaWLFLI6hZyB8Rpzorya25GjjE7K1G7KtD7KHCHKFi3FNu+qUiuG481/fhUNk87RFL/Y2Mvvtw1Q0Evlagg6+djFrbxnRSNu+/S9rSednWRgIZ13TNNkaGiIffv2sX//fsLhMBaLBafTictlpSzYg8+9Bbvn2G/YuZyLaLSOaKSeWKwGQ7dRazhoy/tozgewq14UmxdRTKOoCexNHtxr5uBa1oYihxee1Pr+9dy56U6GkoMs7vSz+KAfBYWwL88zS8cx/DYWVi7k8sbLWduwlgZvA8I0CQ/0ERkawFNWjr+qBpc/cMYCEKNoUsjp5LM6uVSRZCRHOpYnFc2TiuZIRfMkIzky8RN31NSUHPOdj7HU+yfcLbNh5pUw8yqoWXTc7ZJTNZ7K84sXe/nFxl4ihzuI2jSVap+dKq+DSo+dKp994rG92suCOv/rDn3VDZNdg3E2dUeo9jl4y8Ia7JpsuZPePBlYSOc9XdfRTjBRVybTTWhkHaHeh0gY+xCq8arfs1IoOCgWnegFJ7Z8gEC+CqfpwlALGGoRQyliqoVSJ1G7hcqKRTQsuhp/cAEWyxSkzz5LFYwCL4+9TE+ih54d2+BPe7HkTQqayYZF4/RXZwkmbNRE7LQlyykPa5A9Ntum1e7AX12Dv6qGQHU19XPmM3PFBSjq1AV1+UyRyHCGyFCKyHCayFCaSH+Uw6lDsFBgnmsdy9z34bGEwVMNTWugYQXUr4DaxWArvU+EEKcUOOWKBr/fNsCPn+umezz9mvs6rCpLGgOsailjZWsZS5uCuG0WesIZNnSG2HBwnBe6wiRzR0cJVXnt3HphCx9Y3SRngZXeFBlYSBJgGFmisZcI9TxMeOxZ8vbQn3U8IRSsmXK86gwCtcspa70cn38hqio/qAES4yEe+tbXGeoopXhXbBqicOxEd0WLScon8BbtaKkTT4JX3tDEBTe9l1kXXIT6GvkoMok4XVtewupwMGv1RaiW0/eNXAhB354IWx7uZuRQKfupqhjMdT3DMufd2NU0Ub2RsN5ERG8mos4hXKwnV7DjdBq4nCYul4nLaeByCVxuQdOCaoILl4F29P1jmoKBaJaxZI6xZCkb6Fgyx1giz0gix67BOLHMscGZRVUIumyMp46dH8Xn0FjVWs6uwdjE3ClOq4VbVjby0YtaaSqXQbJ06mRgIUknUCwmKBRCFArj5AshwuFuhocOEI32YphFFMWKqthQFSsUBORMTMMAfxi3N4zdnj3umIqh4S7OIFh9EeUtlxMILJ3WrRqGrvPcr3/G1ofuB8DmdFE1q51cjYN9rmHWF7eSMksdI1UDPFkNf85OOw3UF4JY9oUgXwo4CgErw4ts9NZmyZo5Wv2tzHG3UzdsRd87yOje/RMJwsrqG7n0Ax9mxrJVp/XWihCCgQNRtjzUw1BnDCjdBXlzn6Qm7c4XWdHeQdmc2dB0ATSses3+G6ZZmgdlU0+ELT1RNnVHGIyV3pdWi8Ly5iAXz6zg4vZKFtb7sagKBd3kTzuH+NFz3ewbPhIUwdXzqrmkvZLFDQFm13hlZlHpNcnAQpImUTocZvdTz3OgZydxWy9u7zgebwSfL4TV+qpZNE0VR7EZqzWAqtpQVCuqRSs916zY3VWUVa/C71+OzVY+NS/oDIgMDVDM5ahsaT2m1aFgFNgR2sHO0E52hHawI7SDSC4ysd1WVJjb42Netxe7Xvq9mLtIR2OSqpidhjEnmnn0Apgr13BklIlbLA1zF7D2gx+lZuas0/4ahzqjbH6oh4H9UQDcATvldW6C5YJy2xBl+m5chUPkCnbSeQeZgpNswUEm7ySa8dMfbz58JJN2x3Os9NxL0DoM5TNLeTZsHrC5X/HoLvXnmHVNafuRcsSyDMayzK/z4bKdvNOnEIIXusL88NlDrO84tvXOpqnMrfWxuMHP4oYADUEnmkXBoqpoqoJFVdBUBbtmoSHoRJUZR6cdGVhI0mmSy+Xo6Ohgz9ad9A30othD+Pxj+H1j+PyjOByvnb/glWzWRtyexbhci3C7FlFTswTrK0amGOkihf4kAI72AMp5OBRRCMFAaoCdoZ3sDO0kZ+Tw2/34hRu2DpB8cS9G9tjgLetT6KyJc7AmScKjYysqLOzyM6/Hh8U8fMGbU8WMG67GWRGkYBQoGAXyRp6iUSRv5Knz1HFd63VY1T9/JFAyksPmsGB3vbFjhfoSbLlvN4f2HUlfXgowFrkewcRCxgyQNfxkzAAZ00/W9KMpRZyWJM6KMlzNs3C2LcVZVYEn4CjNo3KKOkaT/PHlIXYMxNg5ECf+qr4vr8Xr0FjRHGRlaxkrW8pY1OCXHUOnARlYSNIZIIQgHo8zODhI//799HV0ETPHcPoiqBYdRRGoiomimChq6dHpTOLzjeF2x487XrFgJx+vwZpoIRhdSHV0ITZRulioPiueNXW4V9Zg8UyfPh35TIbtjz5Iz45t1LbPZs5Fa6lqmYFu6nTFu9gb3jux9A92smC/m7ZBNwoKhiKI+ApEvUVi3gJRT5GYt0jWboACLb4WPrPsM1zRdMWUJqgK9SXZ/FA33TvG/6zj+AMmTe0OmhZWU7+wGavzFe+TQgbGOyC0H8b2lR6Tw1A1D1G/gmHvArZka3l5MM2uwRjhw9PK64ZAmAYVZogmcwBfMUSXXs1e0UyS0i0/m6ayuMHP/Do/lV770eXwSJZytx1VAVOAKQSGKTCFwBSlRGHW8zBgPh/JwEKSpohpmoTHxhjbu5fxgweJDA0TS8RJGAZpl4ui1YqCgk0r4vWP4/OP4fWN4faFjps/xTA0cvEqlFQlqCqKYoJqotoEqtuCoinYrLX4fPOoqlp6uMXDOUWvfOoVzSJdsS62736O/gefxtKXOOF+psNCzFUgbs+RdugEq2p5y8J3sKhtFd6KSpxeH4qiYAqTXeO7eLL3SZ7sexJDGHx4/oe5adZNk9LS8WpHAozhg3Hsbg2Xz4bTa8PlteH02XB5rRi6IDMyTGawh+x4hGxGIWMGSJtlCI62GqgUqXN20VTWT7W2H29mF241/NpZR60uqF1SGt2i2WG8s7REukrpzl9dXq2WXUYT2wpN7BEtbDdnEuONTW7nsKqsai3nkpkVXNxewZwar8xCepaSgYUknWWErpPvOoQeCiH0Iug6RsKgMKJSHNPQ8ybhwH6iZTvIBQ6i+YbRrCfOp3AypqlQyJUh8jXYzTbqq65i5uIrsAWmZ7ARHR4k1NvNeH/v4aWP2PAQQrx2oiyL3UbRb2XEFifkSJFw6STcOjmbgWYoNDrquLn1Xcz3z0XP59ELBSxWKzaHE6vDgc3pxOZwodltOH1+bHbHaXyRvbDvQQpdmxkYdtA3XkdfejZJo+q4XVV03PY0Xo+OL2jFU+bApQ/iTO/DGd+BUx/CpcaxK6njZyS22KCsDXy1pblb4n3HHV9XrGzzX80DjnewPV9HKJUnnMpzCtOqTKjw2Ll4ZjkXzaxgRqWbcredco8Nj12TAccUk4GFJJ1DhBCYyQKq2zrRj0IIk5GR7fT2PUkmM4jI5jFiKYx4DlG0gdUPCuiuMRTPKDbPOJp2/H3yfM5NMdyCK9RGXaSJMpuG4rSjaG4MuyDviFGwh8lbwwjNoKLqKqqXvxXNeX7OzKkXCoQH+4mPDpMMjzMy3Muu7s3EQ6O4chZc+cnNeKmrAmV5A7d86P/QXNU2qcc+GaEXifUM0rdziP7ONNGohVRSwTRO7eNeUUxctjxuT6lDqrvCj7u6HHfAidNrJZ/RyYzHSi0n41Ey8RyZNGhGkirbQWqsB6huK8d/+QcwZ1xB7HD/DVVRUA93BFWV0s+94QzPHc698dKhCNmiccIy2TWVCk8pyPA6NEyTw7dTSrdUDFMghEA3j95q0U2BaQoMIXDbNC6bXcU186tZ0hCQnU/fBBlYSNJ5TOg6mc27SD1/CCOewkim0VMJ4to4seAY2aoERm0Ye/koquXoB7VpKmQSNdhRsbqiYDtxR1MtW46vdz7OoRbiDi+jXg2qq7AFAmiaNrFYLBZsNhsejwefz4fX68XlcqFOYXKrN6sz2sl/bf0vXuh7jjq9jAsci5mtNOHPWEmMjhIdGSKfSqHZ7WSVAjEzQdFiolsEQXc5ignpTAKlYGA1VDRdwaqrWETpApa3mohVjbzvfZ9jRuXMM/76TFOQiedJhnMkI0eWPNlk4fBSJJsskM+cOLfIm2FXklS7h6ieVUvVojlUzW3F5T95wJrXDbb1xnj+4DibuiOMJHKMp/JkCgYWDK5VN/MR7VHKSfB741J+ZVz5hm+9QClZ2NXzqrlmfg1rZpTLYbanSAYWkjRNmYUC+tgYRjhMIRmhO/QCY8UdGO6D2JzHdxjVc27UVBB33I8Fg1TTIXjFLZhEooKx0RkkU+WYhlaaEM6wYhgaQqjAsd/8VFXF4/Hg9Xqpr69nyZIl1NbWTjRjm2aBfH4Uh6PhrGzajuaieG1eNPW1Wy5G0iN8Z/t3eLDrQQRHP0btFjtLqpawqmYVK6tXMrRjJy//4T4c0dIFO2PX0dc08v6b/47ZFXP+7PLm9BzD6WEyxQwzgzOxW/68liZDN8kmC2QSBVLRPJl4nlQsTzqWJx0vBSEOtxWXz3Z4sePyl/qB5DJFRrsTjB4cI9SfwTCPHyniscWpDiaprLNS1VZB5bx2HLXNJ0+Jnk9R2PJz1Je+h5boP2aTbnEy0PxO+mZ9mJy3GVVRsFgULEppaKyqHn0ciGZZt3eUp/ePkcofDZ68do01beWsbCljRUuQBfV+2Zn0JGRgIUnScaKxDg51PcLoaIye3hzjIRXTPNoJ0WKxIESe8vJ+qqoPEQwOH3+v/RWEqWAUnaRijYQjDYyEK4853hEBXWe2c5BAVQ+5hmFMexFrwUuF8xLq5n4If+WKszLIOBUHIge4t+NeyhxlrKxZyeLKxdgsx47aMU2Dxx7+X3bcfz/WZKkFKe4qEm+x4S4vp7yyjrqaVloaZjOzdi4VzgpSxRTxfJx4Pk4sH5t4HM2MMpQaKi3pIcazR0eS2FQbiyoXsaJmBSuqV7CochFObWr61xi6SbhrmNHnn2a0Y5SxVCVRvQ44/qLtUBMEXXH8AUGg2kWwqQZ/QwWBgXuxbL8LcocDYmdZaSr7slbY+D8wsgsAU1iINb2PSMP7CNQGKLcNoEQPQeTwEu0GvQDtV1OY/TZeNOby2P4I6/aOEkoeO5TZYVVZ2lgaSru4wU/AVbr14rFreB0abps2bW+jyMBCkqTXJIQgFArR2dlJZ2cnfX19mKaJy+VixowZzJgxg4YGH7nMekYG/kQ+N4yhpzDVAkI78X1wxdCwR+aihuZTGJ/JkKsHvWYb5ZXdJ8xaeoQ1XYZvfBlBsRaPtx1VSWJmRzHGR9FHR9HHRjHSaTxr1xK46d1Yq4/vmHgu0ItF1v3x5+z644NYcieuw6LFJO0wSLl0Uk6d5OHHlFMn5dLJW81XNxLh1JzYLXZiuRj2ooo7a8Gd0/DmbTQrNVRU1tO4bCkzG+fT6m+l3FF+5gM5vUBhqJPQvi5GD0UIDRuMRf0kimUn/RUFA59llKAzSrC1nsCCZQQbgmhWlVBfgtCeDsYPDjOeCqCLox1kveoYLY5NtNo3U2fbi0V51e0dZxBmX48590Z22ZewsTfF5p4oW3ojx6VLPxGPXWN2jZdr5lVz7fwaWircb7paziUysJAk6Q3J5XKkUinKyspes4+EHgoR/vUvif7+N+j5BHq1Sn6ZndzCPHowf9LfM4p2QuONhEItJBIVBMuGqKropax8ENXyig9+Q0NBRREqmBxJfoBSEFiHNGw9FvzeBVRf8TGMeYs52NWFoijMmjULv99/0vOfTQrZDC898QC93fuJjY+QjcYwE1m03ClO625RUa0ammbFanNgtdkBQTI8jlE8+YVxNJijtybDeINKdW0TLb4Watw1E0u1q5oadw0+m++MBR6FdJZ45wFiXb3EBsPExovEEjZi+QoK4tQv2JpFJ6j2EdXr0cXR20E2m0lzm0Zrm06L/jDWjgcg84p8IXZfKZV69QLMqvn0W1t5PhZgc1+SAyNJUnmdVF4nmStSPEnn19nVXq6ZX80182pYUO/DFBBO5xlL5BlN5BhNlOZ7qfDYuXxOFfXn6CgtGVhIknRamZkMsfvvJ7djB7aWFmyzZ2POcBAV2xkff4JEcieq6qCi/Eqqq99GRcVaIpEkO3bsoKOjg3A4jK7rqGqR8vIBKip7KSsbPC6Xx8kIUyGdDpJIVpBOlZHPu/BqDTRXL2LOrCXUzKhHdVsRBQMjUcBMFjCSBYrxPIlIHNVqoay9GnuLH9V2dmSNLBbyRMaGGBnpxYimyISjxMdGSITGiIdGSUcjr38QwOUP4C2vQPN7SNryxPsHUYaOzekR8ufpr86QdhgUNZOiJg4/mqgOO25PaZisQ3PgsBx9dFld1LprafA20OBpoAI/lmiO+PAwiqJS09ZOeWPTa04edyqEoZNJGURHMsRG0kQOP0ZHMugFk4pGDxWNXiqbPFQ2evFXuVAxKRYFA/ujdO8cp2fnONnk0UDLarfQtrSC2S3j1KceRNn/IKRGjjmvLqyMGgsY0NaStjRR1+qgeVkTztaF5K1eUjmdaKbIi13jPL53lBe7wuivGE/rdWhkCgbGK9Z5yLBK3U8GB5vN2bTXBLh8ThVXzKliaWMAbRL7dCRzRUbiOdqr33in1tcjAwtJkqZUsRhFVR1YLCf+dmaaJolEgvHxccbHxwmHw4yPDzI21o1uFFAQKEppcWkOAqpKwd2NyzeKzxfCbj956nRd19DzHiy5MorxBhLhRmJpP2mlQJY8hwdqYBcaZcJDud1DVWsBV30fwtmF1ebBbq85vFRjd5Se26zlWCyO0rwviu2M307QCwUKuSxGsYheLGAUChi6jl4oIISJJ1iOp7wCzXp8P5dkZJzOl17kwMbnGDqw75RmTctZDbJ2g4zj8KPdoGA18WY0/CkrgbQVR+H4AEKz26mZ0U7NzFnUtM2ipq0dX0UlyhkeLWSagrGeBN07QhzcOkZi/GiSL0/QzqyV1cxqCVMcOsBAZ5LBIQfDqXoMcWwfGQWDGmsHLYFOmluKlM1oRKmcBWWtRK31PLkvwwu7R+noiWEpCnKKoMoyxmrHfi7QtrPQ3IZdKbXmhYWXdcZyHjVX8by5AJfTyYVt5TitlomhsrppHn4UaKpyuH+HFc8r+nrYNZWxRJ6heI7heJbhWI6hWJbk4Y6pu//1Wjz2yR06LQMLSZLOSbquMzg4SHd3N93d3QwMDGAYR/sjOJ1OZrbOpKXSgXt0E6nwJgr2KEVfgaIvh2I78e2YQsFBNFpHNFJHLFKDEBYCZcOUlQ9QFhzEepLfOzkFVbWjqnYsFifBwAXU178Pv3/5Wd8RNR2LcnDzi/Tv2UU+m6GQyVDIZo4+z2UnZo09FSmnTsxTxGIqlMdt2PTjAwiL1YqvsgpbmZ+8RyFsy9BvCTHsSmIG7DgtTuyafaJ1xKk5mRWcxfLq5cwpm/O6o3RejxCC4a44B14aoWvr2GsOq3V5LdTX5fEoY/T12wmnj50s0GcZwaEkSZnlZMwAJ+qQ+mo2NYum5FGEjoIABCiQFTbS2BhVrYyrClGLSVQVxFRBXBUTQfAbEXBZ+eMdF9NUPrmzLMvAQpKk80KhUKC/v59QKER9fT319fXH9QERQmCmMxjhcfLjg8RGO+gJHSCiD+EIDKL5BsDyyr4HCggFXpHe2ihaiUUaGI/VoiCw2TPY7RlsttKj3ZY5peBDmHWoymVo6YVYEgWMeIJ0KkkmkyVTyJM1IKdYyGkWQEEFLKJ0abIc/tmuqDTUVTLnirXUt7chRA7DSGEYWez2ajTNMwk1+xqvQQhy6RTpaIRUNHL0MRYhl0rhr6yirK6BsvpGArV1RM0E/cl+nh98nkcPPUpqdIzKuI2KmJ3qhJNAQuO1MomHAnkO1qforstQsB6/o1NzsrhyMcuql7G8ajnzy+bhtr/5OtCLBj07wxx4aYS+3WGsDgv1s4M0zA5SPztIsMZ1THCYjOTo2dpHz7ZBBnoNTPPY959KEbclgkcNY1dTFEw3WeEnp5ST0x2INxMdlA6MJWBDVDnIl1lJuQWu7CECyYMEcgPYHS7sngAuXxC7rRwjV0Yu4SQRVbj+U8tRtMlNOy8DC0mSpMNMs0A8vo1weD3hyLOkUvsBcLnaqKi4nPKyy3ElqyjsPUB41wHGRsKEMzmiiiDushNzahQ0BRAoiomqGscsNluWqupDVFb2YDmckMwwNMbGWhkPNWMYJ/62rVnz2GzZYxdrDqsth8VSPLwcO3pEUTR8vkUEgxdSFrwQv38Jqnr2ZEkVQrBzfCePdj/Koz2PMp4dRzEpjVLJaARyDppFFVUFL660Sm5oHA63jigWFdfcZuyLmyk0edgd2s3Brh3Yw0XKErbDixW7biFvFxhuDc3nwhkI4C+voqKyHm9ZOc5AAKffj9Prw2LRUBUVq8WK3+bHbXUfEzQYuomqKiiqQqaYYSg1xEBqgIHkAOlimgUVC1hcuRiPrRTIFHI6Q50xhCnwBB243QbOwiBKvAeiPZCNlTqDNl8EVgfCFOQzOtlUAb1ogijVkTBMxOge6N5AsXsTibhC3KglrtcSM2qJG3UY4tjAQEWn2tpJvW0n1bYOYno9o8VZjBRmkTKPHSn1wc8142+b3EyvMrCQJEk6iXx+FNPUcTrrX3M/oesUBwbIdR0i3tVFtK+X7OAQ2dER9KLAcHoxHV4MpxfDXYbhc6C1dGGv347mDk9qmU1TwTS149K2K7qGM1yLc6QOV6wSZ6Ecq9eGxe/F4vNj8XnR6upQ29ooahq5XI5cLkc2k8XUDVpnzsDhcLziPEXi8a2oqh2fbzGK8ub7RRimwdbRrWwa2UStu5YFFQtoC7Qdc1sjHYuyb8Mz7F3/JKG+non1DreHYiH/mqNcXo+JIG8zyTgM0g6diK9ALGBQqLTj8PvwOwL4bD5i+RgDyQHCuRP/zVRFZXZwNsuql7GsahnLqpdR4ax40+U6jhClyd46Hy8tvS8gDJ2UWc5IYQ4DhUX0FxaRNKpPeggFkzL7CNW2Tmose2j9Pz/AUVZ+0v3fDBlYSJIknSZCCIzxcfJdXeS7uij09mLx+7E1NWOtbcDUyohHdjCS/gMZayeKBdAUFE2lNAFp6Ruz1erDZqvCbqvEZq/EbqvCZqvEZiuHjJXsllHGdowykjMZVpMMq3FwRAkEhwkERggERrDZjp111DQtZBNVpOJVJGIVxOPl5IT9pPfqLai0OaqY2ZTHVnWAmOUFDJEEwOFooKb6Rmpq3oHbXfr2WygU0DTttKRtH+s5xJ5nnmD/xqcQlhD5uA1N81DZPIOq1hlUtbRR1TID3anSM9TB4MghQmMDxMKjZKIxjGQGa1ZgzynYc6C8OuHHK2RtBuP+PBF/gYImsJgKFhMc2PFbvHgtblRUQoUwcTOJoQp0i8CwCHRVEAhUMKthIUtnrGZN+1p8rsBx5xBCMJAaYFdoF7vGd5EsJFEVFVVRsSgWFEXBolhwaA4ubbiUZVXLSq0p+SQcWg8HnwA9D1VzoWoecUsbA4M2Bg5EGe9PEah2Ud3qo2aGn6pmLzaH9sqTnzyb6ZskAwtJkqTzhJEqkHy2g/SWXuJ6njE1xZiSZowkWecg/uAIfv8Ifl/ohP1AcjkXhbybQsGFnndj5r2IvA8dA2fFIcrKBo9pCVELHoSqI7SjQUsuVsbYaDPDsVZUYeKxZ/G4dFweA6eniM2aRVWtoDSiaE0oaj2KUsqHoSgKbrd7ItW71+vF4/FgsZRGlJimTjK5m0j0eSKR54nHtyFEEYvqoa7uFhobP4TT2VDat2Agcgaqx4ryGhkwTcMgm0yQikbIxKJER4YZ7jrA8KFO4kNDpzQq5o3Q7QpWnwe3P0BWKRA3kkT0GFmRLwUkqiDjMIh6C0S9RQq24/uSNHgauHHmjdzYdiP1nhO3pmWKGfZH9tOb6KXaXc0M/wyqXdVnpMOwDCwkSZKmgUKhwPDwMOFwGIvFgqKMouf2U8jtplDcR1EZOrXj5J2MjzcxPt5EPF6FpggCFf1UVR0iGBxCVd/4pSKfd5JOB8mkA5ii1MKhHJlXRRFYFQWnM4XHP4TlVbd4DEPDciRxmlDw5y6grO86rH1NKEIBi4LFb0cL2LEcXrSAA9Vnw+KxonptWNzWUivRqxTzOUK93Yx2dxHqOYRRLGKxWkuLdvRRVVX0YhG9kEcvFA4veTKpOOH4GKloBNJ5VPONX9QVjx2lygsVHhI+g5cTu0kpWYqWUj6RebULecvst1HnqWff2B46Q/vpCncyEhtCNUEzSpPcWXUVj+mgylJGuerHjxuXaefdf/NP+N3BN1yu1yIDC0mSJIlCIUw2208+P0o+P0w+P0ouP0I+P4qhpwj4VlLuuhh7oYEDnQfZebCL3nApM6XNolFh91Jh0wgEDmEJ7ED3dYNhRc37IO/FzLkwck6KeQdC07H4olg8ISyu6BsqZ7FoIx6rIRarIRqtJZfzEgwOUd+wl2DwaBIrkajHPXQBFt2JIiyophWEiiqsYFowBBSFQsEUFIWgaFEoaApCdRLw1FFeWU5lXRXBhkqsFa5jkqOZpkms+1nGuu4jkn+JnC2CNeHAGrJiHVaw9BtY+hSsSR9aZSWet1yO860XM6QOsqt3A4Oj+8im41jMKoKWNmrsNQQ1P0I30At54mOjjPf1kAiNTc4f9zXc8B9fZVbjgkk9pgwsJEmSpDcllUqh6zp+v/+4Jna9mEO12Cb6WAghSh1cd+9GD0dAVcHQKBo6Ocs4WXWEnDaGEAYYAgwQJuiilAhKFN1Yx2diTbRgUexYVCsqCiaCEcIMmENEg/1UNu6nqvrQKWdmPZFCwU4mEyCdDpBNB1FS1diyNdg8o9jLDuKq6MHmTL3ucYShIYSCqp28Y6lpWtFopaxsKVV1F+D3LcLpbEJRVPKZDOP9vYz39TDe30NkaJBCNkMhm6WQy5LPpCnmsvDKq7OqYLFZcXgsuCsNbB4DM12PxRLEsELWUiSl5IiKJDEzzZdv/zEuz+ReY0/1+j25abkkSZKkc57Hc/IcEZrVcczPiqJga2zE1tg4KecWhonIGxiZPO35JEYkTCE0xtDAcnp29ZDw7cDiHUdRTBTVLD0qAkU1UBSBqhioiomqGCiHn6MKVKuBzZbHZhslEBg96fkNw0IsWks40kAyUYHDmcLliuF2xXG54jhdcSwWfaJbqBAKetFGUbejF0vDft2eKBZLEZMOxmMdjMfunthXMV0opgfF9GARbhzVXhqqXJiqDV1xUhBeiqZKriDIZnQ0aw6vL4ndHkZRehAifrTu2Y/LtRbDvJxoxIs6Pk4+FIJsBqt96uYjkYGFJEmSdNZQLCqKS0V1WQEP1NfiBPzA3DdxPCOVRg+NUQgNkortI505SFrvJaUMk3dGEI4MIu9ES83GyWq8notpD/qwBjQsikLBalKw6ORUnQJFskaOXGEQTdNwu6twJHTEps2Yz72IPZEFh49I/YWMt2qkaiKIwBBubxiPJ1JqbbGkEZY0glFM4NVtHipgB+wu8AeO3Xbk/kIu58bQbbg9UdKZJ4EnKRQrSaXmkk43AirjgyPUtkxOsPdGycBCkiRJOm9ZPG4snlbsra14ufi47YaRQVUdf1a+Dq68EjObJfHoY6Sffx53PE79wQTG9iT5nMG4fw5jVZXk/QLh0BGOAoqjAPY8ii2Pas1jUwWaKtAsAk01sFhMVFUH3U4hXU4mEyCe8RDJuEiaJijg9YRpru8gUHkIvz+E3x9CzQbx91+Br3DyuXRON9nHQpIkSZJOMzOfB9NEsdlQLCef+dXMFzCiKfRICiOWxkimMVNRjEQYIxpCHx8hHx4nnUjiKOpYVC+Gz0py/jCJ2QcxHKUhwstm/Zpgw+pJfQ2yj4UkSZIknSVU+6mlXVftNtSaMqw1ZW/4HIaRZ3TsQRKJnZMeVLwRMrCQJEmSpPOAxWKnrvbd1NW+e0rLMfk5WSVJkiRJmrZkYCFJkiRJ0qSRgYUkSZIkSZNGBhaSJEmSJE0aGVhIkiRJkjRp3lRg8d3vfpeWlhYcDgerV69m06ZNk10uSZIkSZLOQW84sLj77rv57Gc/y5e+9CW2bdvG4sWLufbaaxkbO/2ztUmSJEmSdHZ7w4HFN77xDT7+8Y/zkY98hHnz5vH9738fl8vFT37yk9NRPkmSJEmSziFvKLAoFAps3bqVq6666ugBVJWrrrqKF1988YS/k8/nSSQSxyySJEmSJJ2f3lBgMT4+jmEYVFdXH7O+urqakZGRE/7OnXfeid/vn1gaJ2lqXUmSJEmSzj6nfVTI3//93xOPxyeW/v7+031KSZIkSZKmyBuaK6SiogKLxcLo6Ogx60dHR6mpqTnh79jtduynOPmKJEmSJEnntjfUYmGz2Vi+fDlPPvnkxDrTNHnyySdZs2bNpBdOkiRJkqRzyxue3fSzn/0st956KytWrGDVqlV885vfJJ1O85GPfOSUfl8IASA7cUqSJEnSOeTIdfvIdfxk3nBgccsttxAKhfjiF7/IyMgIS5Ys4dFHHz2uQ+fJJJNJANmJU5IkSZLOQclkEr/ff9Ltini90GOSmabJ0NAQXq8XRVEm7biJRILGxkb6+/vx+XyTdtxziayDElkPsg6OkPUg6wBkHRzx59aDEIJkMkldXR2qevKeFG+4xeLPpaoqDQ0Np+34Pp9vWr9xQNbBEbIeZB0cIetB1gHIOjjiz6mH12qpOEJOQiZJkiRJ0qSRgYUkSZIkSZPmvAks7HY7X/rSl6Z1zgxZByWyHmQdHCHrQdYByDo44kzVwxnvvClJkiRJ0vnrvGmxkCRJkiRp6snAQpIkSZKkSSMDC0mSJEmSJo0MLCRJkiRJmjTnTWDx3e9+l5aWFhwOB6tXr2bTpk1TXaTT5tlnn+WGG26grq4ORVG4//77j9kuhOCLX/witbW1OJ1OrrrqKjo7O6emsKfJnXfeycqVK/F6vVRVVfGOd7yDAwcOHLNPLpfjjjvuoLy8HI/Hw0033XTczLznuu9973ssWrRoIuHNmjVreOSRRya2T4c6eKWvfvWrKIrCZz7zmYl106EO/uVf/gVFUY5Z5syZM7F9OtQBwODgIB/84AcpLy/H6XSycOFCtmzZMrF9Onw2trS0HPdeUBSFO+64Azgz74XzIrC4++67+exnP8uXvvQltm3bxuLFi7n22msZGxub6qKdFul0msWLF/Pd7373hNu//vWv8+1vf5vvf//7vPTSS7jdbq699lpyudwZLunps379eu644w42btzIunXrKBaLXHPNNaTT6Yl9/vZv/5YHH3yQe++9l/Xr1zM0NMS73vWuKSz15GtoaOCrX/0qW7duZcuWLVxxxRW8/e1vZ8+ePcD0qIMjNm/ezA9+8AMWLVp0zPrpUgfz589neHh4YtmwYcPEtulQB9FolIsuugir1cojjzzC3r17+c///E+CweDEPtPhs3Hz5s3HvA/WrVsHwM033wycofeCOA+sWrVK3HHHHRM/G4Yh6urqxJ133jmFpTozAHHfffdN/GyapqipqRH//u//PrEuFosJu90ufvOb30xBCc+MsbExAYj169cLIUqv2Wq1invvvXdin3379glAvPjii1NVzDMiGAyKH//4x9OqDpLJpGhvbxfr1q0Ta9euFZ/+9KeFENPnffClL31JLF68+ITbpksdfP7znxcXX3zxSbdP18/GT3/606KtrU2YpnnG3gvnfItFoVBg69atXHXVVRPrVFXlqquu4sUXX5zCkk2N7u5uRkZGjqkPv9/P6tWrz+v6iMfjAJSVlQGwdetWisXiMfUwZ84cmpqaztt6MAyD3/72t6TTadasWTOt6uCOO+7g+uuvP+a1wvR6H3R2dlJXV8eMGTP4wAc+QF9fHzB96uCPf/wjK1as4Oabb6aqqoqlS5fyox/9aGL7dPxsLBQK/PKXv+SjH/0oiqKcsffCOR9YjI+PYxjGcdO2V1dXMzIyMkWlmjpHXvN0qg/TNPnMZz7DRRddxIIFC4BSPdhsNgKBwDH7no/1sGvXLjweD3a7ndtuu4377ruPefPmTZs6+O1vf8u2bdu48847j9s2Xepg9erV/OxnP+PRRx/le9/7Ht3d3VxyySUkk8lpUweHDh3ie9/7Hu3t7Tz22GN88pOf5G/+5m/4+c9/DkzPz8b777+fWCzGhz/8YeDM/T+c8dlNJWmy3XHHHezevfuYe8rTyezZs3n55ZeJx+P87ne/49Zbb2X9+vVTXawzor+/n09/+tOsW7cOh8Mx1cWZMm95y1smni9atIjVq1fT3NzMPffcg9PpnMKSnTmmabJixQq+8pWvALB06VJ2797N97//fW699dYpLt3UuOuuu3jLW95CXV3dGT3vOd9iUVFRgcViOa5X6+joKDU1NVNUqqlz5DVPl/r41Kc+xZ/+9CeefvppGhoaJtbX1NRQKBSIxWLH7H8+1oPNZmPmzJksX76cO++8k8WLF/Otb31rWtTB1q1bGRsbY9myZWiahqZprF+/nm9/+9tomkZ1dfV5XwcnEggEmDVrFgcPHpwW7wOA2tpa5s2bd8y6uXPnTtwSmm6fjb29vTzxxBP85V/+5cS6M/VeOOcDC5vNxvLly3nyyScn1pmmyZNPPsmaNWumsGRTo7W1lZqammPqI5FI8NJLL51X9SGE4FOf+hT33XcfTz31FK2trcdsX758OVar9Zh6OHDgAH19fedVPZyIaZrk8/lpUQdXXnklu3bt4uWXX55YVqxYwQc+8IGJ5+d7HZxIKpWiq6uL2traafE+ALjooouOG3Le0dFBc3MzMH0+G4/46U9/SlVVFddff/3EujP2Xpi0bqBT6Le//a2w2+3iZz/7mdi7d6/4q7/6KxEIBMTIyMhUF+20SCaTYvv27WL79u0CEN/4xjfE9u3bRW9vrxBCiK9+9asiEAiIBx54QOzcuVO8/e1vF62trSKbzU5xySfPJz/5SeH3+8UzzzwjhoeHJ5ZMJjOxz2233SaamprEU089JbZs2SLWrFkj1qxZM4Wlnnxf+MIXxPr160V3d7fYuXOn+MIXviAURRGPP/64EGJ61MGrvXJUiBDTow7+7u/+TjzzzDOiu7tbPP/88+Kqq64SFRUVYmxsTAgxPepg06ZNQtM08eUvf1l0dnaKX/3qV8Llcolf/vKXE/tMh89GIUojI5uamsTnP//547adiffCeRFYCCHEd77zHdHU1CRsNptYtWqV2Lhx41QX6bR5+umnBXDccuuttwohSsOq/vmf/1lUV1cLu90urrzySnHgwIGpLfQkO9HrB8RPf/rTiX2y2ay4/fbbRTAYFC6XS7zzne8Uw8PDU1fo0+CjH/2oaG5uFjabTVRWVoorr7xyIqgQYnrUwau9OrCYDnVwyy23iNraWmGz2UR9fb245ZZbxMGDBye2T4c6EEKIBx98UCxYsEDY7XYxZ84c8cMf/vCY7dPhs1EIIR577DEBnPC1nYn3gpw2XZIkSZKkSXPO97GQJEmSJOnsIQMLSZIkSZImjQwsJEmSJEmaNDKwkCRJkiRp0sjAQpIkSZKkSSMDC0mSJEmSJo0MLCRJkiRJmjQysJAkSZIkadLIwEKSJEmSpEkjAwtJkiRJkiaNDCwkSZIkSZo0MrCQJEmSJGnS/P9wVlCaOar+PgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df3.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_pickle('Wiki2_Triple_model_curves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = None\n",
    "for i in train_dataloader:\n",
    "    sample = i['input_ids']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  15.29 M \n",
      "fwd MACs:                                                               288.11 GMACs\n",
      "fwd FLOPs:                                                              576.88 GFLOPS\n",
      "fwd+bwd MACs:                                                           864.32 GMACs\n",
      "fwd+bwd FLOPs:                                                          1.73 TFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "Model(\n",
      "  15.29 M = 100% Params, 288.11 GMACs = 100% MACs, 576.88 GFLOPS = 100% FLOPs\n",
      "  (embedding): Embedding(6.43 M = 42.08% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 50258, 128)\n",
      "  (pe): PositionalEncoding(\n",
      "    0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs\n",
      "    (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-15): 16 x Block(\n",
      "      148.1 K = 0.97% Params, 4.83 GMACs = 1.68% MACs, 9.71 GFLOPS = 1.68% FLOPs\n",
      "      (multihead): MultiHead(\n",
      "        131.07 K = 0.86% Params, 4.29 GMACs = 1.49% MACs, 8.59 GFLOPS = 1.49% FLOPs\n",
      "        (head1): AttentionHead(\n",
      "          49.15 K = 0.32% Params, 1.61 GMACs = 0.56% MACs, 3.22 GFLOPS = 0.56% FLOPs\n",
      "          (k): Linear(16.38 K = 0.11% Params, 536.87 MMACs = 0.19% MACs, 1.07 GFLOPS = 0.19% FLOPs, in_features=128, out_features=128, bias=False)\n",
      "          (q): Linear(16.38 K = 0.11% Params, 536.87 MMACs = 0.19% MACs, 1.07 GFLOPS = 0.19% FLOPs, in_features=128, out_features=128, bias=False)\n",
      "          (v): Linear(16.38 K = 0.11% Params, 536.87 MMACs = 0.19% MACs, 1.07 GFLOPS = 0.19% FLOPs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (head2): AttentionHead(\n",
      "          49.15 K = 0.32% Params, 1.61 GMACs = 0.56% MACs, 3.22 GFLOPS = 0.56% FLOPs\n",
      "          (k): Linear(16.38 K = 0.11% Params, 536.87 MMACs = 0.19% MACs, 1.07 GFLOPS = 0.19% FLOPs, in_features=128, out_features=128, bias=False)\n",
      "          (q): Linear(16.38 K = 0.11% Params, 536.87 MMACs = 0.19% MACs, 1.07 GFLOPS = 0.19% FLOPs, in_features=128, out_features=128, bias=False)\n",
      "          (v): Linear(16.38 K = 0.11% Params, 536.87 MMACs = 0.19% MACs, 1.07 GFLOPS = 0.19% FLOPs, in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (mh_lin): Linear(32.77 K = 0.21% Params, 1.07 GMACs = 0.37% MACs, 2.15 GFLOPS = 0.37% FLOPs, in_features=256, out_features=128, bias=False)\n",
      "        (drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "      )\n",
      "      (l_norm_1): LayerNorm(256 = 0% Params, 0 MACs = 0% MACs, 20.97 MFLOPS = 0% FLOPs, (128,), eps=1e-05, elementwise_affine=True)\n",
      "      (l_norm_2): LayerNorm(256 = 0% Params, 0 MACs = 0% MACs, 20.97 MFLOPS = 0% FLOPs, (128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): Linear(16.51 K = 0.11% Params, 536.87 MMACs = 0.19% MACs, 1.07 GFLOPS = 0.19% FLOPs, in_features=128, out_features=128, bias=True)\n",
      "      (drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (f_lin): Linear(6.48 M = 42.41% Params, 210.8 GMACs = 73.17% MACs, 421.59 GFLOPS = 73.08% FLOPs, in_features=128, out_features=50258, bias=True)\n",
      "  (drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('576.88 GFLOPS', '288.11 GMACs', '15.29 M')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Used to calculate the number of FLOPs in a model \n",
    "# calculate_flops(model=Model(16).to(device), kwargs={'inp':sample})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the split points\n",
    "train_split_1_4 = len(train_dataset) // 4\n",
    "train_split_2_4 = 2 * len(train_dataset) // 4\n",
    "train_split_3_4 = 3 * len(train_dataset) // 4\n",
    "\n",
    "val_split_1_4 = len(val_dataset) // 4\n",
    "val_split_2_4 = 2 * len(val_dataset) // 4\n",
    "val_split_3_4 = 3 * len(val_dataset) // 4\n",
    "\n",
    "# Create subsets for each quarter\n",
    "train_dataset_1_4 = Subset(train_dataset, range(train_split_1_4))\n",
    "train_dataset_2_4 = Subset(train_dataset, range(train_split_1_4, train_split_2_4))\n",
    "train_dataset_3_4 = Subset(train_dataset, range(train_split_2_4, train_split_3_4))\n",
    "train_dataset_4_4 = Subset(train_dataset, range(train_split_3_4, len(train_dataset)))\n",
    "\n",
    "val_dataset_1_4 = Subset(val_dataset, range(val_split_1_4))\n",
    "val_dataset_2_4 = Subset(val_dataset, range(val_split_1_4, val_split_2_4))\n",
    "val_dataset_3_4 = Subset(val_dataset, range(val_split_2_4, val_split_3_4))\n",
    "val_dataset_4_4 = Subset(val_dataset, range(val_split_3_4, len(val_dataset)))\n",
    "\n",
    "# Create DataLoaders for each quarter\n",
    "train_dataloader_1_4 = DataLoader(train_dataset_1_4, shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "train_dataloader_2_4 = DataLoader(train_dataset_2_4, shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "train_dataloader_3_4 = DataLoader(train_dataset_3_4, shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "train_dataloader_4_4 = DataLoader(train_dataset_4_4, shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "val_dataloader_1_4 = DataLoader(val_dataset_1_4, shuffle=False, batch_size=batch_size, collate_fn=data_collator)\n",
    "val_dataloader_2_4 = DataLoader(val_dataset_2_4, shuffle=False, batch_size=batch_size, collate_fn=data_collator)\n",
    "val_dataloader_3_4 = DataLoader(val_dataset_3_4, shuffle=False, batch_size=batch_size, collate_fn=data_collator)\n",
    "val_dataloader_4_4 = DataLoader(val_dataset_4_4, shuffle=False, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "# Convert to lists if necessary\n",
    "train_dataloader_1_4 = list(train_dataloader_1_4)\n",
    "train_dataloader_2_4 = list(train_dataloader_2_4)\n",
    "train_dataloader_3_4 = list(train_dataloader_3_4)\n",
    "train_dataloader_4_4 = list(train_dataloader_4_4)\n",
    "\n",
    "val_dataloader_1_4 = list(val_dataloader_1_4)\n",
    "val_dataloader_2_4 = list(val_dataloader_2_4)\n",
    "val_dataloader_3_4 = list(val_dataloader_3_4)\n",
    "val_dataloader_4_4 = list(val_dataloader_4_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_1 = Model(4).to(device)\n",
    "optimizer4_1 = torch.optim.Adam(model4_1.parameters(), lr=learning_rate)\n",
    "\n",
    "model4_2 = Model(4).to(device)\n",
    "optimizer4_2 = torch.optim.Adam(model4_2.parameters(), lr=learning_rate)\n",
    "\n",
    "model4_3 = Model(4).to(device)\n",
    "optimizer4_3 = torch.optim.Adam(model4_3.parameters(), lr=learning_rate)\n",
    "\n",
    "model4_4 = Model(4).to(device)\n",
    "optimizer4_4 = torch.optim.Adam(model4_4.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 10.5908, Val Loss: 9.9350\n",
      "Epoch 2, Train Loss: 9.8026, Val Loss: 9.1507\n",
      "Epoch 3, Train Loss: 9.1330, Val Loss: 8.3853\n",
      "Epoch 4, Train Loss: 8.4546, Val Loss: 7.7329\n",
      "Epoch 5, Train Loss: 7.8519, Val Loss: 7.0795\n",
      "Epoch 6, Train Loss: 7.3061, Val Loss: 6.5023\n",
      "Epoch 7, Train Loss: 6.8216, Val Loss: 6.0250\n",
      "Epoch 8, Train Loss: 6.3428, Val Loss: 5.6322\n",
      "Epoch 9, Train Loss: 5.9643, Val Loss: 5.2235\n",
      "Epoch 10, Train Loss: 5.5819, Val Loss: 4.8601\n",
      "Epoch 11, Train Loss: 5.2639, Val Loss: 4.5819\n",
      "Epoch 12, Train Loss: 4.9883, Val Loss: 4.2987\n",
      "Epoch 13, Train Loss: 4.7430, Val Loss: 4.0892\n",
      "Epoch 14, Train Loss: 4.5220, Val Loss: 3.8903\n",
      "Epoch 15, Train Loss: 4.3297, Val Loss: 3.7130\n",
      "Epoch 16, Train Loss: 4.1284, Val Loss: 3.5524\n",
      "Epoch 17, Train Loss: 3.9715, Val Loss: 3.3776\n",
      "Epoch 18, Train Loss: 3.7925, Val Loss: 3.2168\n",
      "Epoch 19, Train Loss: 3.6224, Val Loss: 3.1344\n",
      "Epoch 20, Train Loss: 3.5283, Val Loss: 3.0080\n",
      "Epoch 21, Train Loss: 3.3781, Val Loss: 2.9163\n",
      "Epoch 22, Train Loss: 3.2485, Val Loss: 2.7584\n",
      "Epoch 23, Train Loss: 3.1408, Val Loss: 2.6540\n",
      "Epoch 24, Train Loss: 2.9877, Val Loss: 2.5537\n",
      "Epoch 25, Train Loss: 2.8880, Val Loss: 2.4560\n",
      "Epoch 26, Train Loss: 2.7589, Val Loss: 2.3867\n",
      "Epoch 27, Train Loss: 2.6334, Val Loss: 2.2989\n",
      "Epoch 28, Train Loss: 2.5261, Val Loss: 2.2154\n",
      "Epoch 29, Train Loss: 2.4567, Val Loss: 2.0885\n",
      "Epoch 30, Train Loss: 2.3778, Val Loss: 2.0580\n",
      "Epoch 1, Train Loss: 10.6974, Val Loss: 10.0654\n",
      "Epoch 2, Train Loss: 9.9067, Val Loss: 9.2915\n",
      "Epoch 3, Train Loss: 9.2582, Val Loss: 8.5968\n",
      "Epoch 4, Train Loss: 8.6354, Val Loss: 7.8877\n",
      "Epoch 5, Train Loss: 8.0253, Val Loss: 7.1941\n",
      "Epoch 6, Train Loss: 7.4490, Val Loss: 6.6867\n",
      "Epoch 7, Train Loss: 6.9483, Val Loss: 6.1321\n",
      "Epoch 8, Train Loss: 6.4780, Val Loss: 5.7569\n",
      "Epoch 9, Train Loss: 6.0726, Val Loss: 5.2882\n",
      "Epoch 10, Train Loss: 5.7080, Val Loss: 4.9833\n",
      "Epoch 11, Train Loss: 5.4017, Val Loss: 4.6813\n",
      "Epoch 12, Train Loss: 5.1193, Val Loss: 4.4191\n",
      "Epoch 13, Train Loss: 4.8627, Val Loss: 4.1525\n",
      "Epoch 14, Train Loss: 4.6024, Val Loss: 3.9510\n",
      "Epoch 15, Train Loss: 4.3967, Val Loss: 3.7687\n",
      "Epoch 16, Train Loss: 4.1838, Val Loss: 3.5309\n",
      "Epoch 17, Train Loss: 4.0406, Val Loss: 3.4572\n",
      "Epoch 18, Train Loss: 3.8131, Val Loss: 3.2819\n",
      "Epoch 19, Train Loss: 3.7036, Val Loss: 3.2569\n",
      "Epoch 20, Train Loss: 3.5712, Val Loss: 2.9884\n",
      "Epoch 21, Train Loss: 3.4169, Val Loss: 2.9721\n",
      "Epoch 22, Train Loss: 3.2784, Val Loss: 2.8248\n",
      "Epoch 23, Train Loss: 3.1453, Val Loss: 2.7531\n",
      "Epoch 24, Train Loss: 3.0094, Val Loss: 2.6861\n",
      "Epoch 25, Train Loss: 2.9309, Val Loss: 2.5552\n",
      "Epoch 26, Train Loss: 2.8200, Val Loss: 2.4240\n",
      "Epoch 27, Train Loss: 2.6538, Val Loss: 2.3690\n",
      "Epoch 28, Train Loss: 2.5499, Val Loss: 2.3164\n",
      "Epoch 29, Train Loss: 2.4757, Val Loss: 2.2061\n",
      "Epoch 30, Train Loss: 2.3696, Val Loss: 2.1983\n",
      "Epoch 1, Train Loss: 10.6996, Val Loss: 10.0781\n",
      "Epoch 2, Train Loss: 9.9129, Val Loss: 9.2974\n",
      "Epoch 3, Train Loss: 9.2412, Val Loss: 8.5554\n",
      "Epoch 4, Train Loss: 8.5904, Val Loss: 7.8678\n",
      "Epoch 5, Train Loss: 7.9994, Val Loss: 7.2331\n",
      "Epoch 6, Train Loss: 7.4406, Val Loss: 6.6728\n",
      "Epoch 7, Train Loss: 6.9216, Val Loss: 6.1764\n",
      "Epoch 8, Train Loss: 6.4711, Val Loss: 5.7514\n",
      "Epoch 9, Train Loss: 6.0766, Val Loss: 5.3673\n",
      "Epoch 10, Train Loss: 5.7413, Val Loss: 5.0317\n",
      "Epoch 11, Train Loss: 5.4012, Val Loss: 4.7349\n",
      "Epoch 12, Train Loss: 5.1190, Val Loss: 4.4762\n",
      "Epoch 13, Train Loss: 4.8451, Val Loss: 4.2404\n",
      "Epoch 14, Train Loss: 4.6420, Val Loss: 4.0351\n",
      "Epoch 15, Train Loss: 4.4209, Val Loss: 3.8527\n",
      "Epoch 16, Train Loss: 4.2413, Val Loss: 3.6845\n",
      "Epoch 17, Train Loss: 4.0631, Val Loss: 3.5259\n",
      "Epoch 18, Train Loss: 3.8933, Val Loss: 3.3882\n",
      "Epoch 19, Train Loss: 3.7318, Val Loss: 3.2319\n",
      "Epoch 20, Train Loss: 3.5822, Val Loss: 3.1103\n",
      "Epoch 21, Train Loss: 3.4505, Val Loss: 2.9837\n",
      "Epoch 22, Train Loss: 3.3069, Val Loss: 2.8745\n",
      "Epoch 23, Train Loss: 3.1877, Val Loss: 2.7473\n",
      "Epoch 24, Train Loss: 3.0467, Val Loss: 2.6510\n",
      "Epoch 25, Train Loss: 2.9233, Val Loss: 2.5233\n",
      "Epoch 26, Train Loss: 2.8061, Val Loss: 2.4568\n",
      "Epoch 27, Train Loss: 2.7176, Val Loss: 2.3514\n",
      "Epoch 28, Train Loss: 2.6036, Val Loss: 2.2623\n",
      "Epoch 29, Train Loss: 2.4936, Val Loss: 2.1759\n",
      "Epoch 30, Train Loss: 2.4154, Val Loss: 2.0840\n",
      "Epoch 1, Train Loss: 10.6132, Val Loss: 9.9240\n",
      "Epoch 2, Train Loss: 9.8182, Val Loss: 9.1859\n",
      "Epoch 3, Train Loss: 9.1491, Val Loss: 8.4435\n",
      "Epoch 4, Train Loss: 8.5106, Val Loss: 7.7785\n",
      "Epoch 5, Train Loss: 7.9152, Val Loss: 7.1871\n",
      "Epoch 6, Train Loss: 7.3831, Val Loss: 6.5329\n",
      "Epoch 7, Train Loss: 6.8489, Val Loss: 6.0643\n",
      "Epoch 8, Train Loss: 6.4385, Val Loss: 5.5876\n",
      "Epoch 9, Train Loss: 6.0390, Val Loss: 5.3000\n",
      "Epoch 10, Train Loss: 5.6813, Val Loss: 4.9237\n",
      "Epoch 11, Train Loss: 5.4225, Val Loss: 4.5955\n",
      "Epoch 12, Train Loss: 5.0631, Val Loss: 4.3705\n",
      "Epoch 13, Train Loss: 4.8310, Val Loss: 4.1421\n",
      "Epoch 14, Train Loss: 4.6197, Val Loss: 3.9157\n",
      "Epoch 15, Train Loss: 4.3358, Val Loss: 3.7870\n",
      "Epoch 16, Train Loss: 4.2244, Val Loss: 3.5780\n",
      "Epoch 17, Train Loss: 4.0301, Val Loss: 3.4304\n",
      "Epoch 18, Train Loss: 3.8830, Val Loss: 3.2943\n",
      "Epoch 19, Train Loss: 3.7341, Val Loss: 3.1716\n",
      "Epoch 20, Train Loss: 3.5725, Val Loss: 3.0631\n",
      "Epoch 21, Train Loss: 3.4280, Val Loss: 2.9375\n",
      "Epoch 22, Train Loss: 3.2834, Val Loss: 2.8212\n",
      "Epoch 23, Train Loss: 3.1739, Val Loss: 2.7178\n",
      "Epoch 24, Train Loss: 3.0339, Val Loss: 2.6317\n",
      "Epoch 25, Train Loss: 2.9261, Val Loss: 2.5408\n",
      "Epoch 26, Train Loss: 2.8243, Val Loss: 2.4457\n",
      "Epoch 27, Train Loss: 2.6974, Val Loss: 2.3584\n",
      "Epoch 28, Train Loss: 2.5998, Val Loss: 2.2711\n",
      "Epoch 29, Train Loss: 2.4789, Val Loss: 2.1898\n",
      "Epoch 30, Train Loss: 2.3880, Val Loss: 2.1185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.924049377441406,\n",
       " 9.18587131500244,\n",
       " 8.443549919128419,\n",
       " 7.778538799285888,\n",
       " 7.187076473236084,\n",
       " 6.532893085479737,\n",
       " 6.064302921295166,\n",
       " 5.587611484527588,\n",
       " 5.300025558471679,\n",
       " 4.923692512512207,\n",
       " 4.5954833984375,\n",
       " 4.370462894439697,\n",
       " 4.142061805725097,\n",
       " 3.915741300582886,\n",
       " 3.787037801742554,\n",
       " 3.578040599822998,\n",
       " 3.4303598403930664,\n",
       " 3.2942681312561035,\n",
       " 3.1716026782989504,\n",
       " 3.063149166107178,\n",
       " 2.9374897956848143,\n",
       " 2.8211555957794188,\n",
       " 2.717837333679199,\n",
       " 2.631736326217651,\n",
       " 2.540820598602295,\n",
       " 2.445720100402832,\n",
       " 2.3583571910858154,\n",
       " 2.2711021900177,\n",
       " 2.1897669792175294,\n",
       " 2.1185183048248293]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model4_1, optimizer4_1, 30, train_dataloader_1_4, val_dataloader_1_4)\n",
    "train(model4_2, optimizer4_2, 30, train_dataloader_2_4, val_dataloader_2_4)\n",
    "train(model4_3, optimizer4_3, 30, train_dataloader_3_4, val_dataloader_3_4)\n",
    "train(model4_4, optimizer4_4, 30, train_dataloader_4_4, val_dataloader_4_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "Four_Stacked_Model = StackedModel([model4_1, model4_2, model4_3, model4_4], num_models = 4)\n",
    "optimizer_four_stacked = torch.optim.Adam(Four_Stacked_Model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 7.1100, Val Loss: 5.0735\n",
      "Epoch 2, Train Loss: 5.6716, Val Loss: 4.5574\n",
      "Epoch 3, Train Loss: 5.2651, Val Loss: 4.3211\n",
      "Epoch 4, Train Loss: 4.9897, Val Loss: 3.9756\n",
      "Epoch 5, Train Loss: 4.7751, Val Loss: 3.8593\n",
      "Epoch 6, Train Loss: 4.5752, Val Loss: 3.7093\n",
      "Epoch 7, Train Loss: 4.4626, Val Loss: 3.4900\n",
      "Epoch 8, Train Loss: 4.3196, Val Loss: 3.4496\n",
      "Epoch 9, Train Loss: 4.1883, Val Loss: 3.3128\n",
      "Epoch 10, Train Loss: 4.0466, Val Loss: 3.1545\n",
      "Epoch 11, Train Loss: 3.9678, Val Loss: 3.0342\n",
      "Epoch 12, Train Loss: 3.8263, Val Loss: 3.0375\n",
      "Epoch 13, Train Loss: 3.7317, Val Loss: 2.8469\n",
      "Epoch 14, Train Loss: 3.6442, Val Loss: 2.7772\n",
      "Epoch 15, Train Loss: 3.5362, Val Loss: 2.6870\n",
      "Epoch 16, Train Loss: 3.4487, Val Loss: 2.5718\n",
      "Epoch 17, Train Loss: 3.3569, Val Loss: 2.5326\n",
      "Epoch 18, Train Loss: 3.2490, Val Loss: 2.3826\n",
      "Epoch 19, Train Loss: 3.1869, Val Loss: 2.3580\n",
      "Epoch 20, Train Loss: 3.0777, Val Loss: 2.3120\n",
      "Epoch 21, Train Loss: 3.0273, Val Loss: 2.2135\n",
      "Epoch 22, Train Loss: 2.9449, Val Loss: 2.1418\n",
      "Epoch 23, Train Loss: 2.8802, Val Loss: 2.0358\n",
      "Epoch 24, Train Loss: 2.7894, Val Loss: 2.0837\n",
      "Epoch 25, Train Loss: 2.7186, Val Loss: 1.9531\n",
      "Epoch 26, Train Loss: 2.6526, Val Loss: 1.9369\n",
      "Epoch 27, Train Loss: 2.5892, Val Loss: 1.8563\n",
      "Epoch 28, Train Loss: 2.5500, Val Loss: 1.8136\n",
      "Epoch 29, Train Loss: 2.4669, Val Loss: 1.7792\n",
      "Epoch 30, Train Loss: 2.4187, Val Loss: 1.7240\n",
      "Epoch 31, Train Loss: 2.3380, Val Loss: 1.6673\n",
      "Epoch 32, Train Loss: 2.2855, Val Loss: 1.6152\n",
      "Epoch 33, Train Loss: 2.2227, Val Loss: 1.6588\n",
      "Epoch 34, Train Loss: 2.1667, Val Loss: 1.5742\n",
      "Epoch 35, Train Loss: 2.1095, Val Loss: 1.4611\n",
      "Epoch 36, Train Loss: 2.0575, Val Loss: 1.4514\n",
      "Epoch 37, Train Loss: 2.0140, Val Loss: 1.4154\n",
      "Epoch 38, Train Loss: 1.9678, Val Loss: 1.3618\n",
      "Epoch 39, Train Loss: 1.9363, Val Loss: 1.3474\n",
      "Epoch 40, Train Loss: 1.8748, Val Loss: 1.3381\n",
      "Epoch 41, Train Loss: 1.8517, Val Loss: 1.3587\n",
      "Epoch 42, Train Loss: 1.8049, Val Loss: 1.2147\n",
      "Epoch 43, Train Loss: 1.7498, Val Loss: 1.1876\n",
      "Epoch 44, Train Loss: 1.7049, Val Loss: 1.1868\n",
      "Epoch 45, Train Loss: 1.6577, Val Loss: 1.1530\n",
      "Epoch 46, Train Loss: 1.6205, Val Loss: 1.0553\n",
      "Epoch 47, Train Loss: 1.5662, Val Loss: 1.1043\n",
      "Epoch 48, Train Loss: 1.5466, Val Loss: 1.0201\n",
      "Epoch 49, Train Loss: 1.5162, Val Loss: 0.9960\n",
      "Epoch 50, Train Loss: 1.4864, Val Loss: 1.0068\n",
      "Epoch 51, Train Loss: 1.4537, Val Loss: 0.9936\n",
      "Epoch 52, Train Loss: 1.4221, Val Loss: 0.9170\n",
      "Epoch 53, Train Loss: 1.3757, Val Loss: 0.9338\n",
      "Epoch 54, Train Loss: 1.3426, Val Loss: 0.8890\n",
      "Epoch 55, Train Loss: 1.3027, Val Loss: 0.8980\n",
      "Epoch 56, Train Loss: 1.2751, Val Loss: 0.8264\n",
      "Epoch 57, Train Loss: 1.2462, Val Loss: 0.8719\n",
      "Epoch 58, Train Loss: 1.2190, Val Loss: 0.8295\n",
      "Epoch 59, Train Loss: 1.1967, Val Loss: 0.7831\n",
      "Epoch 60, Train Loss: 1.1672, Val Loss: 0.7918\n",
      "Epoch 61, Train Loss: 1.1338, Val Loss: 0.7667\n",
      "Epoch 62, Train Loss: 1.1033, Val Loss: 0.7507\n",
      "Epoch 63, Train Loss: 1.0695, Val Loss: 0.7346\n",
      "Epoch 64, Train Loss: 1.0480, Val Loss: 0.6714\n",
      "Epoch 65, Train Loss: 1.0492, Val Loss: 0.6643\n",
      "Epoch 66, Train Loss: 1.0139, Val Loss: 0.6547\n",
      "Epoch 67, Train Loss: 0.9848, Val Loss: 0.6479\n",
      "Epoch 68, Train Loss: 0.9457, Val Loss: 0.5917\n",
      "Epoch 69, Train Loss: 0.9538, Val Loss: 0.6136\n",
      "Epoch 70, Train Loss: 0.9409, Val Loss: 0.5888\n"
     ]
    }
   ],
   "source": [
    "four_stack_curve = train(Four_Stacked_Model, optimizer_four_stacked, 70, train_dataloader, val_dataloader)\n",
    "torch.save(Four_Stacked_Model.state_dict(), 'models/Four_Stacked_Model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "Four_Stacked_Avg = StackedModelAVG([model4_1, model4_2, model4_3, model4_4], num_models = 4)\n",
    "optimizer_four_stacked_avg = torch.optim.Adam(Four_Stacked_Avg.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 7.6048, Val Loss: 6.3323\n",
      "Epoch 2, Train Loss: 6.6631, Val Loss: 5.6834\n",
      "Epoch 3, Train Loss: 6.1597, Val Loss: 5.1503\n",
      "Epoch 4, Train Loss: 5.7733, Val Loss: 4.7387\n",
      "Epoch 5, Train Loss: 5.4063, Val Loss: 4.4228\n",
      "Epoch 6, Train Loss: 5.0934, Val Loss: 4.0743\n",
      "Epoch 7, Train Loss: 4.8245, Val Loss: 3.8500\n",
      "Epoch 8, Train Loss: 4.5716, Val Loss: 3.6018\n",
      "Epoch 9, Train Loss: 4.3510, Val Loss: 3.3827\n",
      "Epoch 10, Train Loss: 4.1556, Val Loss: 3.2169\n",
      "Epoch 11, Train Loss: 4.0106, Val Loss: 3.1304\n",
      "Epoch 12, Train Loss: 3.8437, Val Loss: 2.9357\n",
      "Epoch 13, Train Loss: 3.6938, Val Loss: 2.7520\n",
      "Epoch 14, Train Loss: 3.5787, Val Loss: 2.6711\n",
      "Epoch 15, Train Loss: 3.4485, Val Loss: 2.5698\n",
      "Epoch 16, Train Loss: 3.3346, Val Loss: 2.4463\n",
      "Epoch 17, Train Loss: 3.2361, Val Loss: 2.3811\n",
      "Epoch 18, Train Loss: 3.1316, Val Loss: 2.2859\n",
      "Epoch 19, Train Loss: 3.0291, Val Loss: 2.2059\n",
      "Epoch 20, Train Loss: 2.9244, Val Loss: 2.0887\n",
      "Epoch 21, Train Loss: 2.8421, Val Loss: 2.1021\n",
      "Epoch 22, Train Loss: 2.7589, Val Loss: 2.0220\n",
      "Epoch 23, Train Loss: 2.6550, Val Loss: 1.9221\n",
      "Epoch 24, Train Loss: 2.5762, Val Loss: 1.8406\n",
      "Epoch 25, Train Loss: 2.5028, Val Loss: 1.7767\n",
      "Epoch 26, Train Loss: 2.4351, Val Loss: 1.7186\n",
      "Epoch 27, Train Loss: 2.3395, Val Loss: 1.6809\n",
      "Epoch 28, Train Loss: 2.2949, Val Loss: 1.5744\n",
      "Epoch 29, Train Loss: 2.2127, Val Loss: 1.5676\n",
      "Epoch 30, Train Loss: 2.1268, Val Loss: 1.4876\n",
      "Epoch 31, Train Loss: 2.0898, Val Loss: 1.4301\n",
      "Epoch 32, Train Loss: 2.0338, Val Loss: 1.4290\n",
      "Epoch 33, Train Loss: 1.9655, Val Loss: 1.4072\n",
      "Epoch 34, Train Loss: 1.9063, Val Loss: 1.3426\n",
      "Epoch 35, Train Loss: 1.8685, Val Loss: 1.2466\n",
      "Epoch 36, Train Loss: 1.8050, Val Loss: 1.2252\n",
      "Epoch 37, Train Loss: 1.7327, Val Loss: 1.2056\n",
      "Epoch 38, Train Loss: 1.7039, Val Loss: 1.2580\n",
      "Epoch 39, Train Loss: 1.6399, Val Loss: 1.1374\n",
      "Epoch 40, Train Loss: 1.6221, Val Loss: 1.0753\n",
      "Epoch 41, Train Loss: 1.5847, Val Loss: 1.0587\n",
      "Epoch 42, Train Loss: 1.5074, Val Loss: 1.0442\n",
      "Epoch 43, Train Loss: 1.4762, Val Loss: 1.0311\n",
      "Epoch 44, Train Loss: 1.4439, Val Loss: 1.0134\n",
      "Epoch 45, Train Loss: 1.4051, Val Loss: 0.9710\n",
      "Epoch 46, Train Loss: 1.3523, Val Loss: 0.9154\n",
      "Epoch 47, Train Loss: 1.3332, Val Loss: 0.9130\n",
      "Epoch 48, Train Loss: 1.3018, Val Loss: 0.8554\n",
      "Epoch 49, Train Loss: 1.2717, Val Loss: 0.8818\n",
      "Epoch 50, Train Loss: 1.2207, Val Loss: 0.8078\n",
      "Epoch 51, Train Loss: 1.2033, Val Loss: 0.8234\n",
      "Epoch 52, Train Loss: 1.1689, Val Loss: 0.7952\n",
      "Epoch 53, Train Loss: 1.1276, Val Loss: 0.7660\n",
      "Epoch 54, Train Loss: 1.1040, Val Loss: 0.7558\n",
      "Epoch 55, Train Loss: 1.0901, Val Loss: 0.7034\n",
      "Epoch 56, Train Loss: 1.0475, Val Loss: 0.7210\n",
      "Epoch 57, Train Loss: 1.0104, Val Loss: 0.6917\n",
      "Epoch 58, Train Loss: 0.9954, Val Loss: 0.6988\n",
      "Epoch 59, Train Loss: 0.9649, Val Loss: 0.6532\n",
      "Epoch 60, Train Loss: 0.9430, Val Loss: 0.6676\n",
      "Epoch 61, Train Loss: 0.9210, Val Loss: 0.6084\n",
      "Epoch 62, Train Loss: 0.8965, Val Loss: 0.6683\n",
      "Epoch 63, Train Loss: 0.8727, Val Loss: 0.6070\n",
      "Epoch 64, Train Loss: 0.8669, Val Loss: 0.5785\n",
      "Epoch 65, Train Loss: 0.8274, Val Loss: 0.5570\n",
      "Epoch 66, Train Loss: 0.8148, Val Loss: 0.5661\n",
      "Epoch 67, Train Loss: 0.7918, Val Loss: 0.5046\n",
      "Epoch 68, Train Loss: 0.7776, Val Loss: 0.5388\n",
      "Epoch 69, Train Loss: 0.7463, Val Loss: 0.5392\n",
      "Epoch 70, Train Loss: 0.7620, Val Loss: 0.4738\n"
     ]
    }
   ],
   "source": [
    "four_stack_avg_curve = train(Four_Stacked_Avg, optimizer_four_stacked_avg, 70, train_dataloader, val_dataloader)\n",
    "torch.save(Four_Stacked_Avg.state_dict(), 'models/Four_Stacked_Avg.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "Four_Stacked_Same = StackedModel([model4_1, model4_2, model4_3, model4_4], num_models = 4, same=True)\n",
    "optimizer_four_stacked_same = torch.optim.Adam(Four_Stacked_Same.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 5.5611, Val Loss: 3.5305\n",
      "Epoch 2, Train Loss: 4.1847, Val Loss: 2.9212\n",
      "Epoch 3, Train Loss: 3.7272, Val Loss: 2.6730\n",
      "Epoch 4, Train Loss: 3.4409, Val Loss: 2.4370\n",
      "Epoch 5, Train Loss: 3.2165, Val Loss: 2.2999\n",
      "Epoch 6, Train Loss: 3.0689, Val Loss: 2.1780\n",
      "Epoch 7, Train Loss: 2.9234, Val Loss: 2.0302\n",
      "Epoch 8, Train Loss: 2.7589, Val Loss: 1.9439\n",
      "Epoch 9, Train Loss: 2.6594, Val Loss: 1.8833\n",
      "Epoch 10, Train Loss: 2.5570, Val Loss: 1.7356\n",
      "Epoch 11, Train Loss: 2.4267, Val Loss: 1.6764\n",
      "Epoch 12, Train Loss: 2.3599, Val Loss: 1.6613\n",
      "Epoch 13, Train Loss: 2.2738, Val Loss: 1.5747\n",
      "Epoch 14, Train Loss: 2.1786, Val Loss: 1.5656\n",
      "Epoch 15, Train Loss: 2.1160, Val Loss: 1.4824\n",
      "Epoch 16, Train Loss: 2.0396, Val Loss: 1.3941\n",
      "Epoch 17, Train Loss: 1.9518, Val Loss: 1.3526\n",
      "Epoch 18, Train Loss: 1.8861, Val Loss: 1.3619\n",
      "Epoch 19, Train Loss: 1.8584, Val Loss: 1.3098\n",
      "Epoch 20, Train Loss: 1.7803, Val Loss: 1.2406\n",
      "Epoch 21, Train Loss: 1.7412, Val Loss: 1.2302\n",
      "Epoch 22, Train Loss: 1.6905, Val Loss: 1.1457\n",
      "Epoch 23, Train Loss: 1.6221, Val Loss: 1.1551\n",
      "Epoch 24, Train Loss: 1.6053, Val Loss: 1.1717\n",
      "Epoch 25, Train Loss: 1.5316, Val Loss: 1.0895\n",
      "Epoch 26, Train Loss: 1.4961, Val Loss: 1.0513\n",
      "Epoch 27, Train Loss: 1.4434, Val Loss: 0.9822\n",
      "Epoch 28, Train Loss: 1.4278, Val Loss: 1.0282\n",
      "Epoch 29, Train Loss: 1.3917, Val Loss: 0.9041\n",
      "Epoch 30, Train Loss: 1.3413, Val Loss: 0.9348\n",
      "Epoch 31, Train Loss: 1.3112, Val Loss: 0.9409\n",
      "Epoch 32, Train Loss: 1.2664, Val Loss: 0.9216\n",
      "Epoch 33, Train Loss: 1.2306, Val Loss: 0.8403\n",
      "Epoch 34, Train Loss: 1.1942, Val Loss: 0.8408\n",
      "Epoch 35, Train Loss: 1.1735, Val Loss: 0.8139\n",
      "Epoch 36, Train Loss: 1.1445, Val Loss: 0.7961\n",
      "Epoch 37, Train Loss: 1.0937, Val Loss: 0.7757\n",
      "Epoch 38, Train Loss: 1.0864, Val Loss: 0.8097\n",
      "Epoch 39, Train Loss: 1.0466, Val Loss: 0.7001\n",
      "Epoch 40, Train Loss: 1.0100, Val Loss: 0.7192\n",
      "Epoch 41, Train Loss: 1.0027, Val Loss: 0.7109\n",
      "Epoch 42, Train Loss: 0.9718, Val Loss: 0.6639\n",
      "Epoch 43, Train Loss: 0.9467, Val Loss: 0.6763\n",
      "Epoch 44, Train Loss: 0.9293, Val Loss: 0.6144\n",
      "Epoch 45, Train Loss: 0.8904, Val Loss: 0.6733\n",
      "Epoch 46, Train Loss: 0.8737, Val Loss: 0.6548\n",
      "Epoch 47, Train Loss: 0.8559, Val Loss: 0.6099\n",
      "Epoch 48, Train Loss: 0.8466, Val Loss: 0.5999\n",
      "Epoch 49, Train Loss: 0.8315, Val Loss: 0.5517\n",
      "Epoch 50, Train Loss: 0.8061, Val Loss: 0.5293\n",
      "Epoch 51, Train Loss: 0.7955, Val Loss: 0.5355\n",
      "Epoch 52, Train Loss: 0.7452, Val Loss: 0.5531\n",
      "Epoch 53, Train Loss: 0.7493, Val Loss: 0.5434\n",
      "Epoch 54, Train Loss: 0.7077, Val Loss: 0.5077\n",
      "Epoch 55, Train Loss: 0.7062, Val Loss: 0.5164\n",
      "Epoch 56, Train Loss: 0.6739, Val Loss: 0.5365\n",
      "Epoch 57, Train Loss: 0.6666, Val Loss: 0.4676\n",
      "Epoch 58, Train Loss: 0.6780, Val Loss: 0.4604\n",
      "Epoch 59, Train Loss: 0.6425, Val Loss: 0.4628\n",
      "Epoch 60, Train Loss: 0.6360, Val Loss: 0.4616\n",
      "Epoch 61, Train Loss: 0.6252, Val Loss: 0.4432\n",
      "Epoch 62, Train Loss: 0.5909, Val Loss: 0.4568\n",
      "Epoch 63, Train Loss: 0.5879, Val Loss: 0.4262\n",
      "Epoch 64, Train Loss: 0.5630, Val Loss: 0.4565\n",
      "Epoch 65, Train Loss: 0.5643, Val Loss: 0.4211\n",
      "Epoch 66, Train Loss: 0.5430, Val Loss: 0.3846\n",
      "Epoch 67, Train Loss: 0.5444, Val Loss: 0.3799\n",
      "Epoch 68, Train Loss: 0.5040, Val Loss: 0.3770\n",
      "Epoch 69, Train Loss: 0.5269, Val Loss: 0.3688\n",
      "Epoch 70, Train Loss: 0.5001, Val Loss: 0.3684\n"
     ]
    }
   ],
   "source": [
    "four_stack_same_curve = train(Four_Stacked_Same, optimizer_four_stacked_same, 70, train_dataloader, val_dataloader)\n",
    "torch.save(Four_Stacked_Same.state_dict(), 'models/Four_Stacked_Same.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "Four_InterWeaved_Model = InterWeavedModel([model4_1, model4_2, model4_3, model4_4], num_models = 4)\n",
    "optimizer_four_interweaved = torch.optim.Adam(Four_InterWeaved_Model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 7.2770, Val Loss: 5.5200\n",
      "Epoch 2, Train Loss: 5.9534, Val Loss: 4.6584\n",
      "Epoch 3, Train Loss: 5.4043, Val Loss: 4.3470\n",
      "Epoch 4, Train Loss: 5.1145, Val Loss: 4.1658\n",
      "Epoch 5, Train Loss: 4.8798, Val Loss: 3.8655\n",
      "Epoch 6, Train Loss: 4.6877, Val Loss: 3.8207\n",
      "Epoch 7, Train Loss: 4.5108, Val Loss: 3.6057\n",
      "Epoch 8, Train Loss: 4.3751, Val Loss: 3.3989\n",
      "Epoch 9, Train Loss: 4.2503, Val Loss: 3.3314\n",
      "Epoch 10, Train Loss: 4.0674, Val Loss: 3.2365\n",
      "Epoch 11, Train Loss: 3.9769, Val Loss: 3.0948\n",
      "Epoch 12, Train Loss: 3.8511, Val Loss: 3.0300\n",
      "Epoch 13, Train Loss: 3.7592, Val Loss: 2.8484\n",
      "Epoch 14, Train Loss: 3.6348, Val Loss: 2.6989\n",
      "Epoch 15, Train Loss: 3.5454, Val Loss: 2.6657\n",
      "Epoch 16, Train Loss: 3.4470, Val Loss: 2.5089\n",
      "Epoch 17, Train Loss: 3.3378, Val Loss: 2.5558\n",
      "Epoch 18, Train Loss: 3.2810, Val Loss: 2.4450\n",
      "Epoch 19, Train Loss: 3.1436, Val Loss: 2.3329\n",
      "Epoch 20, Train Loss: 3.0956, Val Loss: 2.2713\n",
      "Epoch 21, Train Loss: 3.0064, Val Loss: 2.2319\n",
      "Epoch 22, Train Loss: 2.9079, Val Loss: 2.1348\n",
      "Epoch 23, Train Loss: 2.8446, Val Loss: 2.1329\n",
      "Epoch 24, Train Loss: 2.7749, Val Loss: 1.9687\n",
      "Epoch 25, Train Loss: 2.6742, Val Loss: 1.9648\n",
      "Epoch 26, Train Loss: 2.6044, Val Loss: 1.8718\n",
      "Epoch 27, Train Loss: 2.5839, Val Loss: 1.7925\n",
      "Epoch 28, Train Loss: 2.4878, Val Loss: 1.7476\n",
      "Epoch 29, Train Loss: 2.4339, Val Loss: 1.6970\n",
      "Epoch 30, Train Loss: 2.4130, Val Loss: 1.6780\n",
      "Epoch 31, Train Loss: 2.3135, Val Loss: 1.6253\n",
      "Epoch 32, Train Loss: 2.2482, Val Loss: 1.5856\n",
      "Epoch 33, Train Loss: 2.1813, Val Loss: 1.5740\n",
      "Epoch 34, Train Loss: 2.1608, Val Loss: 1.5409\n",
      "Epoch 35, Train Loss: 2.0652, Val Loss: 1.4627\n",
      "Epoch 36, Train Loss: 2.0274, Val Loss: 1.4439\n",
      "Epoch 37, Train Loss: 1.9826, Val Loss: 1.4334\n",
      "Epoch 38, Train Loss: 1.9296, Val Loss: 1.3371\n",
      "Epoch 39, Train Loss: 1.9023, Val Loss: 1.3274\n",
      "Epoch 40, Train Loss: 1.8356, Val Loss: 1.2525\n",
      "Epoch 41, Train Loss: 1.8321, Val Loss: 1.3303\n",
      "Epoch 42, Train Loss: 1.7552, Val Loss: 1.2411\n",
      "Epoch 43, Train Loss: 1.6985, Val Loss: 1.1524\n",
      "Epoch 44, Train Loss: 1.6781, Val Loss: 1.1280\n",
      "Epoch 45, Train Loss: 1.6253, Val Loss: 1.1804\n",
      "Epoch 46, Train Loss: 1.5945, Val Loss: 1.0942\n",
      "Epoch 47, Train Loss: 1.5485, Val Loss: 1.0777\n",
      "Epoch 48, Train Loss: 1.5399, Val Loss: 1.0201\n",
      "Epoch 49, Train Loss: 1.4686, Val Loss: 0.9832\n",
      "Epoch 50, Train Loss: 1.4466, Val Loss: 0.9855\n",
      "Epoch 51, Train Loss: 1.4079, Val Loss: 0.9519\n",
      "Epoch 52, Train Loss: 1.3673, Val Loss: 0.9035\n",
      "Epoch 53, Train Loss: 1.3485, Val Loss: 0.8653\n",
      "Epoch 54, Train Loss: 1.3130, Val Loss: 0.8638\n",
      "Epoch 55, Train Loss: 1.2970, Val Loss: 0.8514\n",
      "Epoch 56, Train Loss: 1.2652, Val Loss: 0.7829\n",
      "Epoch 57, Train Loss: 1.2244, Val Loss: 0.8143\n",
      "Epoch 58, Train Loss: 1.1725, Val Loss: 0.7237\n",
      "Epoch 59, Train Loss: 1.1532, Val Loss: 0.7375\n",
      "Epoch 60, Train Loss: 1.1571, Val Loss: 0.7197\n",
      "Epoch 61, Train Loss: 1.1002, Val Loss: 0.7356\n",
      "Epoch 62, Train Loss: 1.0890, Val Loss: 0.7034\n",
      "Epoch 63, Train Loss: 1.0574, Val Loss: 0.6960\n",
      "Epoch 64, Train Loss: 1.0147, Val Loss: 0.6781\n",
      "Epoch 65, Train Loss: 0.9930, Val Loss: 0.6807\n",
      "Epoch 66, Train Loss: 0.9745, Val Loss: 0.7159\n",
      "Epoch 67, Train Loss: 0.9479, Val Loss: 0.6754\n",
      "Epoch 68, Train Loss: 0.9268, Val Loss: 0.6287\n",
      "Epoch 69, Train Loss: 0.9354, Val Loss: 0.6013\n",
      "Epoch 70, Train Loss: 0.8943, Val Loss: 0.6036\n"
     ]
    }
   ],
   "source": [
    "four_interweaved_curve = train(Four_InterWeaved_Model, optimizer_four_interweaved, 70, train_dataloader, val_dataloader)\n",
    "torch.save(Four_InterWeaved_Model.state_dict(), 'models/Four_InterWeaved_Model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "Four_InterWeaved_Model_avg = InterWeavedModelAVG([model4_1, model4_2, model4_3, model4_4], num_models = 4)\n",
    "optimizer_four_interweaved_avg = torch.optim.Adam(Four_InterWeaved_Model_avg.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 7.3314, Val Loss: 6.1827\n",
      "Epoch 2, Train Loss: 6.5291, Val Loss: 5.5129\n",
      "Epoch 3, Train Loss: 6.0614, Val Loss: 5.0762\n",
      "Epoch 4, Train Loss: 5.6430, Val Loss: 4.6297\n",
      "Epoch 5, Train Loss: 5.2901, Val Loss: 4.3406\n",
      "Epoch 6, Train Loss: 4.9996, Val Loss: 3.9724\n",
      "Epoch 7, Train Loss: 4.7218, Val Loss: 3.7491\n",
      "Epoch 8, Train Loss: 4.4839, Val Loss: 3.5160\n",
      "Epoch 9, Train Loss: 4.3038, Val Loss: 3.3626\n",
      "Epoch 10, Train Loss: 4.0948, Val Loss: 3.2043\n",
      "Epoch 11, Train Loss: 3.9468, Val Loss: 3.0289\n",
      "Epoch 12, Train Loss: 3.7739, Val Loss: 2.8879\n",
      "Epoch 13, Train Loss: 3.6470, Val Loss: 2.7556\n",
      "Epoch 14, Train Loss: 3.5349, Val Loss: 2.6317\n",
      "Epoch 15, Train Loss: 3.4093, Val Loss: 2.5389\n",
      "Epoch 16, Train Loss: 3.2676, Val Loss: 2.4248\n",
      "Epoch 17, Train Loss: 3.1642, Val Loss: 2.2757\n",
      "Epoch 18, Train Loss: 3.0466, Val Loss: 2.2754\n",
      "Epoch 19, Train Loss: 2.9730, Val Loss: 2.1775\n",
      "Epoch 20, Train Loss: 2.8864, Val Loss: 2.1038\n",
      "Epoch 21, Train Loss: 2.7517, Val Loss: 2.0650\n",
      "Epoch 22, Train Loss: 2.7051, Val Loss: 1.9665\n",
      "Epoch 23, Train Loss: 2.5960, Val Loss: 1.9022\n",
      "Epoch 24, Train Loss: 2.5418, Val Loss: 1.7845\n",
      "Epoch 25, Train Loss: 2.4640, Val Loss: 1.7307\n",
      "Epoch 26, Train Loss: 2.3978, Val Loss: 1.7524\n",
      "Epoch 27, Train Loss: 2.3153, Val Loss: 1.6574\n",
      "Epoch 28, Train Loss: 2.2560, Val Loss: 1.5785\n",
      "Epoch 29, Train Loss: 2.1985, Val Loss: 1.5304\n",
      "Epoch 30, Train Loss: 2.1325, Val Loss: 1.5707\n",
      "Epoch 31, Train Loss: 2.0633, Val Loss: 1.4324\n",
      "Epoch 32, Train Loss: 2.0047, Val Loss: 1.3822\n",
      "Epoch 33, Train Loss: 1.9360, Val Loss: 1.3708\n",
      "Epoch 34, Train Loss: 1.8794, Val Loss: 1.3275\n",
      "Epoch 35, Train Loss: 1.8327, Val Loss: 1.2713\n",
      "Epoch 36, Train Loss: 1.7817, Val Loss: 1.2002\n",
      "Epoch 37, Train Loss: 1.7053, Val Loss: 1.1538\n",
      "Epoch 38, Train Loss: 1.6811, Val Loss: 1.1915\n",
      "Epoch 39, Train Loss: 1.6355, Val Loss: 1.0888\n",
      "Epoch 40, Train Loss: 1.6059, Val Loss: 1.1375\n",
      "Epoch 41, Train Loss: 1.5400, Val Loss: 1.0629\n",
      "Epoch 42, Train Loss: 1.5094, Val Loss: 1.0664\n",
      "Epoch 43, Train Loss: 1.4746, Val Loss: 1.0146\n",
      "Epoch 44, Train Loss: 1.4428, Val Loss: 1.0167\n",
      "Epoch 45, Train Loss: 1.3970, Val Loss: 0.9552\n",
      "Epoch 46, Train Loss: 1.3423, Val Loss: 0.9501\n",
      "Epoch 47, Train Loss: 1.3006, Val Loss: 0.8913\n",
      "Epoch 48, Train Loss: 1.2688, Val Loss: 0.9296\n",
      "Epoch 49, Train Loss: 1.2205, Val Loss: 0.8142\n",
      "Epoch 50, Train Loss: 1.2187, Val Loss: 0.7766\n",
      "Epoch 51, Train Loss: 1.1693, Val Loss: 0.8125\n",
      "Epoch 52, Train Loss: 1.1466, Val Loss: 0.8123\n",
      "Epoch 53, Train Loss: 1.1297, Val Loss: 0.7532\n",
      "Epoch 54, Train Loss: 1.0975, Val Loss: 0.7167\n",
      "Epoch 55, Train Loss: 1.0759, Val Loss: 0.7539\n",
      "Epoch 56, Train Loss: 1.0368, Val Loss: 0.7090\n",
      "Epoch 57, Train Loss: 1.0107, Val Loss: 0.6924\n",
      "Epoch 58, Train Loss: 0.9999, Val Loss: 0.6871\n",
      "Epoch 59, Train Loss: 0.9671, Val Loss: 0.6318\n",
      "Epoch 60, Train Loss: 0.9318, Val Loss: 0.6135\n",
      "Epoch 61, Train Loss: 0.9122, Val Loss: 0.6168\n",
      "Epoch 62, Train Loss: 0.9072, Val Loss: 0.6047\n",
      "Epoch 63, Train Loss: 0.8524, Val Loss: 0.5965\n",
      "Epoch 64, Train Loss: 0.8589, Val Loss: 0.5902\n",
      "Epoch 65, Train Loss: 0.8555, Val Loss: 0.5619\n",
      "Epoch 66, Train Loss: 0.8118, Val Loss: 0.5115\n",
      "Epoch 67, Train Loss: 0.7901, Val Loss: 0.5225\n",
      "Epoch 68, Train Loss: 0.7699, Val Loss: 0.5402\n",
      "Epoch 69, Train Loss: 0.7509, Val Loss: 0.5135\n",
      "Epoch 70, Train Loss: 0.7442, Val Loss: 0.5336\n"
     ]
    }
   ],
   "source": [
    "four_interweaved_avg_curve = train(Four_InterWeaved_Model_avg, optimizer_four_interweaved_avg, 70, train_dataloader, val_dataloader)\n",
    "torch.save(Four_InterWeaved_Model_avg.state_dict(), 'models/Four_InterWeaved_Model_avg.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "Four_InterWeaved_Model_same = InterWeavedModel([model4_1, model4_2, model4_3, model4_4], num_models = 4, same=True)\n",
    "optimizer_four_interweaved_same = torch.optim.Adam(Four_InterWeaved_Model_same.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 4.9986, Val Loss: 3.2209\n",
      "Epoch 2, Train Loss: 4.0076, Val Loss: 2.8484\n",
      "Epoch 3, Train Loss: 3.6213, Val Loss: 2.5998\n",
      "Epoch 4, Train Loss: 3.3534, Val Loss: 2.3601\n",
      "Epoch 5, Train Loss: 3.1416, Val Loss: 2.2324\n",
      "Epoch 6, Train Loss: 2.9945, Val Loss: 2.1054\n",
      "Epoch 7, Train Loss: 2.8266, Val Loss: 2.0236\n",
      "Epoch 8, Train Loss: 2.6919, Val Loss: 1.9268\n",
      "Epoch 9, Train Loss: 2.6016, Val Loss: 1.8170\n",
      "Epoch 10, Train Loss: 2.5083, Val Loss: 1.7413\n",
      "Epoch 11, Train Loss: 2.4047, Val Loss: 1.6715\n",
      "Epoch 12, Train Loss: 2.3146, Val Loss: 1.6429\n",
      "Epoch 13, Train Loss: 2.2167, Val Loss: 1.6203\n",
      "Epoch 14, Train Loss: 2.1367, Val Loss: 1.4960\n",
      "Epoch 15, Train Loss: 2.0919, Val Loss: 1.4695\n",
      "Epoch 16, Train Loss: 2.0034, Val Loss: 1.4287\n",
      "Epoch 17, Train Loss: 1.9371, Val Loss: 1.3473\n",
      "Epoch 18, Train Loss: 1.8983, Val Loss: 1.3161\n",
      "Epoch 19, Train Loss: 1.8208, Val Loss: 1.2842\n",
      "Epoch 20, Train Loss: 1.7941, Val Loss: 1.2391\n",
      "Epoch 21, Train Loss: 1.7217, Val Loss: 1.2155\n",
      "Epoch 22, Train Loss: 1.6823, Val Loss: 1.1322\n",
      "Epoch 23, Train Loss: 1.6158, Val Loss: 1.1279\n",
      "Epoch 24, Train Loss: 1.5516, Val Loss: 1.0683\n",
      "Epoch 25, Train Loss: 1.5274, Val Loss: 1.0586\n",
      "Epoch 26, Train Loss: 1.5107, Val Loss: 1.0829\n",
      "Epoch 27, Train Loss: 1.4580, Val Loss: 0.9685\n",
      "Epoch 28, Train Loss: 1.4053, Val Loss: 0.9432\n",
      "Epoch 29, Train Loss: 1.3626, Val Loss: 0.9923\n",
      "Epoch 30, Train Loss: 1.3255, Val Loss: 0.9354\n",
      "Epoch 31, Train Loss: 1.3063, Val Loss: 0.9045\n",
      "Epoch 32, Train Loss: 1.2713, Val Loss: 0.8872\n",
      "Epoch 33, Train Loss: 1.2217, Val Loss: 0.8719\n",
      "Epoch 34, Train Loss: 1.1947, Val Loss: 0.8339\n",
      "Epoch 35, Train Loss: 1.1472, Val Loss: 0.8201\n",
      "Epoch 36, Train Loss: 1.1296, Val Loss: 0.8033\n",
      "Epoch 37, Train Loss: 1.0838, Val Loss: 0.7919\n",
      "Epoch 38, Train Loss: 1.0808, Val Loss: 0.7229\n",
      "Epoch 39, Train Loss: 1.0561, Val Loss: 0.6958\n",
      "Epoch 40, Train Loss: 1.0129, Val Loss: 0.6852\n",
      "Epoch 41, Train Loss: 0.9988, Val Loss: 0.6706\n",
      "Epoch 42, Train Loss: 0.9674, Val Loss: 0.6654\n",
      "Epoch 43, Train Loss: 0.9415, Val Loss: 0.6688\n",
      "Epoch 44, Train Loss: 0.9271, Val Loss: 0.6234\n",
      "Epoch 45, Train Loss: 0.8966, Val Loss: 0.6779\n",
      "Epoch 46, Train Loss: 0.8737, Val Loss: 0.5944\n",
      "Epoch 47, Train Loss: 0.8604, Val Loss: 0.6044\n",
      "Epoch 48, Train Loss: 0.8181, Val Loss: 0.6126\n",
      "Epoch 49, Train Loss: 0.8024, Val Loss: 0.5937\n",
      "Epoch 50, Train Loss: 0.8031, Val Loss: 0.5874\n",
      "Epoch 51, Train Loss: 0.7742, Val Loss: 0.6009\n",
      "Epoch 52, Train Loss: 0.7594, Val Loss: 0.5425\n",
      "Epoch 53, Train Loss: 0.7393, Val Loss: 0.5390\n",
      "Epoch 54, Train Loss: 0.7357, Val Loss: 0.5394\n",
      "Epoch 55, Train Loss: 0.7015, Val Loss: 0.4947\n",
      "Epoch 56, Train Loss: 0.6899, Val Loss: 0.4646\n",
      "Epoch 57, Train Loss: 0.6751, Val Loss: 0.4616\n",
      "Epoch 58, Train Loss: 0.6462, Val Loss: 0.4202\n",
      "Epoch 59, Train Loss: 0.6223, Val Loss: 0.4568\n",
      "Epoch 60, Train Loss: 0.6117, Val Loss: 0.4237\n",
      "Epoch 61, Train Loss: 0.6085, Val Loss: 0.4239\n",
      "Epoch 62, Train Loss: 0.5907, Val Loss: 0.4021\n",
      "Epoch 63, Train Loss: 0.5784, Val Loss: 0.4310\n",
      "Epoch 64, Train Loss: 0.5663, Val Loss: 0.4184\n",
      "Epoch 65, Train Loss: 0.5431, Val Loss: 0.4216\n",
      "Epoch 66, Train Loss: 0.5341, Val Loss: 0.4050\n",
      "Epoch 67, Train Loss: 0.5251, Val Loss: 0.3593\n",
      "Epoch 68, Train Loss: 0.5097, Val Loss: 0.3585\n",
      "Epoch 69, Train Loss: 0.5129, Val Loss: 0.3469\n",
      "Epoch 70, Train Loss: 0.4886, Val Loss: 0.3716\n"
     ]
    }
   ],
   "source": [
    "four_interweaved_same_curve = train(Four_InterWeaved_Model_same, optimizer_four_interweaved_same, 70, train_dataloader, val_dataloader)\n",
    "torch.save(Four_InterWeaved_Model_same.state_dict(), 'models/Four_InterWeaved_Model_same.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "Four_single_stack = StackedModel([model4_1, model4_1, model4_1, model4_1], num_models = 4)\n",
    "optimizer_four_single_stack = torch.optim.Adam(Four_single_stack.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 5.1752, Val Loss: 3.3279\n",
      "Epoch 2, Train Loss: 3.8908, Val Loss: 2.9059\n",
      "Epoch 3, Train Loss: 3.5137, Val Loss: 2.5781\n",
      "Epoch 4, Train Loss: 3.2685, Val Loss: 2.4512\n",
      "Epoch 5, Train Loss: 3.1116, Val Loss: 2.3297\n",
      "Epoch 6, Train Loss: 2.9380, Val Loss: 2.1589\n",
      "Epoch 7, Train Loss: 2.7678, Val Loss: 2.0606\n",
      "Epoch 8, Train Loss: 2.6162, Val Loss: 1.8769\n",
      "Epoch 9, Train Loss: 2.5196, Val Loss: 1.7503\n",
      "Epoch 10, Train Loss: 2.3918, Val Loss: 1.7126\n",
      "Epoch 11, Train Loss: 2.3254, Val Loss: 1.6841\n",
      "Epoch 12, Train Loss: 2.2186, Val Loss: 1.6001\n",
      "Epoch 13, Train Loss: 2.1504, Val Loss: 1.5420\n",
      "Epoch 14, Train Loss: 2.0786, Val Loss: 1.4725\n",
      "Epoch 15, Train Loss: 2.0279, Val Loss: 1.3658\n",
      "Epoch 16, Train Loss: 1.9709, Val Loss: 1.3742\n",
      "Epoch 17, Train Loss: 1.8774, Val Loss: 1.3043\n",
      "Epoch 18, Train Loss: 1.8334, Val Loss: 1.2458\n",
      "Epoch 19, Train Loss: 1.7844, Val Loss: 1.2655\n",
      "Epoch 20, Train Loss: 1.7281, Val Loss: 1.2591\n",
      "Epoch 21, Train Loss: 1.6932, Val Loss: 1.1357\n",
      "Epoch 22, Train Loss: 1.6384, Val Loss: 1.1424\n",
      "Epoch 23, Train Loss: 1.5804, Val Loss: 1.0664\n",
      "Epoch 24, Train Loss: 1.5435, Val Loss: 1.0353\n",
      "Epoch 25, Train Loss: 1.5058, Val Loss: 1.0624\n",
      "Epoch 26, Train Loss: 1.4656, Val Loss: 1.0457\n",
      "Epoch 27, Train Loss: 1.4121, Val Loss: 0.9921\n",
      "Epoch 28, Train Loss: 1.3849, Val Loss: 1.0006\n",
      "Epoch 29, Train Loss: 1.3352, Val Loss: 0.9219\n",
      "Epoch 30, Train Loss: 1.3247, Val Loss: 0.9400\n",
      "Epoch 31, Train Loss: 1.2749, Val Loss: 0.8720\n",
      "Epoch 32, Train Loss: 1.2294, Val Loss: 0.9045\n",
      "Epoch 33, Train Loss: 1.2125, Val Loss: 0.8882\n",
      "Epoch 34, Train Loss: 1.1637, Val Loss: 0.8653\n",
      "Epoch 35, Train Loss: 1.1560, Val Loss: 0.8702\n",
      "Epoch 36, Train Loss: 1.1147, Val Loss: 0.7753\n",
      "Epoch 37, Train Loss: 1.0901, Val Loss: 0.8077\n",
      "Epoch 38, Train Loss: 1.0598, Val Loss: 0.7726\n",
      "Epoch 39, Train Loss: 1.0314, Val Loss: 0.7199\n",
      "Epoch 40, Train Loss: 1.0169, Val Loss: 0.7057\n",
      "Epoch 41, Train Loss: 0.9701, Val Loss: 0.6954\n",
      "Epoch 42, Train Loss: 0.9485, Val Loss: 0.6690\n",
      "Epoch 43, Train Loss: 0.9235, Val Loss: 0.6690\n",
      "Epoch 44, Train Loss: 0.8978, Val Loss: 0.6208\n",
      "Epoch 45, Train Loss: 0.8904, Val Loss: 0.6078\n",
      "Epoch 46, Train Loss: 0.8816, Val Loss: 0.6350\n",
      "Epoch 47, Train Loss: 0.8393, Val Loss: 0.6281\n",
      "Epoch 48, Train Loss: 0.8152, Val Loss: 0.5750\n",
      "Epoch 49, Train Loss: 0.8224, Val Loss: 0.5450\n",
      "Epoch 50, Train Loss: 0.7702, Val Loss: 0.5749\n",
      "Epoch 51, Train Loss: 0.7758, Val Loss: 0.5285\n",
      "Epoch 52, Train Loss: 0.7560, Val Loss: 0.5095\n",
      "Epoch 53, Train Loss: 0.7283, Val Loss: 0.5103\n",
      "Epoch 54, Train Loss: 0.7162, Val Loss: 0.5323\n",
      "Epoch 55, Train Loss: 0.6861, Val Loss: 0.5033\n",
      "Epoch 56, Train Loss: 0.6733, Val Loss: 0.4546\n",
      "Epoch 57, Train Loss: 0.6822, Val Loss: 0.5016\n",
      "Epoch 58, Train Loss: 0.6397, Val Loss: 0.4773\n",
      "Epoch 59, Train Loss: 0.6472, Val Loss: 0.4463\n",
      "Epoch 60, Train Loss: 0.6225, Val Loss: 0.4787\n",
      "Epoch 61, Train Loss: 0.6071, Val Loss: 0.4062\n",
      "Epoch 62, Train Loss: 0.5892, Val Loss: 0.4505\n",
      "Epoch 63, Train Loss: 0.5675, Val Loss: 0.4219\n",
      "Epoch 64, Train Loss: 0.5722, Val Loss: 0.4326\n",
      "Epoch 65, Train Loss: 0.5504, Val Loss: 0.3930\n",
      "Epoch 66, Train Loss: 0.5273, Val Loss: 0.3805\n",
      "Epoch 67, Train Loss: 0.5292, Val Loss: 0.3611\n",
      "Epoch 68, Train Loss: 0.5151, Val Loss: 0.3531\n",
      "Epoch 69, Train Loss: 0.5035, Val Loss: 0.3846\n",
      "Epoch 70, Train Loss: 0.4965, Val Loss: 0.3258\n"
     ]
    }
   ],
   "source": [
    "four_single_stack_curve = train(Four_single_stack, optimizer_four_single_stack, 70, train_dataloader, val_dataloader)\n",
    "torch.save(Four_single_stack.state_dict(), 'models/Four_single_stack.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "Four_single_interweaved = InterWeavedModel([model4_1, model4_1, model4_1, model4_1], num_models = 4)\n",
    "optimizer_four_single_interweaved = torch.optim.Adam(Four_single_interweaved.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.14 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[181], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m four_single_interweaved_curve \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFour_single_interweaved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_four_single_interweaved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(Four_single_interweaved\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/Four_single_interweaved.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/home/da0698@unt.ad.unt.edu/anaconda3/envs/dlgpu/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[56], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(mdl, optim, epochs, train_dataloader, val_dataloader, device)\u001b[0m\n\u001b[1;32m     38\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 40\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/home/da0698@unt.ad.unt.edu/anaconda3/envs/dlgpu/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/da0698@unt.ad.unt.edu/anaconda3/envs/dlgpu/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/da0698@unt.ad.unt.edu/anaconda3/envs/dlgpu/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.14 GiB. GPU "
     ]
    }
   ],
   "source": [
    "four_single_interweaved_curve = train(Four_single_interweaved, optimizer_four_single_interweaved, 70, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scratch_Four_Big_Model = Model(16).to(device)\n",
    "optimizer_scratch_four_big = torch.optim.Adam(Scratch_Four_Big_Model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_four_big_curve = train(Scratch_Four_Big_Model, optimizer_scratch_four_big, 70, train_dataloader, val_dataloader)\n",
    "torch.save(Scratch_Four_Big_Model.state_dict(), 'models/Scratch_Four_Big_Model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({'Big Model': scratch_four_big_curve, 'Stacked Model': four_stack_curve, 'Stacked Model (Average)': four_stack_avg_curve, 'Stacked Model (Same)': four_stack_same_curve, 'Interweaved Model': four_interweaved_curve, 'Interweaved Model (Average)': four_interweaved_avg_curve, 'Interweaved Model (Same)': four_interweaved_same_curve, 'Single Stacked Model': four_single_stack_curve, 'Single Interweaved Model': four_single_interweaved_curve})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_pickle('Wiki2_Four_model_curves')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
