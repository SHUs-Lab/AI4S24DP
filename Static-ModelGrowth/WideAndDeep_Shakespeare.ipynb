{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespear_dat.txt', 'r') as f:\n",
    "    dat = f.read()\n",
    "chars = sorted(list(set(dat)))\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda i: ''.join([itos[l] for l in i])\n",
    "import torch, math\n",
    "data = torch.tensor(encode(dat))\n",
    "device = torch.device('cuda:0')\n",
    "# train test split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "block_size = 8\n",
    "batch_size = 2048\n",
    "val_batch_size = 2048\n",
    "vocab_size = len(chars)\n",
    "emb_size = 32\n",
    "new_emb_size = 64\n",
    "num_small_layers = 4\n",
    "multi_heads = 2\n",
    "num_large_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size if split=='train' else val_batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(torch.nn.Module):\n",
    "    def __init__(self, big=False):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.k = torch.nn.Linear(new_emb_size if big else emb_size, new_emb_size if big else emb_size, bias=False)\n",
    "        self.q = torch.nn.Linear(new_emb_size if big else emb_size, new_emb_size if big else emb_size, bias=False)\n",
    "        self.v = torch.nn.Linear(new_emb_size if big else emb_size, new_emb_size if big else emb_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
    "    def forward(self, e):\n",
    "        keys = self.k(e)\n",
    "        queries = self.q(e)\n",
    "        values = self.v(e)\n",
    "        ret = keys @ queries.transpose(1, 2)*(1.0/math.sqrt(keys.size(-1)))\n",
    "        ret = torch.masked_fill(ret, self.tril==0, -torch.inf)\n",
    "        ret = torch.softmax(ret, 2)\n",
    "        ret = ret @ values\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(torch.nn.Module):\n",
    "    def __init__(self, big=False):\n",
    "        super(MultiHead, self).__init__()\n",
    "        self.head1 = AttentionHead(big)\n",
    "        self.head2 = AttentionHead(big)\n",
    "        self.mh_lin = torch.nn.Linear(multi_heads*(new_emb_size if big else emb_size), new_emb_size if big else emb_size, bias=False)\n",
    "        self.drop = torch.nn.Dropout(0.1)\n",
    "    def forward(self, inp):\n",
    "        x1 = self.head1(inp)\n",
    "        x2 = self.head2(inp)\n",
    "        return self.mh_lin(self.drop(torch.cat([x1,x2], dim=2))).relu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(torch.nn.Module):\n",
    "    def __init__(self, big=False):\n",
    "        super(Block, self).__init__()\n",
    "        self.multihead = MultiHead(big)\n",
    "        self.l_norm_1 = torch.nn.LayerNorm(new_emb_size if big else emb_size)\n",
    "        self.l_norm_2 = torch.nn.LayerNorm(new_emb_size if big else emb_size)\n",
    "        self.ffn = torch.nn.Linear(new_emb_size if big else emb_size, new_emb_size if big else emb_size)\n",
    "        self.drop = torch.nn.Dropout(0.1)\n",
    "    def forward(self, inp):\n",
    "        m = self.l_norm_1(inp + self.multihead(inp))\n",
    "        m = self.l_norm_2(m + self.ffn(self.drop(m)).relu())\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, emb_size)\n",
    "        self.pe = PositionalEncoding(emb_size)\n",
    "        self.block1 = Block()\n",
    "        self.block2 = Block()\n",
    "        self.block3 = Block()\n",
    "        self.block4 = Block()\n",
    "        self.f_lin = torch.nn.Linear(emb_size, vocab_size)\n",
    "        self.drop = torch.nn.Dropout(0.1)\n",
    "    def forward(self, inp):\n",
    "        e = self.embedding(inp)\n",
    "        e = self.pe(e)\n",
    "        m = self.block1(e)\n",
    "        m = self.block2(m)\n",
    "        m = self.block3(m)\n",
    "        m = self.block4(m)\n",
    "        r = self.f_lin(self.drop(m))\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da0698@unt.ad.unt.edu/anaconda3/envs/dlgpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(mdl):\n",
    "    mdl.eval()\n",
    "    vx, vy = get_batch('val')\n",
    "    out = mdl(vx.to(device))\n",
    "    return loss(out.view(-1, 65), vy.view(-1).to(device)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.enable_grad()\n",
    "def train(mdl, optim, epochs):\n",
    "    ind = 0\n",
    "    for _ in range(epochs):\n",
    "        mdl.train()\n",
    "        optim.zero_grad()\n",
    "        x, y = get_batch('train')\n",
    "        out = mdl(x.to(device))\n",
    "        l = loss(out.view(-1, 65), y.view(-1).to(device))\n",
    "        l.backward()\n",
    "        optim.step()\n",
    "        ind += 1\n",
    "        if ind%100 == 0:\n",
    "            print(l.item())\n",
    "            print(f\"Validation: {validate(mdl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9411230087280273\n",
      "Validation: 2.8431901931762695\n",
      "2.6231515407562256\n",
      "Validation: 2.4993515014648438\n",
      "2.493496894836426\n",
      "Validation: 2.3980233669281006\n",
      "2.4104607105255127\n",
      "Validation: 2.328835964202881\n",
      "2.414330005645752\n",
      "Validation: 2.289748191833496\n",
      "2.3340442180633545\n",
      "Validation: 2.2688584327697754\n",
      "2.3394172191619873\n",
      "Validation: 2.2560274600982666\n",
      "2.3237712383270264\n",
      "Validation: 2.234231472015381\n",
      "2.2912347316741943\n",
      "Validation: 2.2267966270446777\n",
      "2.279399871826172\n",
      "Validation: 2.1773111820220947\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_params = torch.empty((0,emb_size*emb_size)).to(device)\n",
    "q_params = torch.empty((0,emb_size*emb_size)).to(device)\n",
    "v_params = torch.empty((0,emb_size*emb_size)).to(device)\n",
    "lin_params = torch.empty((0, 2*emb_size*emb_size)).to(device)\n",
    "ffn_w_params = torch.empty((0, emb_size*emb_size)).to(device)\n",
    "ffn_b_params = torch.empty((0, emb_size)).to(device)\n",
    "l_norm_w_params = torch.empty((0, emb_size)).to(device)\n",
    "l_norm_b_params = torch.empty((0, emb_size)).to(device)\n",
    "for i in model.state_dict():\n",
    "    if '.k.' in i:\n",
    "        k_params = torch.cat((k_params, model.state_dict()[i].flatten().view(1,-1)), dim=0)\n",
    "    elif '.q.' in i:\n",
    "        q_params = torch.cat((q_params, model.state_dict()[i].flatten().view(1,-1)), dim=0)\n",
    "    elif '.v.' in i:\n",
    "        v_params = torch.cat((v_params, model.state_dict()[i].flatten().view(1,-1)), dim=0)\n",
    "    elif '.mh_lin' in i:\n",
    "        lin_params = torch.cat((lin_params, model.state_dict()[i].flatten().view(1,-1)), dim = 0)\n",
    "    elif 'ffn' in i and 'weight' in i:\n",
    "        ffn_w_params = torch.cat((ffn_w_params, model.state_dict()[i].flatten().view(1,-1)), dim = 0)\n",
    "    elif 'ffn' in i and 'bias' in i:\n",
    "        ffn_b_params = torch.cat((ffn_b_params, model.state_dict()[i].flatten().view(1,-1)), dim = 0)\n",
    "    elif 'l_norm' in i and 'weight' in i:\n",
    "        l_norm_w_params = torch.cat((l_norm_w_params, model.state_dict()[i].flatten().view(1,-1)), dim = 0)\n",
    "    elif 'l_norm' in i and 'bias' in i:\n",
    "        l_norm_b_params = torch.cat((l_norm_b_params, model.state_dict()[i].flatten().view(1,-1)), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideModel(torch.nn.Module):\n",
    "    def __init__(self, big=True):\n",
    "        super(WideModel, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, new_emb_size if big else emb_size)\n",
    "        self.pe = PositionalEncoding(new_emb_size if big else emb_size)\n",
    "        self.block1 = Block(big=big)\n",
    "        self.block2 = Block(big=big)\n",
    "        self.block3 = Block(big=big)\n",
    "        self.block4 = Block(big=big)\n",
    "        self.f_lin = torch.nn.Linear(new_emb_size if big else emb_size, vocab_size)\n",
    "        self.drop = torch.nn.Dropout(0.1)\n",
    "    def forward(self, inp):\n",
    "        e = self.embedding(inp)\n",
    "        e = self.pe(e)\n",
    "        m = self.block1(e)\n",
    "        m = self.block2(m)\n",
    "        m = self.block3(m)\n",
    "        m = self.block4(m)\n",
    "        r = self.f_lin(self.drop(m))\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G_zero -----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "1  0\n",
    "\n",
    "0  0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wideModel = WideModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_zero_emb_or_f_lin(emb):\n",
    "    zero_tensor = torch.zeros(emb.shape).to(device)\n",
    "    return torch.cat((emb, zero_tensor), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_zero_1d(l_norm):\n",
    "    l_zero = torch.zeros(l_norm.shape).to(device)\n",
    "    return torch.cat((l_norm, l_zero), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_zero(weight):\n",
    "    zero_tensor = torch.zeros(weight.shape).to(device)\n",
    "    temp_1= torch.cat((weight, zero_tensor), dim=1)\n",
    "    temp_2= torch.cat((zero_tensor, zero_tensor), dim=1)\n",
    "    return torch.cat((temp_1, temp_2), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1  0\n",
    "\n",
    "0  0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def initWideExtendGZero(has_bias=False):\n",
    "    # Embedding layer\n",
    "    emb = model.embedding.weight\n",
    "    wide_emb = G_zero_emb_or_f_lin(emb)\n",
    "    setattr(wideModel.embedding, 'weight', torch.nn.Parameter(wide_emb, requires_grad=True).to(device))\n",
    "    # f_lin layer weight\n",
    "    f_lin_weight = model.f_lin.weight\n",
    "    wide_f_lin_weight = G_zero_emb_or_f_lin(f_lin_weight)\n",
    "    setattr(wideModel.f_lin, 'weight', torch.nn.Parameter(wide_f_lin_weight, requires_grad=True).to(device))\n",
    "    # f_lin layer bias\n",
    "    if has_bias == True:\n",
    "        f_lin_bias = model.f_lin.bias\n",
    "        wide_f_lin_bias = G_zero_emb_or_f_lin(f_lin_bias)\n",
    "        setattr(wideModel.f_lin, 'bias', torch.nn.Parameter(wide_f_lin_bias, requires_grad=True).to(device))\n",
    "\n",
    "    \n",
    "    ii_list = [1,2,3,4]\n",
    "    for i in range(1, num_small_layers+1):\n",
    "        ii = ii_list[i-1]\n",
    "        small_block = getattr(model, f'block{ii}')\n",
    "        wide_block = getattr(wideModel, f'block{i}')\n",
    "    \n",
    "        # Setting FFN Weights\n",
    "        wide_ffn_weight = G_zero(small_block.ffn.weight)\n",
    "        setattr(wide_block.ffn, 'weight', torch.nn.Parameter((wide_ffn_weight), requires_grad=True).to(device))\n",
    "        if has_bias == True:\n",
    "            wide_ffn_bias = G_zero(small_block.ffn.bias)\n",
    "            setattr(wide_block.ffn, 'bias', torch.nn.Parameter((wide_ffn_bias), requires_grad=True).to(device))\n",
    "\n",
    "        # Setting Norm Layers\n",
    "        l_norm_1_weight = G_zero_1d(small_block.l_norm_1.weight)\n",
    "        l_norm_2_weight = G_zero_1d(small_block.l_norm_2.weight)\n",
    "        setattr(wide_block.l_norm_1, 'weight', torch.nn.Parameter((l_norm_1_weight), requires_grad=True).to(device))\n",
    "        setattr(wide_block.l_norm_2, 'weight', torch.nn.Parameter((l_norm_2_weight), requires_grad=True).to(device))\n",
    "        if has_bias == True:\n",
    "            l_norm_1_bias = G_zero_1d(small_block.l_norm_1.bias)\n",
    "            l_norm_2_bias = G_zero_1d(small_block.l_norm_2.bias)\n",
    "            setattr(wide_block.l_norm_1, 'bias', torch.nn.Parameter((l_norm_1_bias), requires_grad=True).to(device))\n",
    "            setattr(wide_block.l_norm_2, 'bias', torch.nn.Parameter((l_norm_2_bias), requires_grad=True).to(device))\n",
    "\n",
    "        # Setting Multi-Head Attention\n",
    "        wide_mh_lin_weight = G_zero(small_block.multihead.mh_lin.weight)\n",
    "        setattr(wide_block.multihead.mh_lin, 'weight', torch.nn.Parameter((wide_mh_lin_weight), requires_grad=True))\n",
    "\n",
    "        for h in range(1, multi_heads+1):\n",
    "            head = getattr(wide_block.multihead, f'head{h}')\n",
    "            small_head = getattr(small_block.multihead, f'head{h}')\n",
    "\n",
    "            new_k = G_zero(small_head.k.weight)\n",
    "            new_q = G_zero(small_head.q.weight)\n",
    "            new_v = G_zero(small_head.v.weight)\n",
    "            setattr(head.k, 'weight', torch.nn.Parameter((new_k), requires_grad=True).to(device))\n",
    "            setattr(head.q, 'weight', torch.nn.Parameter((new_q), requires_grad=True).to(device))\n",
    "            setattr(head.v, 'weight', torch.nn.Parameter((new_v), requires_grad=True).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "initWideExtendGZero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.346893310546875\n",
      "Validation: 2.2651448249816895\n",
      "2.3022103309631348\n",
      "Validation: 2.231424331665039\n",
      "2.2866806983947754\n",
      "Validation: 2.1899495124816895\n",
      "2.241973400115967\n",
      "Validation: 2.1845719814300537\n",
      "2.2491180896759033\n",
      "Validation: 2.164644241333008\n",
      "2.24556303024292\n",
      "Validation: 2.187959909439087\n",
      "2.2474703788757324\n",
      "Validation: 2.1449010372161865\n",
      "2.2014172077178955\n",
      "Validation: 2.1533243656158447\n",
      "2.196962594985962\n",
      "Validation: 2.146059989929199\n",
      "2.1789472103118896\n",
      "Validation: 2.1582231521606445\n"
     ]
    }
   ],
   "source": [
    "wideModel.train()\n",
    "optim_b = torch.optim.Adam(params=wideModel.parameters(), lr = 1e-3)\n",
    "\n",
    "train(wideModel, optim_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 -----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "1    0\n",
    "\n",
    "0    1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_zero_v2(weight):\n",
    "    zero_tensor = torch.zeros(weight.shape).to(device)\n",
    "    temp_1= torch.cat((weight, zero_tensor), dim=1)\n",
    "    temp_2= torch.cat((zero_tensor, weight), dim=1)\n",
    "    return torch.cat((temp_1, temp_2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wideModel = WideModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def initWideExtendGZeroV2(has_bias=False):\n",
    "    # Embedding layer\n",
    "    emb = model.embedding.weight\n",
    "    wide_emb = G_zero_emb_or_f_lin(emb)\n",
    "    setattr(wideModel.embedding, 'weight', torch.nn.Parameter(wide_emb, requires_grad=True).to(device))\n",
    "    # f_lin layer weight\n",
    "    f_lin_weight = model.f_lin.weight\n",
    "    wide_f_lin_weight = G_zero_emb_or_f_lin(f_lin_weight)\n",
    "    setattr(wideModel.f_lin, 'weight', torch.nn.Parameter(wide_f_lin_weight, requires_grad=True).to(device))\n",
    "    # f_lin layer bias\n",
    "    if has_bias == True:\n",
    "        f_lin_bias = model.f_lin.bias\n",
    "        wide_f_lin_bias = G_zero_emb_or_f_lin(f_lin_bias)\n",
    "        setattr(wideModel.f_lin, 'bias', torch.nn.Parameter(wide_f_lin_bias, requires_grad=True).to(device))\n",
    "\n",
    "    \n",
    "    ii_list = [1,2,3,4]\n",
    "    for i in range(1, num_small_layers+1):\n",
    "        ii = ii_list[i-1]\n",
    "        small_block = getattr(model, f'block{ii}')\n",
    "        wide_block = getattr(wideModel, f'block{i}')\n",
    "    \n",
    "        # Setting FFN Weights\n",
    "        wide_ffn_weight = G_zero_v2(small_block.ffn.weight)\n",
    "        setattr(wide_block.ffn, 'weight', torch.nn.Parameter((wide_ffn_weight), requires_grad=True).to(device))\n",
    "        if has_bias == True:\n",
    "            wide_ffn_bias = G_zero_v2(small_block.ffn.bias)\n",
    "            setattr(wide_block.ffn, 'bias', torch.nn.Parameter((wide_ffn_bias), requires_grad=True).to(device))\n",
    "\n",
    "        # Setting Norm Layers\n",
    "        l_norm_1_weight = G_zero_1d(small_block.l_norm_1.weight)\n",
    "        l_norm_2_weight = G_zero_1d(small_block.l_norm_2.weight)\n",
    "        setattr(wide_block.l_norm_1, 'weight', torch.nn.Parameter((l_norm_1_weight), requires_grad=True).to(device))\n",
    "        setattr(wide_block.l_norm_2, 'weight', torch.nn.Parameter((l_norm_2_weight), requires_grad=True).to(device))\n",
    "        if has_bias == True:\n",
    "            l_norm_1_bias = G_zero_1d(small_block.l_norm_1.bias)\n",
    "            l_norm_2_bias = G_zero_1d(small_block.l_norm_2.bias)\n",
    "            setattr(wide_block.l_norm_1, 'bias', torch.nn.Parameter((l_norm_1_bias), requires_grad=True).to(device))\n",
    "            setattr(wide_block.l_norm_2, 'bias', torch.nn.Parameter((l_norm_2_bias), requires_grad=True).to(device))\n",
    "\n",
    "        # Setting Multi-Head Attention\n",
    "        wide_mh_lin_weight = G_zero_v2(small_block.multihead.mh_lin.weight)\n",
    "        setattr(wide_block.multihead.mh_lin, 'weight', torch.nn.Parameter((wide_mh_lin_weight), requires_grad=True))\n",
    "\n",
    "        for h in range(1, multi_heads+1):\n",
    "            head = getattr(wide_block.multihead, f'head{h}')\n",
    "            small_head = getattr(small_block.multihead, f'head{h}')\n",
    "\n",
    "            new_k = G_zero_v2(small_head.k.weight)\n",
    "            new_q = G_zero_v2(small_head.q.weight)\n",
    "            new_v = G_zero_v2(small_head.v.weight)\n",
    "            setattr(head.k, 'weight', torch.nn.Parameter((new_k), requires_grad=True).to(device))\n",
    "            setattr(head.q, 'weight', torch.nn.Parameter((new_q), requires_grad=True).to(device))\n",
    "            setattr(head.v, 'weight', torch.nn.Parameter((new_v), requires_grad=True).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "initWideExtendGZeroV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3463587760925293\n",
      "Validation: 2.2469334602355957\n",
      "2.31365704536438\n",
      "Validation: 2.216831684112549\n",
      "2.244840621948242\n",
      "Validation: 2.202496290206909\n",
      "2.2367405891418457\n",
      "Validation: 2.1984591484069824\n",
      "2.221161365509033\n",
      "Validation: 2.1632261276245117\n",
      "2.2085494995117188\n",
      "Validation: 2.1445350646972656\n",
      "2.1996874809265137\n",
      "Validation: 2.1144328117370605\n",
      "2.1736671924591064\n",
      "Validation: 2.1193418502807617\n",
      "2.1432104110717773\n",
      "Validation: 2.089792251586914\n",
      "2.1327497959136963\n",
      "Validation: 2.099591016769409\n"
     ]
    }
   ],
   "source": [
    "wideModel.train()\n",
    "optim_b = torch.optim.Adam(params=wideModel.parameters(), lr = 1e-3)\n",
    "\n",
    "train(wideModel, optim_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.533486843109131\n",
      "Validation: 2.471588134765625\n",
      "2.388390064239502\n",
      "Validation: 2.31054425239563\n",
      "2.29331374168396\n",
      "Validation: 2.2138044834136963\n",
      "2.207923173904419\n",
      "Validation: 2.1886467933654785\n",
      "2.1867995262145996\n",
      "Validation: 2.180229663848877\n",
      "2.1649115085601807\n",
      "Validation: 2.1219332218170166\n",
      "2.14483380317688\n",
      "Validation: 2.130613327026367\n",
      "2.113114595413208\n",
      "Validation: 2.1115593910217285\n",
      "2.0959486961364746\n",
      "Validation: 2.0923686027526855\n",
      "2.085519313812256\n",
      "Validation: 2.087606191635132\n"
     ]
    }
   ],
   "source": [
    "rawWideModel = WideModel().to(device)\n",
    "optim_b = torch.optim.Adam(params=rawWideModel.parameters(), lr = 1e-3)\n",
    "train(rawWideModel, optim_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Become Deeper + Wider -----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigModel(torch.nn.Module):\n",
    "    def __init__(self, big=True):\n",
    "        super(BigModel, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, new_emb_size if big else emb_size)\n",
    "        self.pe = PositionalEncoding(new_emb_size if big else emb_size)\n",
    "        self.block1 = Block(big=big)\n",
    "        self.block2 = Block(big=big)\n",
    "        self.block3 = Block(big=big)\n",
    "        self.block4 = Block(big=big)\n",
    "        self.block5 = Block(big=big)\n",
    "        self.block6 = Block(big=big)\n",
    "        self.block7 = Block(big=big)\n",
    "        self.block8 = Block(big=big)\n",
    "        self.f_lin = torch.nn.Linear(new_emb_size if big else emb_size, vocab_size)\n",
    "        self.drop = torch.nn.Dropout(0.1)\n",
    "    def forward(self, inp):\n",
    "        e = self.embedding(inp)\n",
    "        e = self.pe(e)\n",
    "        m = self.block1(e)\n",
    "        m = self.block2(m)\n",
    "        m = self.block3(m)\n",
    "        m = self.block4(m)\n",
    "        m = self.block5(m)\n",
    "        m = self.block6(m)\n",
    "        m = self.block7(m)\n",
    "        m = self.block8(m)\n",
    "        r = self.f_lin(self.drop(m))\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wideBigModel = BigModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_big_layers = 8\n",
    "multi_heads = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def initWideExtendGZero_doubleStack(has_bias=False):\n",
    "    # Embedding layer\n",
    "    emb = model.embedding.weight\n",
    "    wide_emb = G_zero_emb_or_f_lin(emb)\n",
    "    setattr(wideBigModel.embedding, 'weight', torch.nn.Parameter(wide_emb, requires_grad=True).to(device))\n",
    "    # f_lin layer weight\n",
    "    f_lin_weight = model.f_lin.weight\n",
    "    wide_f_lin_weight = G_zero_emb_or_f_lin(f_lin_weight)\n",
    "    setattr(wideBigModel.f_lin, 'weight', torch.nn.Parameter(wide_f_lin_weight, requires_grad=True).to(device))\n",
    "    # f_lin layer bias\n",
    "    if has_bias == True:\n",
    "        f_lin_bias = model.f_lin.bias\n",
    "        wide_f_lin_bias = G_zero_emb_or_f_lin(f_lin_bias)\n",
    "        setattr(wideBigModel.f_lin, 'bias', torch.nn.Parameter(wide_f_lin_bias, requires_grad=True).to(device))\n",
    "\n",
    "    \n",
    "    ii_list = [1,2,3,4,1,2,3,4]\n",
    "    for i in range(1, num_big_layers+1):\n",
    "        ii = ii_list[i-1]\n",
    "        small_block = getattr(model, f'block{ii}')\n",
    "        wide_block = getattr(wideBigModel, f'block{i}')\n",
    "    \n",
    "        # Setting FFN Weights\n",
    "        wide_ffn_weight = G_zero(small_block.ffn.weight)\n",
    "        setattr(wide_block.ffn, 'weight', torch.nn.Parameter((wide_ffn_weight), requires_grad=True).to(device))\n",
    "        if has_bias == True:\n",
    "            wide_ffn_bias = G_zero(small_block.ffn.bias)\n",
    "            setattr(wide_block.ffn, 'bias', torch.nn.Parameter((wide_ffn_bias), requires_grad=True).to(device))\n",
    "\n",
    "        # Setting Norm Layers\n",
    "        l_norm_1_weight = G_zero_1d(small_block.l_norm_1.weight)\n",
    "        l_norm_2_weight = G_zero_1d(small_block.l_norm_2.weight)\n",
    "        setattr(wide_block.l_norm_1, 'weight', torch.nn.Parameter((l_norm_1_weight), requires_grad=True).to(device))\n",
    "        setattr(wide_block.l_norm_2, 'weight', torch.nn.Parameter((l_norm_2_weight), requires_grad=True).to(device))\n",
    "        if has_bias == True:\n",
    "            l_norm_1_bias = G_zero_1d(small_block.l_norm_1.bias)\n",
    "            l_norm_2_bias = G_zero_1d(small_block.l_norm_2.bias)\n",
    "            setattr(wide_block.l_norm_1, 'bias', torch.nn.Parameter((l_norm_1_bias), requires_grad=True).to(device))\n",
    "            setattr(wide_block.l_norm_2, 'bias', torch.nn.Parameter((l_norm_2_bias), requires_grad=True).to(device))\n",
    "\n",
    "        # Setting Multi-Head Attention\n",
    "        wide_mh_lin_weight = G_zero(small_block.multihead.mh_lin.weight)\n",
    "        setattr(wide_block.multihead.mh_lin, 'weight', torch.nn.Parameter((wide_mh_lin_weight), requires_grad=True))\n",
    "\n",
    "        for h in range(1, multi_heads+1):\n",
    "            head = getattr(wide_block.multihead, f'head{h}')\n",
    "            small_head = getattr(small_block.multihead, f'head{h}')\n",
    "\n",
    "            new_k = G_zero(small_head.k.weight)\n",
    "            new_q = G_zero(small_head.q.weight)\n",
    "            new_v = G_zero(small_head.v.weight)\n",
    "            setattr(head.k, 'weight', torch.nn.Parameter((new_k), requires_grad=True).to(device))\n",
    "            setattr(head.q, 'weight', torch.nn.Parameter((new_q), requires_grad=True).to(device))\n",
    "            setattr(head.v, 'weight', torch.nn.Parameter((new_v), requires_grad=True).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "initWideExtendGZero_doubleStack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.366074800491333\n",
      "Validation: 2.2506210803985596\n",
      "2.2922167778015137\n",
      "Validation: 2.2105813026428223\n",
      "2.255974292755127\n",
      "Validation: 2.168656349182129\n",
      "2.194500207901001\n",
      "Validation: 2.122814178466797\n",
      "2.181781053543091\n",
      "Validation: 2.1370317935943604\n",
      "2.1698203086853027\n",
      "Validation: 2.1307668685913086\n",
      "2.1407856941223145\n",
      "Validation: 2.089634656906128\n",
      "2.1276755332946777\n",
      "Validation: 2.097020149230957\n",
      "2.1221821308135986\n",
      "Validation: 2.0919740200042725\n",
      "2.0993220806121826\n",
      "Validation: 2.08664870262146\n"
     ]
    }
   ],
   "source": [
    "optim_b = torch.optim.Adam(params=wideBigModel.parameters(), lr = 1e-3)\n",
    "train(wideBigModel, optim_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wideBigModel = BigModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def initWideExtendGZero_doubleStac_v2(has_bias=False):\n",
    "    # Embedding layer\n",
    "    emb = model.embedding.weight\n",
    "    wide_emb = G_zero_emb_or_f_lin(emb)\n",
    "    setattr(wideBigModel.embedding, 'weight', torch.nn.Parameter(wide_emb, requires_grad=True).to(device))\n",
    "    # f_lin layer weight\n",
    "    f_lin_weight = model.f_lin.weight\n",
    "    wide_f_lin_weight = G_zero_emb_or_f_lin(f_lin_weight)\n",
    "    setattr(wideBigModel.f_lin, 'weight', torch.nn.Parameter(wide_f_lin_weight, requires_grad=True).to(device))\n",
    "    # f_lin layer bias\n",
    "    if has_bias == True:\n",
    "        f_lin_bias = model.f_lin.bias\n",
    "        wide_f_lin_bias = G_zero_emb_or_f_lin(f_lin_bias)\n",
    "        setattr(wideBigModel.f_lin, 'bias', torch.nn.Parameter(wide_f_lin_bias, requires_grad=True).to(device))\n",
    "\n",
    "    \n",
    "    ii_list = [1,2,3,4,1,2,3,4]\n",
    "    for i in range(1, num_big_layers+1):\n",
    "        ii = ii_list[i-1]\n",
    "        small_block = getattr(model, f'block{ii}')\n",
    "        wide_block = getattr(wideBigModel, f'block{i}')\n",
    "    \n",
    "        # Setting FFN Weights\n",
    "        wide_ffn_weight = G_zero_v2(small_block.ffn.weight)\n",
    "        setattr(wide_block.ffn, 'weight', torch.nn.Parameter((wide_ffn_weight), requires_grad=True).to(device))\n",
    "        if has_bias == True:\n",
    "            wide_ffn_bias = G_zero_v2(small_block.ffn.bias)\n",
    "            setattr(wide_block.ffn, 'bias', torch.nn.Parameter((wide_ffn_bias), requires_grad=True).to(device))\n",
    "\n",
    "        # Setting Norm Layers\n",
    "        l_norm_1_weight = G_zero_1d(small_block.l_norm_1.weight)\n",
    "        l_norm_2_weight = G_zero_1d(small_block.l_norm_2.weight)\n",
    "        setattr(wide_block.l_norm_1, 'weight', torch.nn.Parameter((l_norm_1_weight), requires_grad=True).to(device))\n",
    "        setattr(wide_block.l_norm_2, 'weight', torch.nn.Parameter((l_norm_2_weight), requires_grad=True).to(device))\n",
    "        if has_bias == True:\n",
    "            l_norm_1_bias = G_zero_1d(small_block.l_norm_1.bias)\n",
    "            l_norm_2_bias = G_zero_1d(small_block.l_norm_2.bias)\n",
    "            setattr(wide_block.l_norm_1, 'bias', torch.nn.Parameter((l_norm_1_bias), requires_grad=True).to(device))\n",
    "            setattr(wide_block.l_norm_2, 'bias', torch.nn.Parameter((l_norm_2_bias), requires_grad=True).to(device))\n",
    "\n",
    "        # Setting Multi-Head Attention\n",
    "        wide_mh_lin_weight = G_zero_v2(small_block.multihead.mh_lin.weight)\n",
    "        setattr(wide_block.multihead.mh_lin, 'weight', torch.nn.Parameter((wide_mh_lin_weight), requires_grad=True))\n",
    "\n",
    "        for h in range(1, multi_heads+1):\n",
    "            head = getattr(wide_block.multihead, f'head{h}')\n",
    "            small_head = getattr(small_block.multihead, f'head{h}')\n",
    "\n",
    "            new_k = G_zero_v2(small_head.k.weight)\n",
    "            new_q = G_zero_v2(small_head.q.weight)\n",
    "            new_v = G_zero_v2(small_head.v.weight)\n",
    "            setattr(head.k, 'weight', torch.nn.Parameter((new_k), requires_grad=True).to(device))\n",
    "            setattr(head.q, 'weight', torch.nn.Parameter((new_q), requires_grad=True).to(device))\n",
    "            setattr(head.v, 'weight', torch.nn.Parameter((new_v), requires_grad=True).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "initWideExtendGZero_doubleStac_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3297884464263916\n",
      "Validation: 2.2664928436279297\n",
      "2.2681915760040283\n",
      "Validation: 2.2018697261810303\n",
      "2.207615852355957\n",
      "Validation: 2.1389591693878174\n",
      "2.1661393642425537\n",
      "Validation: 2.1297335624694824\n",
      "2.1508235931396484\n",
      "Validation: 2.1076908111572266\n",
      "2.130034923553467\n",
      "Validation: 2.1098811626434326\n",
      "2.1205713748931885\n",
      "Validation: 2.0871658325195312\n",
      "2.1106972694396973\n",
      "Validation: 2.0801334381103516\n",
      "2.09464955329895\n",
      "Validation: 2.0674517154693604\n",
      "2.058530330657959\n",
      "Validation: 2.055280923843384\n"
     ]
    }
   ],
   "source": [
    "optim_b = torch.optim.Adam(params=wideBigModel.parameters(), lr = 1e-3)\n",
    "train(wideBigModel, optim_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## G Zero Cross stack -----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "## 1 0\n",
    "## 0 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "wideBigModel = BigModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def initWideExtendGZero_crossStack(has_bias=False):\n",
    "    # Embedding layer\n",
    "    emb = model.embedding.weight\n",
    "    wide_emb = G_zero_emb_or_f_lin(emb)\n",
    "    setattr(wideBigModel.embedding, 'weight', torch.nn.Parameter(wide_emb, requires_grad=True).to(device))\n",
    "    # f_lin layer weight\n",
    "    f_lin_weight = model.f_lin.weight\n",
    "    wide_f_lin_weight = G_zero_emb_or_f_lin(f_lin_weight)\n",
    "    setattr(wideBigModel.f_lin, 'weight', torch.nn.Parameter(wide_f_lin_weight, requires_grad=True).to(device))\n",
    "    # f_lin layer bias\n",
    "    if has_bias == True:\n",
    "        f_lin_bias = model.f_lin.bias\n",
    "        wide_f_lin_bias = G_zero_emb_or_f_lin(f_lin_bias)\n",
    "        setattr(wideBigModel.f_lin, 'bias', torch.nn.Parameter(wide_f_lin_bias, requires_grad=True).to(device))\n",
    "\n",
    "    \n",
    "    bi_list = [1,1,2,2,3,3,4,4]\n",
    "    for i in range(1, num_big_layers+1):\n",
    "        bi = bi_list[i-1]\n",
    "        small_block = getattr(model, f'block{bi}')\n",
    "        wide_block = getattr(wideBigModel, f'block{i}')\n",
    "    \n",
    "        # Setting FFN Weights\n",
    "        wide_ffn_weight = G_zero(small_block.ffn.weight)\n",
    "        setattr(wide_block.ffn, 'weight', torch.nn.Parameter((wide_ffn_weight), requires_grad=True).to(device))\n",
    "        if has_bias == True:\n",
    "            wide_ffn_bias = G_zero(small_block.ffn.bias)\n",
    "            setattr(wide_block.ffn, 'bias', torch.nn.Parameter((wide_ffn_bias), requires_grad=True).to(device))\n",
    "\n",
    "        # Setting Norm Layers\n",
    "        l_norm_1_weight = G_zero_1d(small_block.l_norm_1.weight)\n",
    "        l_norm_2_weight = G_zero_1d(small_block.l_norm_2.weight)\n",
    "        setattr(wide_block.l_norm_1, 'weight', torch.nn.Parameter((l_norm_1_weight), requires_grad=True).to(device))\n",
    "        setattr(wide_block.l_norm_2, 'weight', torch.nn.Parameter((l_norm_2_weight), requires_grad=True).to(device))\n",
    "        if has_bias == True:\n",
    "            l_norm_1_bias = G_zero_1d(small_block.l_norm_1.bias)\n",
    "            l_norm_2_bias = G_zero_1d(small_block.l_norm_2.bias)\n",
    "            setattr(wide_block.l_norm_1, 'bias', torch.nn.Parameter((l_norm_1_bias), requires_grad=True).to(device))\n",
    "            setattr(wide_block.l_norm_2, 'bias', torch.nn.Parameter((l_norm_2_bias), requires_grad=True).to(device))\n",
    "\n",
    "        # Setting Multi-Head Attention\n",
    "        wide_mh_lin_weight = G_zero(small_block.multihead.mh_lin.weight)\n",
    "        setattr(wide_block.multihead.mh_lin, 'weight', torch.nn.Parameter((wide_mh_lin_weight), requires_grad=True))\n",
    "\n",
    "        for h in range(1, multi_heads+1):\n",
    "            head = getattr(wide_block.multihead, f'head{h}')\n",
    "            small_head = getattr(small_block.multihead, f'head{h}')\n",
    "\n",
    "            new_k = G_zero(small_head.k.weight)\n",
    "            new_q = G_zero(small_head.q.weight)\n",
    "            new_v = G_zero(small_head.v.weight)\n",
    "            setattr(head.k, 'weight', torch.nn.Parameter((new_k), requires_grad=True).to(device))\n",
    "            setattr(head.q, 'weight', torch.nn.Parameter((new_q), requires_grad=True).to(device))\n",
    "            setattr(head.v, 'weight', torch.nn.Parameter((new_v), requires_grad=True).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3411056995391846\n",
      "Validation: 2.2575109004974365\n",
      "2.280829668045044\n",
      "Validation: 2.2327897548675537\n",
      "2.2365355491638184\n",
      "Validation: 2.1618454456329346\n",
      "2.217055320739746\n",
      "Validation: 2.150111675262451\n",
      "2.1955485343933105\n",
      "Validation: 2.126737594604492\n",
      "2.1572203636169434\n",
      "Validation: 2.1443989276885986\n",
      "2.1532928943634033\n",
      "Validation: 2.117771863937378\n",
      "2.138089656829834\n",
      "Validation: 2.1158785820007324\n",
      "2.1390902996063232\n",
      "Validation: 2.098881244659424\n",
      "2.1166787147521973\n",
      "Validation: 2.091095209121704\n"
     ]
    }
   ],
   "source": [
    "initWideExtendGZero_crossStack()\n",
    "\n",
    "optim_b = torch.optim.Adam(params=wideBigModel.parameters(), lr = 1e-3)\n",
    "train(wideBigModel, optim_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## G Zero Cross stack V2-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "## 1 0\n",
    "## 0 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wideBigModel = BigModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def initWideExtendGZeroV2_crossStack(has_bias=False):\n",
    "    # Embedding layer\n",
    "    emb = model.embedding.weight\n",
    "    wide_emb = G_zero_emb_or_f_lin(emb)\n",
    "    setattr(wideBigModel.embedding, 'weight', torch.nn.Parameter(wide_emb, requires_grad=True).to(device))\n",
    "    # f_lin layer weight\n",
    "    f_lin_weight = model.f_lin.weight\n",
    "    wide_f_lin_weight = G_zero_emb_or_f_lin(f_lin_weight)\n",
    "    setattr(wideBigModel.f_lin, 'weight', torch.nn.Parameter(wide_f_lin_weight, requires_grad=True).to(device))\n",
    "    # f_lin layer bias\n",
    "    if has_bias == True:\n",
    "        f_lin_bias = model.f_lin.bias\n",
    "        wide_f_lin_bias = G_zero_emb_or_f_lin(f_lin_bias)\n",
    "        setattr(wideBigModel.f_lin, 'bias', torch.nn.Parameter(wide_f_lin_bias, requires_grad=True).to(device))\n",
    "\n",
    "    \n",
    "    bi_list = [1,1,2,2,3,3,4,4]\n",
    "    for i in range(1, num_big_layers+1):\n",
    "        bi = bi_list[i-1]\n",
    "        small_block = getattr(model, f'block{bi}')\n",
    "        wide_block = getattr(wideBigModel, f'block{i}')\n",
    "    \n",
    "        # Setting FFN Weights\n",
    "        wide_ffn_weight = G_zero_v2(small_block.ffn.weight)\n",
    "        setattr(wide_block.ffn, 'weight', torch.nn.Parameter((wide_ffn_weight), requires_grad=True).to(device))\n",
    "        if has_bias == True:\n",
    "            wide_ffn_bias = G_zero_v2(small_block.ffn.bias)\n",
    "            setattr(wide_block.ffn, 'bias', torch.nn.Parameter((wide_ffn_bias), requires_grad=True).to(device))\n",
    "\n",
    "        # Setting Norm Layers\n",
    "        l_norm_1_weight = G_zero_1d(small_block.l_norm_1.weight)\n",
    "        l_norm_2_weight = G_zero_1d(small_block.l_norm_2.weight)\n",
    "        setattr(wide_block.l_norm_1, 'weight', torch.nn.Parameter((l_norm_1_weight), requires_grad=True).to(device))\n",
    "        setattr(wide_block.l_norm_2, 'weight', torch.nn.Parameter((l_norm_2_weight), requires_grad=True).to(device))\n",
    "        if has_bias == True:\n",
    "            l_norm_1_bias = G_zero_1d(small_block.l_norm_1.bias)\n",
    "            l_norm_2_bias = G_zero_1d(small_block.l_norm_2.bias)\n",
    "            setattr(wide_block.l_norm_1, 'bias', torch.nn.Parameter((l_norm_1_bias), requires_grad=True).to(device))\n",
    "            setattr(wide_block.l_norm_2, 'bias', torch.nn.Parameter((l_norm_2_bias), requires_grad=True).to(device))\n",
    "\n",
    "        # Setting Multi-Head Attention\n",
    "        wide_mh_lin_weight = G_zero_v2(small_block.multihead.mh_lin.weight)\n",
    "        setattr(wide_block.multihead.mh_lin, 'weight', torch.nn.Parameter((wide_mh_lin_weight), requires_grad=True))\n",
    "\n",
    "        for h in range(1, multi_heads+1):\n",
    "            head = getattr(wide_block.multihead, f'head{h}')\n",
    "            small_head = getattr(small_block.multihead, f'head{h}')\n",
    "\n",
    "            new_k = G_zero_v2(small_head.k.weight)\n",
    "            new_q = G_zero_v2(small_head.q.weight)\n",
    "            new_v = G_zero_v2(small_head.v.weight)\n",
    "            setattr(head.k, 'weight', torch.nn.Parameter((new_k), requires_grad=True).to(device))\n",
    "            setattr(head.q, 'weight', torch.nn.Parameter((new_q), requires_grad=True).to(device))\n",
    "            setattr(head.v, 'weight', torch.nn.Parameter((new_v), requires_grad=True).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.341594934463501\n",
      "Validation: 2.2380168437957764\n",
      "2.264225721359253\n",
      "Validation: 2.213564157485962\n",
      "2.228527307510376\n",
      "Validation: 2.1478302478790283\n",
      "2.1843695640563965\n",
      "Validation: 2.108391046524048\n",
      "2.1760964393615723\n",
      "Validation: 2.1332359313964844\n",
      "2.1275227069854736\n",
      "Validation: 2.1187002658843994\n",
      "2.1264758110046387\n",
      "Validation: 2.0834903717041016\n",
      "2.1154003143310547\n",
      "Validation: 2.0648632049560547\n",
      "2.0729153156280518\n",
      "Validation: 2.0608694553375244\n",
      "2.034621000289917\n",
      "Validation: 2.0541322231292725\n"
     ]
    }
   ],
   "source": [
    "initWideExtendGZeroV2_crossStack()\n",
    "\n",
    "optim_b = torch.optim.Adam(params=wideBigModel.parameters(), lr = 1e-3)\n",
    "train(wideBigModel, optim_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4599757194519043\n",
      "Validation: 2.371760845184326\n",
      "2.2479116916656494\n",
      "Validation: 2.2242133617401123\n",
      "2.1852540969848633\n",
      "Validation: 2.1416192054748535\n",
      "2.120945930480957\n",
      "Validation: 2.0941712856292725\n",
      "2.0811314582824707\n",
      "Validation: 2.0433788299560547\n",
      "2.0192532539367676\n",
      "Validation: 2.03027606010437\n",
      "1.9738414287567139\n",
      "Validation: 2.016104221343994\n",
      "1.9750126600265503\n",
      "Validation: 2.010023355484009\n",
      "1.955073356628418\n",
      "Validation: 2.0032286643981934\n",
      "1.945717453956604\n",
      "Validation: 1.9732179641723633\n"
     ]
    }
   ],
   "source": [
    "wideBigModel_scratch = BigModel().to(device)\n",
    "optim_b = torch.optim.Adam(params=wideBigModel_scratch.parameters(), lr = 1e-3)\n",
    "train(wideBigModel_scratch, optim_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
